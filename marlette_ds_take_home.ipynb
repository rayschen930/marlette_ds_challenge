{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "marlette_ds_challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "R9YghhIiisiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install catboost"
      ],
      "metadata": {
        "id": "OrxWMbHhddo3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import precision_recall_fscore_support, plot_confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostClassifier\n",
        "from catboost import Pool\n",
        "from catboost.utils import get_roc_curve, select_threshold\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# set a seed for reproducibility\n",
        "RANDOM_STATE = 99"
      ],
      "metadata": {
        "id": "_iZ1Oru9hiQb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering"
      ],
      "metadata": {
        "id": "OE8mUaTai8BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('training_data.csv')"
      ],
      "metadata": {
        "id": "w8Q6a_DghiN5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "LPxXL1vgi_98",
        "outputId": "4299a85b-7949-491b-c5fe-dde922e308bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID      var1      var2      var3      var4      var5      var6  \\\n",
              "0  44686  86.52893  80.79771  75.25887  74.02016  69.01476  65.61648   \n",
              "1  44687  68.56225  72.05599  69.52573  68.79211  65.48515  63.00976   \n",
              "2  44688  77.88821  76.62270  73.11046  72.20956  68.26166  65.34046   \n",
              "3  44689  81.11949  78.43038  74.59578  73.63714  69.45540  66.35951   \n",
              "4  44690  62.18698  68.60618  67.86709  67.44987  65.15601  63.13671   \n",
              "\n",
              "       var7      var8      var9  ...     var189  var190  var191     var192  \\\n",
              "0  63.23896  59.07834  56.80397  ...  85.133333   84.45   85.20  85.900000   \n",
              "1  61.19186  57.85757  55.94791  ...  90.533333   86.55   87.24  87.300000   \n",
              "2  63.19467  59.25676  57.01834  ...  93.933333   90.20   89.84  88.600000   \n",
              "3  64.07976  59.88543  57.50303  ...  93.200000   88.15   88.48  87.766667   \n",
              "4  61.52867  58.35072  56.42460  ...  92.733333   88.15   88.00  88.566667   \n",
              "\n",
              "   cat1  cat2  cat3  cat4  cat5  target  \n",
              "0     S     H     C     B     C       0  \n",
              "1     S     I     C     B     C       0  \n",
              "2     S     I     C     B     C       0  \n",
              "3     S     I     C     B     C       0  \n",
              "4     S     I     C     B     C       0  \n",
              "\n",
              "[5 rows x 199 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0ccdf65-57bd-47b8-98aa-8fc987f961ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var189</th>\n",
              "      <th>var190</th>\n",
              "      <th>var191</th>\n",
              "      <th>var192</th>\n",
              "      <th>cat1</th>\n",
              "      <th>cat2</th>\n",
              "      <th>cat3</th>\n",
              "      <th>cat4</th>\n",
              "      <th>cat5</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44686</td>\n",
              "      <td>86.52893</td>\n",
              "      <td>80.79771</td>\n",
              "      <td>75.25887</td>\n",
              "      <td>74.02016</td>\n",
              "      <td>69.01476</td>\n",
              "      <td>65.61648</td>\n",
              "      <td>63.23896</td>\n",
              "      <td>59.07834</td>\n",
              "      <td>56.80397</td>\n",
              "      <td>...</td>\n",
              "      <td>85.133333</td>\n",
              "      <td>84.45</td>\n",
              "      <td>85.20</td>\n",
              "      <td>85.900000</td>\n",
              "      <td>S</td>\n",
              "      <td>H</td>\n",
              "      <td>C</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44687</td>\n",
              "      <td>68.56225</td>\n",
              "      <td>72.05599</td>\n",
              "      <td>69.52573</td>\n",
              "      <td>68.79211</td>\n",
              "      <td>65.48515</td>\n",
              "      <td>63.00976</td>\n",
              "      <td>61.19186</td>\n",
              "      <td>57.85757</td>\n",
              "      <td>55.94791</td>\n",
              "      <td>...</td>\n",
              "      <td>90.533333</td>\n",
              "      <td>86.55</td>\n",
              "      <td>87.24</td>\n",
              "      <td>87.300000</td>\n",
              "      <td>S</td>\n",
              "      <td>I</td>\n",
              "      <td>C</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44688</td>\n",
              "      <td>77.88821</td>\n",
              "      <td>76.62270</td>\n",
              "      <td>73.11046</td>\n",
              "      <td>72.20956</td>\n",
              "      <td>68.26166</td>\n",
              "      <td>65.34046</td>\n",
              "      <td>63.19467</td>\n",
              "      <td>59.25676</td>\n",
              "      <td>57.01834</td>\n",
              "      <td>...</td>\n",
              "      <td>93.933333</td>\n",
              "      <td>90.20</td>\n",
              "      <td>89.84</td>\n",
              "      <td>88.600000</td>\n",
              "      <td>S</td>\n",
              "      <td>I</td>\n",
              "      <td>C</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>44689</td>\n",
              "      <td>81.11949</td>\n",
              "      <td>78.43038</td>\n",
              "      <td>74.59578</td>\n",
              "      <td>73.63714</td>\n",
              "      <td>69.45540</td>\n",
              "      <td>66.35951</td>\n",
              "      <td>64.07976</td>\n",
              "      <td>59.88543</td>\n",
              "      <td>57.50303</td>\n",
              "      <td>...</td>\n",
              "      <td>93.200000</td>\n",
              "      <td>88.15</td>\n",
              "      <td>88.48</td>\n",
              "      <td>87.766667</td>\n",
              "      <td>S</td>\n",
              "      <td>I</td>\n",
              "      <td>C</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44690</td>\n",
              "      <td>62.18698</td>\n",
              "      <td>68.60618</td>\n",
              "      <td>67.86709</td>\n",
              "      <td>67.44987</td>\n",
              "      <td>65.15601</td>\n",
              "      <td>63.13671</td>\n",
              "      <td>61.52867</td>\n",
              "      <td>58.35072</td>\n",
              "      <td>56.42460</td>\n",
              "      <td>...</td>\n",
              "      <td>92.733333</td>\n",
              "      <td>88.15</td>\n",
              "      <td>88.00</td>\n",
              "      <td>88.566667</td>\n",
              "      <td>S</td>\n",
              "      <td>I</td>\n",
              "      <td>C</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 199 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0ccdf65-57bd-47b8-98aa-8fc987f961ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0ccdf65-57bd-47b8-98aa-8fc987f961ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0ccdf65-57bd-47b8-98aa-8fc987f961ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "esmXaHVfhiLJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "01950184-4766-42b6-93e0-325d6dc80dfd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  ID          var1          var2          var3          var4  \\\n",
              "count   14193.000000  14193.000000  14193.000000  14193.000000  14193.000000   \n",
              "unique           NaN           NaN           NaN           NaN           NaN   \n",
              "top              NaN           NaN           NaN           NaN           NaN   \n",
              "freq             NaN           NaN           NaN           NaN           NaN   \n",
              "mean    60691.679067     49.770732     49.816368     49.842512     49.847619   \n",
              "std      9537.840350     19.287088     13.258854     11.027864     10.617691   \n",
              "min     44686.000000      1.962090      6.802490     10.049460     10.790220   \n",
              "25%     52284.000000     35.131290     40.745080     42.377800     42.688460   \n",
              "50%     60726.000000     49.798510     49.924220     50.051180     50.093620   \n",
              "75%     69174.000000     64.213330     59.015310     57.290670     57.032030   \n",
              "max     76811.000000     99.049230     94.186610     92.934460     92.637450   \n",
              "\n",
              "                var5          var6          var7          var8          var9  \\\n",
              "count   14193.000000  14193.000000  14193.000000  14193.000000  14193.000000   \n",
              "unique           NaN           NaN           NaN           NaN           NaN   \n",
              "top              NaN           NaN           NaN           NaN           NaN   \n",
              "freq             NaN           NaN           NaN           NaN           NaN   \n",
              "mean       49.866617     49.877695     49.883878     49.889778     49.889927   \n",
              "std         9.063626      8.015570      7.248920      5.796510      4.958480   \n",
              "min        14.121840     16.951820     19.406260     25.208090     29.421340   \n",
              "25%        43.737880     44.455170     44.995210     46.016590     46.572020   \n",
              "50%        50.080180     50.101920     50.063300     49.967990     49.864340   \n",
              "75%        55.972370     55.194490     54.661910     53.790120     53.208110   \n",
              "max        91.165920     89.667530     88.134180     83.558250     79.443060   \n",
              "\n",
              "        ...        var189        var190        var191        var192   cat1  \\\n",
              "count   ...  14193.000000  14193.000000  14193.000000  14193.000000  14193   \n",
              "unique  ...           NaN           NaN           NaN           NaN     20   \n",
              "top     ...           NaN           NaN           NaN           NaN      M   \n",
              "freq    ...           NaN           NaN           NaN           NaN    716   \n",
              "mean    ...     64.371634     64.739135     65.117889     65.514350    NaN   \n",
              "std     ...     51.892179     51.490296     51.202395     51.035665    NaN   \n",
              "min     ...      1.666667      2.050000      2.160000      2.066667    NaN   \n",
              "25%     ...     22.533333     22.950000     22.880000     23.400000    NaN   \n",
              "50%     ...     50.866667     51.050000     51.400000     51.266667    NaN   \n",
              "75%     ...     92.066667     92.900000     93.760000     94.433333    NaN   \n",
              "max     ...    317.200000    306.600000    295.480000    291.200000    NaN   \n",
              "\n",
              "         cat2   cat3   cat4   cat5        target  \n",
              "count   14193  14193  14193  14193  14193.000000  \n",
              "unique      9      3      3      3           NaN  \n",
              "top         H      B      B      B           NaN  \n",
              "freq     1577   7091  12814  11367           NaN  \n",
              "mean      NaN    NaN    NaN    NaN      0.009441  \n",
              "std       NaN    NaN    NaN    NaN      0.096710  \n",
              "min       NaN    NaN    NaN    NaN      0.000000  \n",
              "25%       NaN    NaN    NaN    NaN      0.000000  \n",
              "50%       NaN    NaN    NaN    NaN      0.000000  \n",
              "75%       NaN    NaN    NaN    NaN      0.000000  \n",
              "max       NaN    NaN    NaN    NaN      1.000000  \n",
              "\n",
              "[11 rows x 199 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2690505b-2250-4662-8041-2a2c7f800f15\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var189</th>\n",
              "      <th>var190</th>\n",
              "      <th>var191</th>\n",
              "      <th>var192</th>\n",
              "      <th>cat1</th>\n",
              "      <th>cat2</th>\n",
              "      <th>cat3</th>\n",
              "      <th>cat4</th>\n",
              "      <th>cat5</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193.000000</td>\n",
              "      <td>14193</td>\n",
              "      <td>14193</td>\n",
              "      <td>14193</td>\n",
              "      <td>14193</td>\n",
              "      <td>14193</td>\n",
              "      <td>14193.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "      <td>H</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>716</td>\n",
              "      <td>1577</td>\n",
              "      <td>7091</td>\n",
              "      <td>12814</td>\n",
              "      <td>11367</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>60691.679067</td>\n",
              "      <td>49.770732</td>\n",
              "      <td>49.816368</td>\n",
              "      <td>49.842512</td>\n",
              "      <td>49.847619</td>\n",
              "      <td>49.866617</td>\n",
              "      <td>49.877695</td>\n",
              "      <td>49.883878</td>\n",
              "      <td>49.889778</td>\n",
              "      <td>49.889927</td>\n",
              "      <td>...</td>\n",
              "      <td>64.371634</td>\n",
              "      <td>64.739135</td>\n",
              "      <td>65.117889</td>\n",
              "      <td>65.514350</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9537.840350</td>\n",
              "      <td>19.287088</td>\n",
              "      <td>13.258854</td>\n",
              "      <td>11.027864</td>\n",
              "      <td>10.617691</td>\n",
              "      <td>9.063626</td>\n",
              "      <td>8.015570</td>\n",
              "      <td>7.248920</td>\n",
              "      <td>5.796510</td>\n",
              "      <td>4.958480</td>\n",
              "      <td>...</td>\n",
              "      <td>51.892179</td>\n",
              "      <td>51.490296</td>\n",
              "      <td>51.202395</td>\n",
              "      <td>51.035665</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.096710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>44686.000000</td>\n",
              "      <td>1.962090</td>\n",
              "      <td>6.802490</td>\n",
              "      <td>10.049460</td>\n",
              "      <td>10.790220</td>\n",
              "      <td>14.121840</td>\n",
              "      <td>16.951820</td>\n",
              "      <td>19.406260</td>\n",
              "      <td>25.208090</td>\n",
              "      <td>29.421340</td>\n",
              "      <td>...</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>2.050000</td>\n",
              "      <td>2.160000</td>\n",
              "      <td>2.066667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>52284.000000</td>\n",
              "      <td>35.131290</td>\n",
              "      <td>40.745080</td>\n",
              "      <td>42.377800</td>\n",
              "      <td>42.688460</td>\n",
              "      <td>43.737880</td>\n",
              "      <td>44.455170</td>\n",
              "      <td>44.995210</td>\n",
              "      <td>46.016590</td>\n",
              "      <td>46.572020</td>\n",
              "      <td>...</td>\n",
              "      <td>22.533333</td>\n",
              "      <td>22.950000</td>\n",
              "      <td>22.880000</td>\n",
              "      <td>23.400000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>60726.000000</td>\n",
              "      <td>49.798510</td>\n",
              "      <td>49.924220</td>\n",
              "      <td>50.051180</td>\n",
              "      <td>50.093620</td>\n",
              "      <td>50.080180</td>\n",
              "      <td>50.101920</td>\n",
              "      <td>50.063300</td>\n",
              "      <td>49.967990</td>\n",
              "      <td>49.864340</td>\n",
              "      <td>...</td>\n",
              "      <td>50.866667</td>\n",
              "      <td>51.050000</td>\n",
              "      <td>51.400000</td>\n",
              "      <td>51.266667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>69174.000000</td>\n",
              "      <td>64.213330</td>\n",
              "      <td>59.015310</td>\n",
              "      <td>57.290670</td>\n",
              "      <td>57.032030</td>\n",
              "      <td>55.972370</td>\n",
              "      <td>55.194490</td>\n",
              "      <td>54.661910</td>\n",
              "      <td>53.790120</td>\n",
              "      <td>53.208110</td>\n",
              "      <td>...</td>\n",
              "      <td>92.066667</td>\n",
              "      <td>92.900000</td>\n",
              "      <td>93.760000</td>\n",
              "      <td>94.433333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>76811.000000</td>\n",
              "      <td>99.049230</td>\n",
              "      <td>94.186610</td>\n",
              "      <td>92.934460</td>\n",
              "      <td>92.637450</td>\n",
              "      <td>91.165920</td>\n",
              "      <td>89.667530</td>\n",
              "      <td>88.134180</td>\n",
              "      <td>83.558250</td>\n",
              "      <td>79.443060</td>\n",
              "      <td>...</td>\n",
              "      <td>317.200000</td>\n",
              "      <td>306.600000</td>\n",
              "      <td>295.480000</td>\n",
              "      <td>291.200000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 199 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2690505b-2250-4662-8041-2a2c7f800f15')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2690505b-2250-4662-8041-2a2c7f800f15 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2690505b-2250-4662-8041-2a2c7f800f15');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The describe method on the 5 categorical variables shows that the highest number of categories is 20 for cat1.  Encoding the 5 categorical variables will result in 33 columns (since we can drop one category for every variable) compared to the original 5.  Given that there are so many numerical variables, one hot encoding will not result in an overly sparse matrix, so this type of encoding will be sufficient for modeling purposes. While Catboost and XGBoost can be implemented without encoding the categorical variables, logistic regression cannot, so we will still proceed with encoding the data beforehand."
      ],
      "metadata": {
        "id": "V9gSk4eJlxui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first scale all the numerical data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = df.iloc[:, 1:193]\n",
        "X[X.columns] = scaler.fit_transform(X[X.columns])"
      ],
      "metadata": {
        "id": "udfjF1nSnpnH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the categorical columns\n",
        "\n",
        "# one hot encoding the 5 categorical columns\n",
        "# get list of categorical columns\n",
        "cat_vars = ['cat1', 'cat2', 'cat3', 'cat4', 'cat5']\n",
        "\n",
        "# dummify each categorical variable, drop the first column,\n",
        "# and add it to the numerical columns\n",
        "for i in cat_vars:\n",
        "  X = X.join(pd.get_dummies(df[i]).iloc[:, 1:], rsuffix=i)\n",
        "\n",
        "\n",
        "# Getting the target volumn as its own series\n",
        "y = df.target\n",
        "\n",
        "# get a sense of whether or not y is imbalanced\n",
        "print(sum(y) / len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0SIZwHTm9qu",
        "outputId": "cb1f2b8b-56f9-41b6-aa3d-c6e997cd82db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009441273867399421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above that the training sample is highly imbalanced, with less than 1% of the samples with a target of 1."
      ],
      "metadata": {
        "id": "8Cv09nBfq3vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting testing and training set\n",
        "\n",
        "x_train, x_test, y_train, y_test =  train_test_split(X, y,\n",
        "                                                     test_size=.33,\n",
        "                                                     random_state = RANDOM_STATE)"
      ],
      "metadata": {
        "id": "yi28fMPkjLD1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_allocation = sum(y_train) / len(y_train)\n",
        "test_label_allocation = sum(y_test) / len(y_test)\n",
        "\n",
        "print('Percentage of ones in training set is: ', train_label_allocation)\n",
        "print('Percentage of ones in testing set is: ', test_label_allocation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAPkmioFVQD1",
        "outputId": "21fadb10-8953-419b-e686-122c584c2b30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of ones in training set is:  0.008833736460195604\n",
            "Percentage of ones in testing set is:  0.01067463706233988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_label_allocation - test_label_allocation) / train_label_allocation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyz29vQzfNDm",
        "outputId": "ec411c5b-7f2a-4341-8947-8bc1d44a2f1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.2083943312594039"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The percentage of the one label in the testing set is roughly 20% off from the percentage of the one label in the training set.  This is problematic so we will rerun the train_test_split function, but individually for both groups, and then join them."
      ],
      "metadata": {
        "id": "truzWCINfIyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = X.join(y)\n",
        "\n",
        "target_0 = cleaned_df[cleaned_df['target']==0]\n",
        "target_1 = cleaned_df[cleaned_df['target']==1]"
      ],
      "metadata": {
        "id": "Sj2s7Ju8Wl3l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting into different training and test sets where the target is 0\n",
        "x_train_0, x_test_0, y_train_0, y_test_0 =  train_test_split(target_0.iloc[:, :-1],\n",
        "                                                             target_0.iloc[:, -1],\n",
        "                                                             test_size=.33,\n",
        "                                                             random_state = RANDOM_STATE)"
      ],
      "metadata": {
        "id": "4X2BOoBJXOet"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting into different training and test sets where the target is 1\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 =  train_test_split(target_1.iloc[:, :-1],\n",
        "                                                             target_1.iloc[:, -1],\n",
        "                                                             test_size=.33,\n",
        "                                                             random_state = RANDOM_STATE)"
      ],
      "metadata": {
        "id": "Ytf3ygMcXqyT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining them \n",
        "x_train = x_train_0.append(x_train_1)\n",
        "x_test = x_test_0.append(x_test_1)\n",
        "y_train = y_train_0.append(y_train_1)\n",
        "y_test = y_test_0.append(y_test_1)"
      ],
      "metadata": {
        "id": "Q2d-v18OX2Kr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_allocation = sum(y_train) / len(y_train)\n",
        "test_label_allocation = sum(y_test) / len(y_test)\n",
        "\n",
        "print('Percentage of ones in training set is: ', train_label_allocation)\n",
        "print('Percentage of ones in testing set is: ', test_label_allocation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCTPc1hxYUEK",
        "outputId": "9f932e1b-18b4-4ecf-c564-44a07bd5f266"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of ones in training set is:  0.009360538493899874\n",
            "Percentage of ones in testing set is:  0.0096051227321238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_label_allocation - test_label_allocation) / train_label_allocation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIKTEvLUYUl7",
        "outputId": "586a1a43-bd2a-482b-95f8-9f6efd97ee3f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.02612929142733805"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The percentage of the one label in the testing set is now less than 3% off from the percentage of the one label in the training set.  Models will work much better now with this data as the distribution of the target variable is more similar.  We will now shuffle the data so all the target=1 observations are not right next to each other at the end."
      ],
      "metadata": {
        "id": "9ZUavMUdYioP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding random state for reproducibility\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=RANDOM_STATE)\n",
        "x_test, y_test = shuffle(x_test, y_test, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "id": "CF80p6bgYwZY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building baseline binary classifiers"
      ],
      "metadata": {
        "id": "59O1tjgkjBt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our data shuffled and split into testing and training sets, we can begin building the baseline models."
      ],
      "metadata": {
        "id": "a3Y8Oii-v8Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to easily get scores in a df\n",
        "\n",
        "score_cols=['Train_acc', 'Train_prec', 'Train_recall', 'Train_fscore', 'Train_auc',\n",
        "            'Test_acc','Test_prec', 'Test_recall', 'Test_fscore', 'Test_auc',\n",
        "            'Model', 'Model_version']\n",
        "all_scores = pd.DataFrame(columns=score_cols)\n",
        "\n",
        "def get_scores(train_pred, test_pred, y_train, y_test, mod, version):\n",
        "  scores_train = precision_recall_fscore_support(y_train, train_pred,\n",
        "                                                 average='binary')\n",
        "  acc_train = sum(train_pred == y_train) / len(y_train)\n",
        "  prec_train = scores_train[0]\n",
        "  recall_train = scores_train[1]\n",
        "  f_score_train = scores_train[2]\n",
        "  auc_train = roc_auc_score(y_train, train_pred)\n",
        "\n",
        "  scores_test = precision_recall_fscore_support(y_test, test_pred,\n",
        "                                                average='binary')\n",
        "  acc_test = sum(test_pred == y_test) / len(y_test)\n",
        "  prec_test = scores_test[0]\n",
        "  recall_test = scores_test[1]\n",
        "  f_score_test = scores_test[2]\n",
        "  auc_test = roc_auc_score(y_test, test_pred)\n",
        "\n",
        "  scores = [acc_train, prec_train, recall_train, f_score_train, \n",
        "            auc_train, acc_test, prec_test, recall_test, f_score_test,\n",
        "            auc_test, mod, version]\n",
        "\n",
        "  df_score = pd.DataFrame(np.reshape(scores, newshape=(1,12)),\n",
        "                          columns=score_cols)\n",
        "  return df_score"
      ],
      "metadata": {
        "id": "OpRUaR1Hi_ib"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at benchmark logistic regression\n",
        "\n",
        "# using a very low value for C and increasing max_inter so the model can converge\n",
        "lr_bench = LogisticRegression(random_state=RANDOM_STATE, C=.00001, max_iter=1000)\n",
        "lr_bench.fit(x_train, y_train)\n",
        "\n",
        "# getting predicitons for LR model\n",
        "lr_train_pred = lr_bench.predict(x_train)\n",
        "lr_test_pred = lr_bench.predict(x_test)"
      ],
      "metadata": {
        "id": "VqDHYLMKi_gI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at benchmark cat boost classifier\n",
        "\n",
        "# using a default catboost classifier\n",
        "cat_bench = CatBoostClassifier(random_state=RANDOM_STATE)\n",
        "cat_bench.fit(x_train, y_train)\n",
        "\n",
        "# getting predicitons for catboost model\n",
        "cat_train_pred = cat_bench.predict(x_train)\n",
        "cat_test_pred = cat_bench.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBiNKR-CdMWQ",
        "outputId": "1b7ebc77-0ef3-4377-ec23-bba98ec86a3f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate set to 0.026951\n",
            "0:\tlearn: 0.6214721\ttotal: 203ms\tremaining: 3m 23s\n",
            "1:\tlearn: 0.5650572\ttotal: 329ms\tremaining: 2m 44s\n",
            "2:\tlearn: 0.5102223\ttotal: 459ms\tremaining: 2m 32s\n",
            "3:\tlearn: 0.4634753\ttotal: 602ms\tremaining: 2m 29s\n",
            "4:\tlearn: 0.4220953\ttotal: 759ms\tremaining: 2m 31s\n",
            "5:\tlearn: 0.3849939\ttotal: 875ms\tremaining: 2m 24s\n",
            "6:\tlearn: 0.3512520\ttotal: 999ms\tremaining: 2m 21s\n",
            "7:\tlearn: 0.3230709\ttotal: 1.12s\tremaining: 2m 19s\n",
            "8:\tlearn: 0.2967875\ttotal: 1.26s\tremaining: 2m 18s\n",
            "9:\tlearn: 0.2715826\ttotal: 1.43s\tremaining: 2m 21s\n",
            "10:\tlearn: 0.2502700\ttotal: 1.54s\tremaining: 2m 18s\n",
            "11:\tlearn: 0.2286762\ttotal: 1.68s\tremaining: 2m 18s\n",
            "12:\tlearn: 0.2107195\ttotal: 1.8s\tremaining: 2m 17s\n",
            "13:\tlearn: 0.1952871\ttotal: 1.93s\tremaining: 2m 15s\n",
            "14:\tlearn: 0.1817283\ttotal: 2.04s\tremaining: 2m 14s\n",
            "15:\tlearn: 0.1694078\ttotal: 2.17s\tremaining: 2m 13s\n",
            "16:\tlearn: 0.1589298\ttotal: 2.28s\tremaining: 2m 11s\n",
            "17:\tlearn: 0.1462302\ttotal: 2.42s\tremaining: 2m 11s\n",
            "18:\tlearn: 0.1368736\ttotal: 2.54s\tremaining: 2m 11s\n",
            "19:\tlearn: 0.1286670\ttotal: 2.72s\tremaining: 2m 13s\n",
            "20:\tlearn: 0.1216767\ttotal: 2.9s\tremaining: 2m 15s\n",
            "21:\tlearn: 0.1152947\ttotal: 3s\tremaining: 2m 13s\n",
            "22:\tlearn: 0.1086353\ttotal: 3.13s\tremaining: 2m 13s\n",
            "23:\tlearn: 0.1029769\ttotal: 3.3s\tremaining: 2m 14s\n",
            "24:\tlearn: 0.0974392\ttotal: 3.48s\tremaining: 2m 15s\n",
            "25:\tlearn: 0.0924973\ttotal: 3.6s\tremaining: 2m 14s\n",
            "26:\tlearn: 0.0884878\ttotal: 3.7s\tremaining: 2m 13s\n",
            "27:\tlearn: 0.0842664\ttotal: 3.87s\tremaining: 2m 14s\n",
            "28:\tlearn: 0.0804660\ttotal: 4s\tremaining: 2m 13s\n",
            "29:\tlearn: 0.0765820\ttotal: 4.11s\tremaining: 2m 12s\n",
            "30:\tlearn: 0.0732798\ttotal: 4.24s\tremaining: 2m 12s\n",
            "31:\tlearn: 0.0704051\ttotal: 4.37s\tremaining: 2m 12s\n",
            "32:\tlearn: 0.0677864\ttotal: 4.47s\tremaining: 2m 11s\n",
            "33:\tlearn: 0.0652957\ttotal: 4.61s\tremaining: 2m 10s\n",
            "34:\tlearn: 0.0634426\ttotal: 4.74s\tremaining: 2m 10s\n",
            "35:\tlearn: 0.0615444\ttotal: 4.87s\tremaining: 2m 10s\n",
            "36:\tlearn: 0.0600518\ttotal: 4.96s\tremaining: 2m 9s\n",
            "37:\tlearn: 0.0580291\ttotal: 5.1s\tremaining: 2m 9s\n",
            "38:\tlearn: 0.0564889\ttotal: 5.24s\tremaining: 2m 9s\n",
            "39:\tlearn: 0.0552898\ttotal: 5.37s\tremaining: 2m 8s\n",
            "40:\tlearn: 0.0536122\ttotal: 5.48s\tremaining: 2m 8s\n",
            "41:\tlearn: 0.0524570\ttotal: 5.6s\tremaining: 2m 7s\n",
            "42:\tlearn: 0.0510345\ttotal: 5.71s\tremaining: 2m 7s\n",
            "43:\tlearn: 0.0498039\ttotal: 5.79s\tremaining: 2m 5s\n",
            "44:\tlearn: 0.0488113\ttotal: 5.93s\tremaining: 2m 5s\n",
            "45:\tlearn: 0.0480714\ttotal: 6.08s\tremaining: 2m 6s\n",
            "46:\tlearn: 0.0470371\ttotal: 6.23s\tremaining: 2m 6s\n",
            "47:\tlearn: 0.0460178\ttotal: 6.39s\tremaining: 2m 6s\n",
            "48:\tlearn: 0.0453723\ttotal: 6.49s\tremaining: 2m 5s\n",
            "49:\tlearn: 0.0445171\ttotal: 6.61s\tremaining: 2m 5s\n",
            "50:\tlearn: 0.0438231\ttotal: 6.76s\tremaining: 2m 5s\n",
            "51:\tlearn: 0.0432081\ttotal: 6.83s\tremaining: 2m 4s\n",
            "52:\tlearn: 0.0427159\ttotal: 6.93s\tremaining: 2m 3s\n",
            "53:\tlearn: 0.0421000\ttotal: 7.04s\tremaining: 2m 3s\n",
            "54:\tlearn: 0.0417413\ttotal: 7.19s\tremaining: 2m 3s\n",
            "55:\tlearn: 0.0412957\ttotal: 7.32s\tremaining: 2m 3s\n",
            "56:\tlearn: 0.0409166\ttotal: 7.44s\tremaining: 2m 3s\n",
            "57:\tlearn: 0.0403860\ttotal: 7.56s\tremaining: 2m 2s\n",
            "58:\tlearn: 0.0397357\ttotal: 7.7s\tremaining: 2m 2s\n",
            "59:\tlearn: 0.0392523\ttotal: 7.81s\tremaining: 2m 2s\n",
            "60:\tlearn: 0.0389464\ttotal: 7.92s\tremaining: 2m 1s\n",
            "61:\tlearn: 0.0384295\ttotal: 8.04s\tremaining: 2m 1s\n",
            "62:\tlearn: 0.0381782\ttotal: 8.14s\tremaining: 2m 1s\n",
            "63:\tlearn: 0.0379622\ttotal: 8.27s\tremaining: 2m\n",
            "64:\tlearn: 0.0376874\ttotal: 8.39s\tremaining: 2m\n",
            "65:\tlearn: 0.0374840\ttotal: 8.52s\tremaining: 2m\n",
            "66:\tlearn: 0.0371187\ttotal: 8.64s\tremaining: 2m\n",
            "67:\tlearn: 0.0368256\ttotal: 8.76s\tremaining: 2m\n",
            "68:\tlearn: 0.0366512\ttotal: 8.87s\tremaining: 1m 59s\n",
            "69:\tlearn: 0.0364616\ttotal: 8.99s\tremaining: 1m 59s\n",
            "70:\tlearn: 0.0359683\ttotal: 9.11s\tremaining: 1m 59s\n",
            "71:\tlearn: 0.0357937\ttotal: 9.22s\tremaining: 1m 58s\n",
            "72:\tlearn: 0.0353887\ttotal: 9.34s\tremaining: 1m 58s\n",
            "73:\tlearn: 0.0351909\ttotal: 9.46s\tremaining: 1m 58s\n",
            "74:\tlearn: 0.0349163\ttotal: 9.58s\tremaining: 1m 58s\n",
            "75:\tlearn: 0.0347607\ttotal: 9.7s\tremaining: 1m 57s\n",
            "76:\tlearn: 0.0346019\ttotal: 9.81s\tremaining: 1m 57s\n",
            "77:\tlearn: 0.0342613\ttotal: 9.93s\tremaining: 1m 57s\n",
            "78:\tlearn: 0.0340609\ttotal: 10.1s\tremaining: 1m 57s\n",
            "79:\tlearn: 0.0336976\ttotal: 10.2s\tremaining: 1m 57s\n",
            "80:\tlearn: 0.0333939\ttotal: 10.4s\tremaining: 1m 57s\n",
            "81:\tlearn: 0.0331291\ttotal: 10.6s\tremaining: 1m 58s\n",
            "82:\tlearn: 0.0328837\ttotal: 10.7s\tremaining: 1m 58s\n",
            "83:\tlearn: 0.0326666\ttotal: 10.9s\tremaining: 1m 58s\n",
            "84:\tlearn: 0.0325598\ttotal: 11s\tremaining: 1m 58s\n",
            "85:\tlearn: 0.0324472\ttotal: 11.2s\tremaining: 1m 58s\n",
            "86:\tlearn: 0.0322984\ttotal: 11.4s\tremaining: 1m 59s\n",
            "87:\tlearn: 0.0322011\ttotal: 11.6s\tremaining: 1m 59s\n",
            "88:\tlearn: 0.0321046\ttotal: 11.7s\tremaining: 1m 59s\n",
            "89:\tlearn: 0.0319880\ttotal: 11.8s\tremaining: 1m 59s\n",
            "90:\tlearn: 0.0317529\ttotal: 12s\tremaining: 1m 59s\n",
            "91:\tlearn: 0.0316186\ttotal: 12.1s\tremaining: 1m 59s\n",
            "92:\tlearn: 0.0313073\ttotal: 12.3s\tremaining: 2m\n",
            "93:\tlearn: 0.0310227\ttotal: 12.4s\tremaining: 1m 59s\n",
            "94:\tlearn: 0.0308089\ttotal: 12.6s\tremaining: 1m 59s\n",
            "95:\tlearn: 0.0306447\ttotal: 12.7s\tremaining: 1m 59s\n",
            "96:\tlearn: 0.0305534\ttotal: 12.8s\tremaining: 1m 59s\n",
            "97:\tlearn: 0.0303882\ttotal: 13s\tremaining: 1m 59s\n",
            "98:\tlearn: 0.0302343\ttotal: 13.1s\tremaining: 1m 58s\n",
            "99:\tlearn: 0.0300203\ttotal: 13.2s\tremaining: 1m 58s\n",
            "100:\tlearn: 0.0299659\ttotal: 13.3s\tremaining: 1m 58s\n",
            "101:\tlearn: 0.0299266\ttotal: 13.5s\tremaining: 1m 58s\n",
            "102:\tlearn: 0.0297690\ttotal: 13.6s\tremaining: 1m 58s\n",
            "103:\tlearn: 0.0296599\ttotal: 13.8s\tremaining: 1m 58s\n",
            "104:\tlearn: 0.0294576\ttotal: 13.9s\tremaining: 1m 58s\n",
            "105:\tlearn: 0.0291814\ttotal: 14.1s\tremaining: 1m 58s\n",
            "106:\tlearn: 0.0289715\ttotal: 14.3s\tremaining: 1m 58s\n",
            "107:\tlearn: 0.0288342\ttotal: 14.4s\tremaining: 1m 58s\n",
            "108:\tlearn: 0.0286278\ttotal: 14.5s\tremaining: 1m 58s\n",
            "109:\tlearn: 0.0285428\ttotal: 14.6s\tremaining: 1m 58s\n",
            "110:\tlearn: 0.0284418\ttotal: 14.8s\tremaining: 1m 58s\n",
            "111:\tlearn: 0.0283404\ttotal: 14.9s\tremaining: 1m 57s\n",
            "112:\tlearn: 0.0281865\ttotal: 15s\tremaining: 1m 57s\n",
            "113:\tlearn: 0.0280948\ttotal: 15.1s\tremaining: 1m 57s\n",
            "114:\tlearn: 0.0279618\ttotal: 15.2s\tremaining: 1m 56s\n",
            "115:\tlearn: 0.0279196\ttotal: 15.4s\tremaining: 1m 57s\n",
            "116:\tlearn: 0.0278127\ttotal: 15.5s\tremaining: 1m 57s\n",
            "117:\tlearn: 0.0276541\ttotal: 15.6s\tremaining: 1m 56s\n",
            "118:\tlearn: 0.0276031\ttotal: 15.8s\tremaining: 1m 56s\n",
            "119:\tlearn: 0.0274792\ttotal: 15.9s\tremaining: 1m 56s\n",
            "120:\tlearn: 0.0274339\ttotal: 16s\tremaining: 1m 56s\n",
            "121:\tlearn: 0.0272835\ttotal: 16.2s\tremaining: 1m 56s\n",
            "122:\tlearn: 0.0272004\ttotal: 16.4s\tremaining: 1m 56s\n",
            "123:\tlearn: 0.0270558\ttotal: 16.5s\tremaining: 1m 56s\n",
            "124:\tlearn: 0.0268809\ttotal: 16.7s\tremaining: 1m 56s\n",
            "125:\tlearn: 0.0267769\ttotal: 16.8s\tremaining: 1m 56s\n",
            "126:\tlearn: 0.0266786\ttotal: 17s\tremaining: 1m 56s\n",
            "127:\tlearn: 0.0266177\ttotal: 17.1s\tremaining: 1m 56s\n",
            "128:\tlearn: 0.0265573\ttotal: 17.2s\tremaining: 1m 56s\n",
            "129:\tlearn: 0.0264488\ttotal: 17.3s\tremaining: 1m 55s\n",
            "130:\tlearn: 0.0263923\ttotal: 17.4s\tremaining: 1m 55s\n",
            "131:\tlearn: 0.0263185\ttotal: 17.6s\tremaining: 1m 55s\n",
            "132:\tlearn: 0.0261877\ttotal: 17.7s\tremaining: 1m 55s\n",
            "133:\tlearn: 0.0260977\ttotal: 17.8s\tremaining: 1m 55s\n",
            "134:\tlearn: 0.0260467\ttotal: 17.9s\tremaining: 1m 54s\n",
            "135:\tlearn: 0.0259770\ttotal: 18s\tremaining: 1m 54s\n",
            "136:\tlearn: 0.0258985\ttotal: 18.2s\tremaining: 1m 54s\n",
            "137:\tlearn: 0.0258568\ttotal: 18.3s\tremaining: 1m 54s\n",
            "138:\tlearn: 0.0256968\ttotal: 18.4s\tremaining: 1m 53s\n",
            "139:\tlearn: 0.0256188\ttotal: 18.5s\tremaining: 1m 53s\n",
            "140:\tlearn: 0.0254858\ttotal: 18.6s\tremaining: 1m 53s\n",
            "141:\tlearn: 0.0253982\ttotal: 18.7s\tremaining: 1m 53s\n",
            "142:\tlearn: 0.0253292\ttotal: 18.9s\tremaining: 1m 53s\n",
            "143:\tlearn: 0.0252647\ttotal: 19s\tremaining: 1m 53s\n",
            "144:\tlearn: 0.0251488\ttotal: 19.2s\tremaining: 1m 53s\n",
            "145:\tlearn: 0.0250009\ttotal: 19.3s\tremaining: 1m 53s\n",
            "146:\tlearn: 0.0249315\ttotal: 19.5s\tremaining: 1m 53s\n",
            "147:\tlearn: 0.0248059\ttotal: 19.7s\tremaining: 1m 53s\n",
            "148:\tlearn: 0.0247205\ttotal: 19.9s\tremaining: 1m 53s\n",
            "149:\tlearn: 0.0246659\ttotal: 20s\tremaining: 1m 53s\n",
            "150:\tlearn: 0.0245425\ttotal: 20.1s\tremaining: 1m 53s\n",
            "151:\tlearn: 0.0245144\ttotal: 20.2s\tremaining: 1m 52s\n",
            "152:\tlearn: 0.0244141\ttotal: 20.4s\tremaining: 1m 52s\n",
            "153:\tlearn: 0.0243695\ttotal: 20.5s\tremaining: 1m 52s\n",
            "154:\tlearn: 0.0242757\ttotal: 20.6s\tremaining: 1m 52s\n",
            "155:\tlearn: 0.0241690\ttotal: 20.7s\tremaining: 1m 52s\n",
            "156:\tlearn: 0.0241072\ttotal: 20.9s\tremaining: 1m 52s\n",
            "157:\tlearn: 0.0240346\ttotal: 21s\tremaining: 1m 51s\n",
            "158:\tlearn: 0.0239927\ttotal: 21.1s\tremaining: 1m 51s\n",
            "159:\tlearn: 0.0239313\ttotal: 21.2s\tremaining: 1m 51s\n",
            "160:\tlearn: 0.0238431\ttotal: 21.4s\tremaining: 1m 51s\n",
            "161:\tlearn: 0.0237409\ttotal: 21.5s\tremaining: 1m 51s\n",
            "162:\tlearn: 0.0237071\ttotal: 21.6s\tremaining: 1m 50s\n",
            "163:\tlearn: 0.0236002\ttotal: 21.7s\tremaining: 1m 50s\n",
            "164:\tlearn: 0.0235632\ttotal: 21.8s\tremaining: 1m 50s\n",
            "165:\tlearn: 0.0234793\ttotal: 21.9s\tremaining: 1m 50s\n",
            "166:\tlearn: 0.0234321\ttotal: 22s\tremaining: 1m 49s\n",
            "167:\tlearn: 0.0232951\ttotal: 22.1s\tremaining: 1m 49s\n",
            "168:\tlearn: 0.0231806\ttotal: 22.2s\tremaining: 1m 49s\n",
            "169:\tlearn: 0.0231290\ttotal: 22.3s\tremaining: 1m 48s\n",
            "170:\tlearn: 0.0230736\ttotal: 22.4s\tremaining: 1m 48s\n",
            "171:\tlearn: 0.0230174\ttotal: 22.5s\tremaining: 1m 48s\n",
            "172:\tlearn: 0.0229309\ttotal: 22.6s\tremaining: 1m 48s\n",
            "173:\tlearn: 0.0228386\ttotal: 22.7s\tremaining: 1m 47s\n",
            "174:\tlearn: 0.0227658\ttotal: 22.8s\tremaining: 1m 47s\n",
            "175:\tlearn: 0.0227288\ttotal: 23s\tremaining: 1m 47s\n",
            "176:\tlearn: 0.0226885\ttotal: 23.1s\tremaining: 1m 47s\n",
            "177:\tlearn: 0.0226546\ttotal: 23.2s\tremaining: 1m 47s\n",
            "178:\tlearn: 0.0226165\ttotal: 23.3s\tremaining: 1m 46s\n",
            "179:\tlearn: 0.0225975\ttotal: 23.5s\tremaining: 1m 46s\n",
            "180:\tlearn: 0.0225626\ttotal: 23.5s\tremaining: 1m 46s\n",
            "181:\tlearn: 0.0225162\ttotal: 23.7s\tremaining: 1m 46s\n",
            "182:\tlearn: 0.0224766\ttotal: 23.8s\tremaining: 1m 46s\n",
            "183:\tlearn: 0.0223969\ttotal: 23.9s\tremaining: 1m 46s\n",
            "184:\tlearn: 0.0223581\ttotal: 24s\tremaining: 1m 45s\n",
            "185:\tlearn: 0.0222929\ttotal: 24.2s\tremaining: 1m 45s\n",
            "186:\tlearn: 0.0222733\ttotal: 24.3s\tremaining: 1m 45s\n",
            "187:\tlearn: 0.0222426\ttotal: 24.5s\tremaining: 1m 45s\n",
            "188:\tlearn: 0.0221710\ttotal: 24.7s\tremaining: 1m 46s\n",
            "189:\tlearn: 0.0221337\ttotal: 25s\tremaining: 1m 46s\n",
            "190:\tlearn: 0.0220901\ttotal: 25.1s\tremaining: 1m 46s\n",
            "191:\tlearn: 0.0220200\ttotal: 25.2s\tremaining: 1m 46s\n",
            "192:\tlearn: 0.0219944\ttotal: 25.3s\tremaining: 1m 45s\n",
            "193:\tlearn: 0.0219392\ttotal: 25.4s\tremaining: 1m 45s\n",
            "194:\tlearn: 0.0218908\ttotal: 25.6s\tremaining: 1m 45s\n",
            "195:\tlearn: 0.0218698\ttotal: 25.7s\tremaining: 1m 45s\n",
            "196:\tlearn: 0.0218375\ttotal: 25.8s\tremaining: 1m 45s\n",
            "197:\tlearn: 0.0218149\ttotal: 25.9s\tremaining: 1m 44s\n",
            "198:\tlearn: 0.0217685\ttotal: 26s\tremaining: 1m 44s\n",
            "199:\tlearn: 0.0216377\ttotal: 26.1s\tremaining: 1m 44s\n",
            "200:\tlearn: 0.0215352\ttotal: 26.2s\tremaining: 1m 44s\n",
            "201:\tlearn: 0.0215029\ttotal: 26.2s\tremaining: 1m 43s\n",
            "202:\tlearn: 0.0214211\ttotal: 26.3s\tremaining: 1m 43s\n",
            "203:\tlearn: 0.0214007\ttotal: 26.4s\tremaining: 1m 42s\n",
            "204:\tlearn: 0.0213607\ttotal: 26.4s\tremaining: 1m 42s\n",
            "205:\tlearn: 0.0212924\ttotal: 26.5s\tremaining: 1m 42s\n",
            "206:\tlearn: 0.0211860\ttotal: 26.5s\tremaining: 1m 41s\n",
            "207:\tlearn: 0.0210724\ttotal: 26.6s\tremaining: 1m 41s\n",
            "208:\tlearn: 0.0210032\ttotal: 26.6s\tremaining: 1m 40s\n",
            "209:\tlearn: 0.0209856\ttotal: 26.7s\tremaining: 1m 40s\n",
            "210:\tlearn: 0.0209462\ttotal: 26.8s\tremaining: 1m 40s\n",
            "211:\tlearn: 0.0209054\ttotal: 26.8s\tremaining: 1m 39s\n",
            "212:\tlearn: 0.0208635\ttotal: 26.9s\tremaining: 1m 39s\n",
            "213:\tlearn: 0.0208200\ttotal: 26.9s\tremaining: 1m 38s\n",
            "214:\tlearn: 0.0207458\ttotal: 27s\tremaining: 1m 38s\n",
            "215:\tlearn: 0.0207071\ttotal: 27.1s\tremaining: 1m 38s\n",
            "216:\tlearn: 0.0206583\ttotal: 27.1s\tremaining: 1m 37s\n",
            "217:\tlearn: 0.0206046\ttotal: 27.2s\tremaining: 1m 37s\n",
            "218:\tlearn: 0.0204807\ttotal: 27.2s\tremaining: 1m 37s\n",
            "219:\tlearn: 0.0203970\ttotal: 27.3s\tremaining: 1m 36s\n",
            "220:\tlearn: 0.0203461\ttotal: 27.4s\tremaining: 1m 36s\n",
            "221:\tlearn: 0.0203205\ttotal: 27.4s\tremaining: 1m 36s\n",
            "222:\tlearn: 0.0202845\ttotal: 27.5s\tremaining: 1m 35s\n",
            "223:\tlearn: 0.0202558\ttotal: 27.5s\tremaining: 1m 35s\n",
            "224:\tlearn: 0.0202019\ttotal: 27.6s\tremaining: 1m 35s\n",
            "225:\tlearn: 0.0201638\ttotal: 27.6s\tremaining: 1m 34s\n",
            "226:\tlearn: 0.0201380\ttotal: 27.7s\tremaining: 1m 34s\n",
            "227:\tlearn: 0.0200630\ttotal: 27.8s\tremaining: 1m 33s\n",
            "228:\tlearn: 0.0199816\ttotal: 27.8s\tremaining: 1m 33s\n",
            "229:\tlearn: 0.0199523\ttotal: 27.9s\tremaining: 1m 33s\n",
            "230:\tlearn: 0.0198732\ttotal: 27.9s\tremaining: 1m 33s\n",
            "231:\tlearn: 0.0198370\ttotal: 28s\tremaining: 1m 32s\n",
            "232:\tlearn: 0.0197935\ttotal: 28.1s\tremaining: 1m 32s\n",
            "233:\tlearn: 0.0197509\ttotal: 28.1s\tremaining: 1m 32s\n",
            "234:\tlearn: 0.0197205\ttotal: 28.2s\tremaining: 1m 31s\n",
            "235:\tlearn: 0.0196893\ttotal: 28.2s\tremaining: 1m 31s\n",
            "236:\tlearn: 0.0196328\ttotal: 28.3s\tremaining: 1m 31s\n",
            "237:\tlearn: 0.0195798\ttotal: 28.3s\tremaining: 1m 30s\n",
            "238:\tlearn: 0.0195400\ttotal: 28.4s\tremaining: 1m 30s\n",
            "239:\tlearn: 0.0195005\ttotal: 28.5s\tremaining: 1m 30s\n",
            "240:\tlearn: 0.0194646\ttotal: 28.5s\tremaining: 1m 29s\n",
            "241:\tlearn: 0.0194326\ttotal: 28.6s\tremaining: 1m 29s\n",
            "242:\tlearn: 0.0193959\ttotal: 28.6s\tremaining: 1m 29s\n",
            "243:\tlearn: 0.0193394\ttotal: 28.7s\tremaining: 1m 28s\n",
            "244:\tlearn: 0.0192716\ttotal: 28.7s\tremaining: 1m 28s\n",
            "245:\tlearn: 0.0192414\ttotal: 28.8s\tremaining: 1m 28s\n",
            "246:\tlearn: 0.0191824\ttotal: 28.9s\tremaining: 1m 28s\n",
            "247:\tlearn: 0.0191385\ttotal: 28.9s\tremaining: 1m 27s\n",
            "248:\tlearn: 0.0190959\ttotal: 29s\tremaining: 1m 27s\n",
            "249:\tlearn: 0.0190412\ttotal: 29s\tremaining: 1m 27s\n",
            "250:\tlearn: 0.0189963\ttotal: 29.1s\tremaining: 1m 26s\n",
            "251:\tlearn: 0.0188751\ttotal: 29.2s\tremaining: 1m 26s\n",
            "252:\tlearn: 0.0188510\ttotal: 29.2s\tremaining: 1m 26s\n",
            "253:\tlearn: 0.0187763\ttotal: 29.3s\tremaining: 1m 25s\n",
            "254:\tlearn: 0.0187177\ttotal: 29.3s\tremaining: 1m 25s\n",
            "255:\tlearn: 0.0186666\ttotal: 29.4s\tremaining: 1m 25s\n",
            "256:\tlearn: 0.0186399\ttotal: 29.5s\tremaining: 1m 25s\n",
            "257:\tlearn: 0.0186268\ttotal: 29.5s\tremaining: 1m 24s\n",
            "258:\tlearn: 0.0185773\ttotal: 29.6s\tremaining: 1m 24s\n",
            "259:\tlearn: 0.0185566\ttotal: 29.6s\tremaining: 1m 24s\n",
            "260:\tlearn: 0.0185251\ttotal: 29.7s\tremaining: 1m 24s\n",
            "261:\tlearn: 0.0184887\ttotal: 29.7s\tremaining: 1m 23s\n",
            "262:\tlearn: 0.0184753\ttotal: 29.8s\tremaining: 1m 23s\n",
            "263:\tlearn: 0.0184212\ttotal: 29.9s\tremaining: 1m 23s\n",
            "264:\tlearn: 0.0183844\ttotal: 29.9s\tremaining: 1m 23s\n",
            "265:\tlearn: 0.0183407\ttotal: 30s\tremaining: 1m 22s\n",
            "266:\tlearn: 0.0182544\ttotal: 30s\tremaining: 1m 22s\n",
            "267:\tlearn: 0.0182149\ttotal: 30.1s\tremaining: 1m 22s\n",
            "268:\tlearn: 0.0181788\ttotal: 30.2s\tremaining: 1m 21s\n",
            "269:\tlearn: 0.0181453\ttotal: 30.2s\tremaining: 1m 21s\n",
            "270:\tlearn: 0.0181158\ttotal: 30.3s\tremaining: 1m 21s\n",
            "271:\tlearn: 0.0180828\ttotal: 30.3s\tremaining: 1m 21s\n",
            "272:\tlearn: 0.0180449\ttotal: 30.4s\tremaining: 1m 20s\n",
            "273:\tlearn: 0.0180008\ttotal: 30.4s\tremaining: 1m 20s\n",
            "274:\tlearn: 0.0179472\ttotal: 30.5s\tremaining: 1m 20s\n",
            "275:\tlearn: 0.0179146\ttotal: 30.6s\tremaining: 1m 20s\n",
            "276:\tlearn: 0.0178673\ttotal: 30.6s\tremaining: 1m 19s\n",
            "277:\tlearn: 0.0178437\ttotal: 30.7s\tremaining: 1m 19s\n",
            "278:\tlearn: 0.0178207\ttotal: 30.7s\tremaining: 1m 19s\n",
            "279:\tlearn: 0.0177774\ttotal: 30.8s\tremaining: 1m 19s\n",
            "280:\tlearn: 0.0177463\ttotal: 30.9s\tremaining: 1m 18s\n",
            "281:\tlearn: 0.0177078\ttotal: 30.9s\tremaining: 1m 18s\n",
            "282:\tlearn: 0.0176602\ttotal: 31s\tremaining: 1m 18s\n",
            "283:\tlearn: 0.0176204\ttotal: 31s\tremaining: 1m 18s\n",
            "284:\tlearn: 0.0175787\ttotal: 31.1s\tremaining: 1m 17s\n",
            "285:\tlearn: 0.0175295\ttotal: 31.1s\tremaining: 1m 17s\n",
            "286:\tlearn: 0.0175009\ttotal: 31.2s\tremaining: 1m 17s\n",
            "287:\tlearn: 0.0174660\ttotal: 31.3s\tremaining: 1m 17s\n",
            "288:\tlearn: 0.0174111\ttotal: 31.3s\tremaining: 1m 17s\n",
            "289:\tlearn: 0.0173761\ttotal: 31.4s\tremaining: 1m 16s\n",
            "290:\tlearn: 0.0173668\ttotal: 31.4s\tremaining: 1m 16s\n",
            "291:\tlearn: 0.0173251\ttotal: 31.5s\tremaining: 1m 16s\n",
            "292:\tlearn: 0.0172857\ttotal: 31.5s\tremaining: 1m 16s\n",
            "293:\tlearn: 0.0172620\ttotal: 31.6s\tremaining: 1m 15s\n",
            "294:\tlearn: 0.0172375\ttotal: 31.7s\tremaining: 1m 15s\n",
            "295:\tlearn: 0.0171447\ttotal: 31.7s\tremaining: 1m 15s\n",
            "296:\tlearn: 0.0171100\ttotal: 31.8s\tremaining: 1m 15s\n",
            "297:\tlearn: 0.0170603\ttotal: 31.8s\tremaining: 1m 14s\n",
            "298:\tlearn: 0.0170427\ttotal: 31.9s\tremaining: 1m 14s\n",
            "299:\tlearn: 0.0170057\ttotal: 31.9s\tremaining: 1m 14s\n",
            "300:\tlearn: 0.0169752\ttotal: 32s\tremaining: 1m 14s\n",
            "301:\tlearn: 0.0169535\ttotal: 32.1s\tremaining: 1m 14s\n",
            "302:\tlearn: 0.0169264\ttotal: 32.1s\tremaining: 1m 13s\n",
            "303:\tlearn: 0.0168957\ttotal: 32.2s\tremaining: 1m 13s\n",
            "304:\tlearn: 0.0168489\ttotal: 32.2s\tremaining: 1m 13s\n",
            "305:\tlearn: 0.0168047\ttotal: 32.3s\tremaining: 1m 13s\n",
            "306:\tlearn: 0.0167884\ttotal: 32.4s\tremaining: 1m 13s\n",
            "307:\tlearn: 0.0167723\ttotal: 32.4s\tremaining: 1m 12s\n",
            "308:\tlearn: 0.0167305\ttotal: 32.5s\tremaining: 1m 12s\n",
            "309:\tlearn: 0.0167147\ttotal: 32.5s\tremaining: 1m 12s\n",
            "310:\tlearn: 0.0166769\ttotal: 32.6s\tremaining: 1m 12s\n",
            "311:\tlearn: 0.0166313\ttotal: 32.6s\tremaining: 1m 11s\n",
            "312:\tlearn: 0.0165847\ttotal: 32.7s\tremaining: 1m 11s\n",
            "313:\tlearn: 0.0165393\ttotal: 32.7s\tremaining: 1m 11s\n",
            "314:\tlearn: 0.0164647\ttotal: 32.8s\tremaining: 1m 11s\n",
            "315:\tlearn: 0.0164488\ttotal: 32.9s\tremaining: 1m 11s\n",
            "316:\tlearn: 0.0164136\ttotal: 32.9s\tremaining: 1m 10s\n",
            "317:\tlearn: 0.0163875\ttotal: 33s\tremaining: 1m 10s\n",
            "318:\tlearn: 0.0163537\ttotal: 33.1s\tremaining: 1m 10s\n",
            "319:\tlearn: 0.0163171\ttotal: 33.1s\tremaining: 1m 10s\n",
            "320:\tlearn: 0.0162721\ttotal: 33.2s\tremaining: 1m 10s\n",
            "321:\tlearn: 0.0162314\ttotal: 33.2s\tremaining: 1m 9s\n",
            "322:\tlearn: 0.0161830\ttotal: 33.3s\tremaining: 1m 9s\n",
            "323:\tlearn: 0.0161648\ttotal: 33.4s\tremaining: 1m 9s\n",
            "324:\tlearn: 0.0161079\ttotal: 33.4s\tremaining: 1m 9s\n",
            "325:\tlearn: 0.0160791\ttotal: 33.5s\tremaining: 1m 9s\n",
            "326:\tlearn: 0.0160440\ttotal: 33.5s\tremaining: 1m 9s\n",
            "327:\tlearn: 0.0159588\ttotal: 33.6s\tremaining: 1m 8s\n",
            "328:\tlearn: 0.0159405\ttotal: 33.6s\tremaining: 1m 8s\n",
            "329:\tlearn: 0.0159148\ttotal: 33.7s\tremaining: 1m 8s\n",
            "330:\tlearn: 0.0158042\ttotal: 33.8s\tremaining: 1m 8s\n",
            "331:\tlearn: 0.0157519\ttotal: 33.8s\tremaining: 1m 8s\n",
            "332:\tlearn: 0.0157216\ttotal: 33.9s\tremaining: 1m 7s\n",
            "333:\tlearn: 0.0156740\ttotal: 34s\tremaining: 1m 7s\n",
            "334:\tlearn: 0.0156439\ttotal: 34s\tremaining: 1m 7s\n",
            "335:\tlearn: 0.0156175\ttotal: 34.1s\tremaining: 1m 7s\n",
            "336:\tlearn: 0.0155678\ttotal: 34.1s\tremaining: 1m 7s\n",
            "337:\tlearn: 0.0155364\ttotal: 34.2s\tremaining: 1m 6s\n",
            "338:\tlearn: 0.0155120\ttotal: 34.2s\tremaining: 1m 6s\n",
            "339:\tlearn: 0.0154878\ttotal: 34.3s\tremaining: 1m 6s\n",
            "340:\tlearn: 0.0154541\ttotal: 34.4s\tremaining: 1m 6s\n",
            "341:\tlearn: 0.0154029\ttotal: 34.4s\tremaining: 1m 6s\n",
            "342:\tlearn: 0.0153536\ttotal: 34.5s\tremaining: 1m 6s\n",
            "343:\tlearn: 0.0152874\ttotal: 34.5s\tremaining: 1m 5s\n",
            "344:\tlearn: 0.0152692\ttotal: 34.6s\tremaining: 1m 5s\n",
            "345:\tlearn: 0.0152479\ttotal: 34.6s\tremaining: 1m 5s\n",
            "346:\tlearn: 0.0152115\ttotal: 34.7s\tremaining: 1m 5s\n",
            "347:\tlearn: 0.0152009\ttotal: 34.8s\tremaining: 1m 5s\n",
            "348:\tlearn: 0.0151540\ttotal: 34.8s\tremaining: 1m 4s\n",
            "349:\tlearn: 0.0151330\ttotal: 34.9s\tremaining: 1m 4s\n",
            "350:\tlearn: 0.0150955\ttotal: 34.9s\tremaining: 1m 4s\n",
            "351:\tlearn: 0.0150762\ttotal: 35s\tremaining: 1m 4s\n",
            "352:\tlearn: 0.0150399\ttotal: 35.1s\tremaining: 1m 4s\n",
            "353:\tlearn: 0.0150088\ttotal: 35.1s\tremaining: 1m 4s\n",
            "354:\tlearn: 0.0150006\ttotal: 35.2s\tremaining: 1m 3s\n",
            "355:\tlearn: 0.0149848\ttotal: 35.2s\tremaining: 1m 3s\n",
            "356:\tlearn: 0.0149659\ttotal: 35.3s\tremaining: 1m 3s\n",
            "357:\tlearn: 0.0149429\ttotal: 35.3s\tremaining: 1m 3s\n",
            "358:\tlearn: 0.0149075\ttotal: 35.4s\tremaining: 1m 3s\n",
            "359:\tlearn: 0.0148440\ttotal: 35.5s\tremaining: 1m 3s\n",
            "360:\tlearn: 0.0148148\ttotal: 35.5s\tremaining: 1m 2s\n",
            "361:\tlearn: 0.0147994\ttotal: 35.6s\tremaining: 1m 2s\n",
            "362:\tlearn: 0.0147837\ttotal: 35.6s\tremaining: 1m 2s\n",
            "363:\tlearn: 0.0147687\ttotal: 35.7s\tremaining: 1m 2s\n",
            "364:\tlearn: 0.0147456\ttotal: 35.7s\tremaining: 1m 2s\n",
            "365:\tlearn: 0.0147095\ttotal: 35.8s\tremaining: 1m 2s\n",
            "366:\tlearn: 0.0146926\ttotal: 35.9s\tremaining: 1m 1s\n",
            "367:\tlearn: 0.0146616\ttotal: 35.9s\tremaining: 1m 1s\n",
            "368:\tlearn: 0.0146314\ttotal: 36s\tremaining: 1m 1s\n",
            "369:\tlearn: 0.0145953\ttotal: 36s\tremaining: 1m 1s\n",
            "370:\tlearn: 0.0145822\ttotal: 36.1s\tremaining: 1m 1s\n",
            "371:\tlearn: 0.0145214\ttotal: 36.2s\tremaining: 1m 1s\n",
            "372:\tlearn: 0.0144720\ttotal: 36.2s\tremaining: 1m\n",
            "373:\tlearn: 0.0144296\ttotal: 36.3s\tremaining: 1m\n",
            "374:\tlearn: 0.0144039\ttotal: 36.3s\tremaining: 1m\n",
            "375:\tlearn: 0.0143418\ttotal: 36.4s\tremaining: 1m\n",
            "376:\tlearn: 0.0143142\ttotal: 36.4s\tremaining: 1m\n",
            "377:\tlearn: 0.0142843\ttotal: 36.5s\tremaining: 1m\n",
            "378:\tlearn: 0.0142712\ttotal: 36.5s\tremaining: 59.9s\n",
            "379:\tlearn: 0.0142438\ttotal: 36.6s\tremaining: 59.7s\n",
            "380:\tlearn: 0.0142084\ttotal: 36.7s\tremaining: 59.6s\n",
            "381:\tlearn: 0.0141961\ttotal: 36.7s\tremaining: 59.4s\n",
            "382:\tlearn: 0.0141731\ttotal: 36.8s\tremaining: 59.3s\n",
            "383:\tlearn: 0.0141554\ttotal: 36.8s\tremaining: 59.1s\n",
            "384:\tlearn: 0.0141406\ttotal: 36.9s\tremaining: 58.9s\n",
            "385:\tlearn: 0.0141203\ttotal: 36.9s\tremaining: 58.8s\n",
            "386:\tlearn: 0.0140599\ttotal: 37s\tremaining: 58.6s\n",
            "387:\tlearn: 0.0140336\ttotal: 37.1s\tremaining: 58.5s\n",
            "388:\tlearn: 0.0140240\ttotal: 37.1s\tremaining: 58.3s\n",
            "389:\tlearn: 0.0139817\ttotal: 37.2s\tremaining: 58.2s\n",
            "390:\tlearn: 0.0139743\ttotal: 37.3s\tremaining: 58s\n",
            "391:\tlearn: 0.0139461\ttotal: 37.3s\tremaining: 57.9s\n",
            "392:\tlearn: 0.0139253\ttotal: 37.4s\tremaining: 57.7s\n",
            "393:\tlearn: 0.0139119\ttotal: 37.4s\tremaining: 57.6s\n",
            "394:\tlearn: 0.0138725\ttotal: 37.5s\tremaining: 57.4s\n",
            "395:\tlearn: 0.0138256\ttotal: 37.5s\tremaining: 57.3s\n",
            "396:\tlearn: 0.0137771\ttotal: 37.6s\tremaining: 57.1s\n",
            "397:\tlearn: 0.0137500\ttotal: 37.6s\tremaining: 56.9s\n",
            "398:\tlearn: 0.0137251\ttotal: 37.7s\tremaining: 56.8s\n",
            "399:\tlearn: 0.0137173\ttotal: 37.8s\tremaining: 56.6s\n",
            "400:\tlearn: 0.0136893\ttotal: 37.8s\tremaining: 56.5s\n",
            "401:\tlearn: 0.0136620\ttotal: 37.9s\tremaining: 56.3s\n",
            "402:\tlearn: 0.0136545\ttotal: 37.9s\tremaining: 56.2s\n",
            "403:\tlearn: 0.0136480\ttotal: 38s\tremaining: 56.1s\n",
            "404:\tlearn: 0.0136177\ttotal: 38.1s\tremaining: 55.9s\n",
            "405:\tlearn: 0.0135778\ttotal: 38.1s\tremaining: 55.8s\n",
            "406:\tlearn: 0.0135617\ttotal: 38.2s\tremaining: 55.6s\n",
            "407:\tlearn: 0.0135249\ttotal: 38.2s\tremaining: 55.5s\n",
            "408:\tlearn: 0.0135012\ttotal: 38.3s\tremaining: 55.3s\n",
            "409:\tlearn: 0.0134722\ttotal: 38.3s\tremaining: 55.2s\n",
            "410:\tlearn: 0.0134414\ttotal: 38.4s\tremaining: 55s\n",
            "411:\tlearn: 0.0134352\ttotal: 38.5s\tremaining: 54.9s\n",
            "412:\tlearn: 0.0134004\ttotal: 38.5s\tremaining: 54.7s\n",
            "413:\tlearn: 0.0133402\ttotal: 38.6s\tremaining: 54.6s\n",
            "414:\tlearn: 0.0133132\ttotal: 38.6s\tremaining: 54.5s\n",
            "415:\tlearn: 0.0132707\ttotal: 38.7s\tremaining: 54.3s\n",
            "416:\tlearn: 0.0132538\ttotal: 38.7s\tremaining: 54.2s\n",
            "417:\tlearn: 0.0132479\ttotal: 38.8s\tremaining: 54s\n",
            "418:\tlearn: 0.0132369\ttotal: 38.9s\tremaining: 53.9s\n",
            "419:\tlearn: 0.0131968\ttotal: 38.9s\tremaining: 53.7s\n",
            "420:\tlearn: 0.0131601\ttotal: 39s\tremaining: 53.6s\n",
            "421:\tlearn: 0.0131366\ttotal: 39s\tremaining: 53.5s\n",
            "422:\tlearn: 0.0131100\ttotal: 39.1s\tremaining: 53.3s\n",
            "423:\tlearn: 0.0130985\ttotal: 39.2s\tremaining: 53.2s\n",
            "424:\tlearn: 0.0130702\ttotal: 39.2s\tremaining: 53.1s\n",
            "425:\tlearn: 0.0130619\ttotal: 39.3s\tremaining: 52.9s\n",
            "426:\tlearn: 0.0130535\ttotal: 39.3s\tremaining: 52.8s\n",
            "427:\tlearn: 0.0130111\ttotal: 39.4s\tremaining: 52.6s\n",
            "428:\tlearn: 0.0129865\ttotal: 39.4s\tremaining: 52.5s\n",
            "429:\tlearn: 0.0129554\ttotal: 39.5s\tremaining: 52.4s\n",
            "430:\tlearn: 0.0129150\ttotal: 39.6s\tremaining: 52.2s\n",
            "431:\tlearn: 0.0128841\ttotal: 39.6s\tremaining: 52.1s\n",
            "432:\tlearn: 0.0128519\ttotal: 39.7s\tremaining: 51.9s\n",
            "433:\tlearn: 0.0128427\ttotal: 39.7s\tremaining: 51.8s\n",
            "434:\tlearn: 0.0128261\ttotal: 39.8s\tremaining: 51.7s\n",
            "435:\tlearn: 0.0128037\ttotal: 39.8s\tremaining: 51.5s\n",
            "436:\tlearn: 0.0127923\ttotal: 39.9s\tremaining: 51.4s\n",
            "437:\tlearn: 0.0127726\ttotal: 40s\tremaining: 51.3s\n",
            "438:\tlearn: 0.0127478\ttotal: 40s\tremaining: 51.1s\n",
            "439:\tlearn: 0.0127319\ttotal: 40.1s\tremaining: 51s\n",
            "440:\tlearn: 0.0127090\ttotal: 40.1s\tremaining: 50.9s\n",
            "441:\tlearn: 0.0127001\ttotal: 40.2s\tremaining: 50.7s\n",
            "442:\tlearn: 0.0126713\ttotal: 40.3s\tremaining: 50.6s\n",
            "443:\tlearn: 0.0126473\ttotal: 40.3s\tremaining: 50.5s\n",
            "444:\tlearn: 0.0126131\ttotal: 40.4s\tremaining: 50.3s\n",
            "445:\tlearn: 0.0126030\ttotal: 40.4s\tremaining: 50.2s\n",
            "446:\tlearn: 0.0125844\ttotal: 40.5s\tremaining: 50.1s\n",
            "447:\tlearn: 0.0125449\ttotal: 40.5s\tremaining: 49.9s\n",
            "448:\tlearn: 0.0125103\ttotal: 40.6s\tremaining: 49.8s\n",
            "449:\tlearn: 0.0124934\ttotal: 40.7s\tremaining: 49.7s\n",
            "450:\tlearn: 0.0124576\ttotal: 40.7s\tremaining: 49.6s\n",
            "451:\tlearn: 0.0124164\ttotal: 40.8s\tremaining: 49.4s\n",
            "452:\tlearn: 0.0123924\ttotal: 40.8s\tremaining: 49.3s\n",
            "453:\tlearn: 0.0123876\ttotal: 40.9s\tremaining: 49.2s\n",
            "454:\tlearn: 0.0123711\ttotal: 40.9s\tremaining: 49s\n",
            "455:\tlearn: 0.0123472\ttotal: 41s\tremaining: 48.9s\n",
            "456:\tlearn: 0.0123197\ttotal: 41.1s\tremaining: 48.8s\n",
            "457:\tlearn: 0.0123145\ttotal: 41.1s\tremaining: 48.7s\n",
            "458:\tlearn: 0.0122831\ttotal: 41.2s\tremaining: 48.5s\n",
            "459:\tlearn: 0.0122547\ttotal: 41.2s\tremaining: 48.4s\n",
            "460:\tlearn: 0.0122191\ttotal: 41.3s\tremaining: 48.3s\n",
            "461:\tlearn: 0.0121747\ttotal: 41.4s\tremaining: 48.2s\n",
            "462:\tlearn: 0.0121615\ttotal: 41.4s\tremaining: 48.1s\n",
            "463:\tlearn: 0.0121391\ttotal: 41.5s\tremaining: 47.9s\n",
            "464:\tlearn: 0.0121107\ttotal: 41.5s\tremaining: 47.8s\n",
            "465:\tlearn: 0.0121020\ttotal: 41.6s\tremaining: 47.7s\n",
            "466:\tlearn: 0.0120808\ttotal: 41.7s\tremaining: 47.5s\n",
            "467:\tlearn: 0.0120591\ttotal: 41.7s\tremaining: 47.4s\n",
            "468:\tlearn: 0.0120506\ttotal: 41.8s\tremaining: 47.3s\n",
            "469:\tlearn: 0.0120355\ttotal: 41.8s\tremaining: 47.2s\n",
            "470:\tlearn: 0.0120144\ttotal: 41.9s\tremaining: 47.1s\n",
            "471:\tlearn: 0.0119953\ttotal: 42s\tremaining: 46.9s\n",
            "472:\tlearn: 0.0119711\ttotal: 42s\tremaining: 46.8s\n",
            "473:\tlearn: 0.0119378\ttotal: 42.1s\tremaining: 46.7s\n",
            "474:\tlearn: 0.0119321\ttotal: 42.1s\tremaining: 46.6s\n",
            "475:\tlearn: 0.0118946\ttotal: 42.2s\tremaining: 46.5s\n",
            "476:\tlearn: 0.0118640\ttotal: 42.3s\tremaining: 46.3s\n",
            "477:\tlearn: 0.0118440\ttotal: 42.3s\tremaining: 46.2s\n",
            "478:\tlearn: 0.0118292\ttotal: 42.4s\tremaining: 46.1s\n",
            "479:\tlearn: 0.0118096\ttotal: 42.4s\tremaining: 46s\n",
            "480:\tlearn: 0.0117767\ttotal: 42.5s\tremaining: 45.8s\n",
            "481:\tlearn: 0.0117678\ttotal: 42.5s\tremaining: 45.7s\n",
            "482:\tlearn: 0.0117530\ttotal: 42.6s\tremaining: 45.6s\n",
            "483:\tlearn: 0.0117295\ttotal: 42.7s\tremaining: 45.5s\n",
            "484:\tlearn: 0.0117130\ttotal: 42.7s\tremaining: 45.4s\n",
            "485:\tlearn: 0.0116877\ttotal: 42.8s\tremaining: 45.2s\n",
            "486:\tlearn: 0.0116825\ttotal: 42.8s\tremaining: 45.1s\n",
            "487:\tlearn: 0.0116674\ttotal: 42.9s\tremaining: 45s\n",
            "488:\tlearn: 0.0116503\ttotal: 42.9s\tremaining: 44.9s\n",
            "489:\tlearn: 0.0116361\ttotal: 43s\tremaining: 44.8s\n",
            "490:\tlearn: 0.0116176\ttotal: 43.1s\tremaining: 44.6s\n",
            "491:\tlearn: 0.0116045\ttotal: 43.1s\tremaining: 44.5s\n",
            "492:\tlearn: 0.0115762\ttotal: 43.2s\tremaining: 44.4s\n",
            "493:\tlearn: 0.0115505\ttotal: 43.2s\tremaining: 44.3s\n",
            "494:\tlearn: 0.0115343\ttotal: 43.3s\tremaining: 44.2s\n",
            "495:\tlearn: 0.0115254\ttotal: 43.4s\tremaining: 44.1s\n",
            "496:\tlearn: 0.0115003\ttotal: 43.4s\tremaining: 44s\n",
            "497:\tlearn: 0.0114708\ttotal: 43.5s\tremaining: 43.9s\n",
            "498:\tlearn: 0.0114579\ttotal: 43.6s\tremaining: 43.7s\n",
            "499:\tlearn: 0.0114394\ttotal: 43.6s\tremaining: 43.6s\n",
            "500:\tlearn: 0.0114183\ttotal: 43.7s\tremaining: 43.5s\n",
            "501:\tlearn: 0.0113999\ttotal: 43.7s\tremaining: 43.4s\n",
            "502:\tlearn: 0.0113917\ttotal: 43.8s\tremaining: 43.3s\n",
            "503:\tlearn: 0.0113773\ttotal: 43.8s\tremaining: 43.1s\n",
            "504:\tlearn: 0.0113604\ttotal: 43.9s\tremaining: 43s\n",
            "505:\tlearn: 0.0113317\ttotal: 44s\tremaining: 42.9s\n",
            "506:\tlearn: 0.0113126\ttotal: 44s\tremaining: 42.8s\n",
            "507:\tlearn: 0.0112964\ttotal: 44.1s\tremaining: 42.7s\n",
            "508:\tlearn: 0.0112834\ttotal: 44.1s\tremaining: 42.6s\n",
            "509:\tlearn: 0.0112679\ttotal: 44.2s\tremaining: 42.5s\n",
            "510:\tlearn: 0.0112515\ttotal: 44.3s\tremaining: 42.4s\n",
            "511:\tlearn: 0.0112432\ttotal: 44.3s\tremaining: 42.2s\n",
            "512:\tlearn: 0.0112249\ttotal: 44.4s\tremaining: 42.1s\n",
            "513:\tlearn: 0.0111977\ttotal: 44.4s\tremaining: 42s\n",
            "514:\tlearn: 0.0111855\ttotal: 44.5s\tremaining: 41.9s\n",
            "515:\tlearn: 0.0111713\ttotal: 44.5s\tremaining: 41.8s\n",
            "516:\tlearn: 0.0111577\ttotal: 44.6s\tremaining: 41.7s\n",
            "517:\tlearn: 0.0111271\ttotal: 44.7s\tremaining: 41.6s\n",
            "518:\tlearn: 0.0111112\ttotal: 44.7s\tremaining: 41.5s\n",
            "519:\tlearn: 0.0110912\ttotal: 44.8s\tremaining: 41.3s\n",
            "520:\tlearn: 0.0110769\ttotal: 44.8s\tremaining: 41.2s\n",
            "521:\tlearn: 0.0110508\ttotal: 44.9s\tremaining: 41.1s\n",
            "522:\tlearn: 0.0110362\ttotal: 45s\tremaining: 41s\n",
            "523:\tlearn: 0.0110267\ttotal: 45s\tremaining: 40.9s\n",
            "524:\tlearn: 0.0110110\ttotal: 45.1s\tremaining: 40.8s\n",
            "525:\tlearn: 0.0109981\ttotal: 45.2s\tremaining: 40.7s\n",
            "526:\tlearn: 0.0109841\ttotal: 45.2s\tremaining: 40.6s\n",
            "527:\tlearn: 0.0109725\ttotal: 45.3s\tremaining: 40.5s\n",
            "528:\tlearn: 0.0109590\ttotal: 45.3s\tremaining: 40.4s\n",
            "529:\tlearn: 0.0109464\ttotal: 45.4s\tremaining: 40.2s\n",
            "530:\tlearn: 0.0109207\ttotal: 45.4s\tremaining: 40.1s\n",
            "531:\tlearn: 0.0109063\ttotal: 45.5s\tremaining: 40s\n",
            "532:\tlearn: 0.0108974\ttotal: 45.5s\tremaining: 39.9s\n",
            "533:\tlearn: 0.0108800\ttotal: 45.6s\tremaining: 39.8s\n",
            "534:\tlearn: 0.0108670\ttotal: 45.7s\tremaining: 39.7s\n",
            "535:\tlearn: 0.0108585\ttotal: 45.7s\tremaining: 39.6s\n",
            "536:\tlearn: 0.0108521\ttotal: 45.8s\tremaining: 39.5s\n",
            "537:\tlearn: 0.0108313\ttotal: 45.8s\tremaining: 39.4s\n",
            "538:\tlearn: 0.0107962\ttotal: 45.9s\tremaining: 39.3s\n",
            "539:\tlearn: 0.0107722\ttotal: 46s\tremaining: 39.1s\n",
            "540:\tlearn: 0.0107611\ttotal: 46s\tremaining: 39s\n",
            "541:\tlearn: 0.0107300\ttotal: 46.1s\tremaining: 38.9s\n",
            "542:\tlearn: 0.0107160\ttotal: 46.1s\tremaining: 38.8s\n",
            "543:\tlearn: 0.0106978\ttotal: 46.2s\tremaining: 38.7s\n",
            "544:\tlearn: 0.0106825\ttotal: 46.3s\tremaining: 38.6s\n",
            "545:\tlearn: 0.0106595\ttotal: 46.3s\tremaining: 38.5s\n",
            "546:\tlearn: 0.0106276\ttotal: 46.4s\tremaining: 38.4s\n",
            "547:\tlearn: 0.0105960\ttotal: 46.4s\tremaining: 38.3s\n",
            "548:\tlearn: 0.0105831\ttotal: 46.5s\tremaining: 38.2s\n",
            "549:\tlearn: 0.0105559\ttotal: 46.5s\tremaining: 38.1s\n",
            "550:\tlearn: 0.0105443\ttotal: 46.6s\tremaining: 38s\n",
            "551:\tlearn: 0.0105301\ttotal: 46.7s\tremaining: 37.9s\n",
            "552:\tlearn: 0.0105124\ttotal: 46.7s\tremaining: 37.8s\n",
            "553:\tlearn: 0.0105022\ttotal: 46.8s\tremaining: 37.7s\n",
            "554:\tlearn: 0.0104807\ttotal: 46.8s\tremaining: 37.6s\n",
            "555:\tlearn: 0.0104580\ttotal: 46.9s\tremaining: 37.4s\n",
            "556:\tlearn: 0.0104503\ttotal: 47s\tremaining: 37.3s\n",
            "557:\tlearn: 0.0104200\ttotal: 47s\tremaining: 37.2s\n",
            "558:\tlearn: 0.0103989\ttotal: 47.1s\tremaining: 37.1s\n",
            "559:\tlearn: 0.0103754\ttotal: 47.1s\tremaining: 37s\n",
            "560:\tlearn: 0.0103655\ttotal: 47.2s\tremaining: 36.9s\n",
            "561:\tlearn: 0.0103430\ttotal: 47.3s\tremaining: 36.8s\n",
            "562:\tlearn: 0.0103375\ttotal: 47.3s\tremaining: 36.7s\n",
            "563:\tlearn: 0.0103067\ttotal: 47.4s\tremaining: 36.6s\n",
            "564:\tlearn: 0.0102886\ttotal: 47.4s\tremaining: 36.5s\n",
            "565:\tlearn: 0.0102612\ttotal: 47.5s\tremaining: 36.4s\n",
            "566:\tlearn: 0.0102463\ttotal: 47.5s\tremaining: 36.3s\n",
            "567:\tlearn: 0.0102273\ttotal: 47.6s\tremaining: 36.2s\n",
            "568:\tlearn: 0.0101992\ttotal: 47.7s\tremaining: 36.1s\n",
            "569:\tlearn: 0.0101741\ttotal: 47.7s\tremaining: 36s\n",
            "570:\tlearn: 0.0101617\ttotal: 47.8s\tremaining: 35.9s\n",
            "571:\tlearn: 0.0101511\ttotal: 47.8s\tremaining: 35.8s\n",
            "572:\tlearn: 0.0101325\ttotal: 47.9s\tremaining: 35.7s\n",
            "573:\tlearn: 0.0101109\ttotal: 47.9s\tremaining: 35.6s\n",
            "574:\tlearn: 0.0100959\ttotal: 48s\tremaining: 35.5s\n",
            "575:\tlearn: 0.0100794\ttotal: 48.1s\tremaining: 35.4s\n",
            "576:\tlearn: 0.0100644\ttotal: 48.1s\tremaining: 35.3s\n",
            "577:\tlearn: 0.0100543\ttotal: 48.2s\tremaining: 35.2s\n",
            "578:\tlearn: 0.0100464\ttotal: 48.2s\tremaining: 35.1s\n",
            "579:\tlearn: 0.0100313\ttotal: 48.3s\tremaining: 35s\n",
            "580:\tlearn: 0.0100221\ttotal: 48.4s\tremaining: 34.9s\n",
            "581:\tlearn: 0.0100125\ttotal: 48.4s\tremaining: 34.8s\n",
            "582:\tlearn: 0.0099892\ttotal: 48.5s\tremaining: 34.7s\n",
            "583:\tlearn: 0.0099746\ttotal: 48.5s\tremaining: 34.6s\n",
            "584:\tlearn: 0.0099585\ttotal: 48.6s\tremaining: 34.5s\n",
            "585:\tlearn: 0.0099475\ttotal: 48.6s\tremaining: 34.4s\n",
            "586:\tlearn: 0.0099340\ttotal: 48.7s\tremaining: 34.3s\n",
            "587:\tlearn: 0.0099273\ttotal: 48.8s\tremaining: 34.2s\n",
            "588:\tlearn: 0.0099132\ttotal: 48.8s\tremaining: 34.1s\n",
            "589:\tlearn: 0.0098934\ttotal: 48.9s\tremaining: 34s\n",
            "590:\tlearn: 0.0098744\ttotal: 48.9s\tremaining: 33.9s\n",
            "591:\tlearn: 0.0098652\ttotal: 49s\tremaining: 33.8s\n",
            "592:\tlearn: 0.0098453\ttotal: 49s\tremaining: 33.7s\n",
            "593:\tlearn: 0.0098340\ttotal: 49.1s\tremaining: 33.6s\n",
            "594:\tlearn: 0.0098179\ttotal: 49.2s\tremaining: 33.5s\n",
            "595:\tlearn: 0.0097908\ttotal: 49.2s\tremaining: 33.4s\n",
            "596:\tlearn: 0.0097773\ttotal: 49.3s\tremaining: 33.3s\n",
            "597:\tlearn: 0.0097376\ttotal: 49.3s\tremaining: 33.2s\n",
            "598:\tlearn: 0.0097251\ttotal: 49.4s\tremaining: 33.1s\n",
            "599:\tlearn: 0.0097116\ttotal: 49.5s\tremaining: 33s\n",
            "600:\tlearn: 0.0096973\ttotal: 49.5s\tremaining: 32.9s\n",
            "601:\tlearn: 0.0096910\ttotal: 49.6s\tremaining: 32.8s\n",
            "602:\tlearn: 0.0096730\ttotal: 49.6s\tremaining: 32.7s\n",
            "603:\tlearn: 0.0096583\ttotal: 49.7s\tremaining: 32.6s\n",
            "604:\tlearn: 0.0096353\ttotal: 49.7s\tremaining: 32.5s\n",
            "605:\tlearn: 0.0096230\ttotal: 49.8s\tremaining: 32.4s\n",
            "606:\tlearn: 0.0096023\ttotal: 49.9s\tremaining: 32.3s\n",
            "607:\tlearn: 0.0095840\ttotal: 49.9s\tremaining: 32.2s\n",
            "608:\tlearn: 0.0095601\ttotal: 50s\tremaining: 32.1s\n",
            "609:\tlearn: 0.0095437\ttotal: 50s\tremaining: 32s\n",
            "610:\tlearn: 0.0095355\ttotal: 50.1s\tremaining: 31.9s\n",
            "611:\tlearn: 0.0095100\ttotal: 50.1s\tremaining: 31.8s\n",
            "612:\tlearn: 0.0094910\ttotal: 50.2s\tremaining: 31.7s\n",
            "613:\tlearn: 0.0094681\ttotal: 50.3s\tremaining: 31.6s\n",
            "614:\tlearn: 0.0094456\ttotal: 50.3s\tremaining: 31.5s\n",
            "615:\tlearn: 0.0094330\ttotal: 50.4s\tremaining: 31.4s\n",
            "616:\tlearn: 0.0094251\ttotal: 50.4s\tremaining: 31.3s\n",
            "617:\tlearn: 0.0094007\ttotal: 50.5s\tremaining: 31.2s\n",
            "618:\tlearn: 0.0093931\ttotal: 50.6s\tremaining: 31.1s\n",
            "619:\tlearn: 0.0093810\ttotal: 50.6s\tremaining: 31s\n",
            "620:\tlearn: 0.0093673\ttotal: 50.7s\tremaining: 30.9s\n",
            "621:\tlearn: 0.0093585\ttotal: 50.7s\tremaining: 30.8s\n",
            "622:\tlearn: 0.0093518\ttotal: 50.8s\tremaining: 30.7s\n",
            "623:\tlearn: 0.0093446\ttotal: 50.8s\tremaining: 30.6s\n",
            "624:\tlearn: 0.0093340\ttotal: 50.9s\tremaining: 30.5s\n",
            "625:\tlearn: 0.0093127\ttotal: 51s\tremaining: 30.4s\n",
            "626:\tlearn: 0.0092957\ttotal: 51s\tremaining: 30.4s\n",
            "627:\tlearn: 0.0092841\ttotal: 51.1s\tremaining: 30.3s\n",
            "628:\tlearn: 0.0092754\ttotal: 51.1s\tremaining: 30.2s\n",
            "629:\tlearn: 0.0092524\ttotal: 51.2s\tremaining: 30.1s\n",
            "630:\tlearn: 0.0092424\ttotal: 51.3s\tremaining: 30s\n",
            "631:\tlearn: 0.0092298\ttotal: 51.3s\tremaining: 29.9s\n",
            "632:\tlearn: 0.0092169\ttotal: 51.4s\tremaining: 29.8s\n",
            "633:\tlearn: 0.0092010\ttotal: 51.4s\tremaining: 29.7s\n",
            "634:\tlearn: 0.0091803\ttotal: 51.5s\tremaining: 29.6s\n",
            "635:\tlearn: 0.0091678\ttotal: 51.5s\tremaining: 29.5s\n",
            "636:\tlearn: 0.0091451\ttotal: 51.6s\tremaining: 29.4s\n",
            "637:\tlearn: 0.0091390\ttotal: 51.7s\tremaining: 29.3s\n",
            "638:\tlearn: 0.0091265\ttotal: 51.7s\tremaining: 29.2s\n",
            "639:\tlearn: 0.0090973\ttotal: 51.8s\tremaining: 29.1s\n",
            "640:\tlearn: 0.0090782\ttotal: 51.8s\tremaining: 29s\n",
            "641:\tlearn: 0.0090712\ttotal: 51.9s\tremaining: 28.9s\n",
            "642:\tlearn: 0.0090630\ttotal: 52s\tremaining: 28.8s\n",
            "643:\tlearn: 0.0090475\ttotal: 52s\tremaining: 28.8s\n",
            "644:\tlearn: 0.0090289\ttotal: 52.1s\tremaining: 28.7s\n",
            "645:\tlearn: 0.0090103\ttotal: 52.1s\tremaining: 28.6s\n",
            "646:\tlearn: 0.0090026\ttotal: 52.2s\tremaining: 28.5s\n",
            "647:\tlearn: 0.0089871\ttotal: 52.3s\tremaining: 28.4s\n",
            "648:\tlearn: 0.0089804\ttotal: 52.3s\tremaining: 28.3s\n",
            "649:\tlearn: 0.0089730\ttotal: 52.4s\tremaining: 28.2s\n",
            "650:\tlearn: 0.0089662\ttotal: 52.4s\tremaining: 28.1s\n",
            "651:\tlearn: 0.0089501\ttotal: 52.5s\tremaining: 28s\n",
            "652:\tlearn: 0.0089255\ttotal: 52.5s\tremaining: 27.9s\n",
            "653:\tlearn: 0.0089014\ttotal: 52.6s\tremaining: 27.8s\n",
            "654:\tlearn: 0.0088917\ttotal: 52.7s\tremaining: 27.7s\n",
            "655:\tlearn: 0.0088737\ttotal: 52.7s\tremaining: 27.6s\n",
            "656:\tlearn: 0.0088583\ttotal: 52.8s\tremaining: 27.5s\n",
            "657:\tlearn: 0.0088425\ttotal: 52.8s\tremaining: 27.5s\n",
            "658:\tlearn: 0.0087848\ttotal: 52.9s\tremaining: 27.4s\n",
            "659:\tlearn: 0.0087749\ttotal: 52.9s\tremaining: 27.3s\n",
            "660:\tlearn: 0.0087649\ttotal: 53s\tremaining: 27.2s\n",
            "661:\tlearn: 0.0087451\ttotal: 53.1s\tremaining: 27.1s\n",
            "662:\tlearn: 0.0087374\ttotal: 53.1s\tremaining: 27s\n",
            "663:\tlearn: 0.0087271\ttotal: 53.2s\tremaining: 26.9s\n",
            "664:\tlearn: 0.0087177\ttotal: 53.2s\tremaining: 26.8s\n",
            "665:\tlearn: 0.0086971\ttotal: 53.3s\tremaining: 26.7s\n",
            "666:\tlearn: 0.0086879\ttotal: 53.4s\tremaining: 26.6s\n",
            "667:\tlearn: 0.0086708\ttotal: 53.4s\tremaining: 26.6s\n",
            "668:\tlearn: 0.0086573\ttotal: 53.5s\tremaining: 26.5s\n",
            "669:\tlearn: 0.0086501\ttotal: 53.6s\tremaining: 26.4s\n",
            "670:\tlearn: 0.0086460\ttotal: 53.6s\tremaining: 26.3s\n",
            "671:\tlearn: 0.0086295\ttotal: 53.7s\tremaining: 26.2s\n",
            "672:\tlearn: 0.0086227\ttotal: 53.7s\tremaining: 26.1s\n",
            "673:\tlearn: 0.0085994\ttotal: 53.8s\tremaining: 26s\n",
            "674:\tlearn: 0.0085817\ttotal: 53.8s\tremaining: 25.9s\n",
            "675:\tlearn: 0.0085744\ttotal: 53.9s\tremaining: 25.8s\n",
            "676:\tlearn: 0.0085654\ttotal: 53.9s\tremaining: 25.7s\n",
            "677:\tlearn: 0.0085565\ttotal: 54s\tremaining: 25.6s\n",
            "678:\tlearn: 0.0085494\ttotal: 54.1s\tremaining: 25.6s\n",
            "679:\tlearn: 0.0085261\ttotal: 54.1s\tremaining: 25.5s\n",
            "680:\tlearn: 0.0085092\ttotal: 54.2s\tremaining: 25.4s\n",
            "681:\tlearn: 0.0084959\ttotal: 54.3s\tremaining: 25.3s\n",
            "682:\tlearn: 0.0084873\ttotal: 54.3s\tremaining: 25.2s\n",
            "683:\tlearn: 0.0084639\ttotal: 54.4s\tremaining: 25.1s\n",
            "684:\tlearn: 0.0084542\ttotal: 54.4s\tremaining: 25s\n",
            "685:\tlearn: 0.0084414\ttotal: 54.5s\tremaining: 24.9s\n",
            "686:\tlearn: 0.0084332\ttotal: 54.5s\tremaining: 24.9s\n",
            "687:\tlearn: 0.0084223\ttotal: 54.6s\tremaining: 24.8s\n",
            "688:\tlearn: 0.0084155\ttotal: 54.7s\tremaining: 24.7s\n",
            "689:\tlearn: 0.0084006\ttotal: 54.7s\tremaining: 24.6s\n",
            "690:\tlearn: 0.0083787\ttotal: 54.8s\tremaining: 24.5s\n",
            "691:\tlearn: 0.0083761\ttotal: 54.8s\tremaining: 24.4s\n",
            "692:\tlearn: 0.0083681\ttotal: 54.9s\tremaining: 24.3s\n",
            "693:\tlearn: 0.0083470\ttotal: 55s\tremaining: 24.2s\n",
            "694:\tlearn: 0.0083315\ttotal: 55s\tremaining: 24.1s\n",
            "695:\tlearn: 0.0083236\ttotal: 55.1s\tremaining: 24.1s\n",
            "696:\tlearn: 0.0083150\ttotal: 55.1s\tremaining: 24s\n",
            "697:\tlearn: 0.0083096\ttotal: 55.2s\tremaining: 23.9s\n",
            "698:\tlearn: 0.0082946\ttotal: 55.2s\tremaining: 23.8s\n",
            "699:\tlearn: 0.0082869\ttotal: 55.3s\tremaining: 23.7s\n",
            "700:\tlearn: 0.0082772\ttotal: 55.4s\tremaining: 23.6s\n",
            "701:\tlearn: 0.0082569\ttotal: 55.4s\tremaining: 23.5s\n",
            "702:\tlearn: 0.0082489\ttotal: 55.5s\tremaining: 23.4s\n",
            "703:\tlearn: 0.0082428\ttotal: 55.5s\tremaining: 23.4s\n",
            "704:\tlearn: 0.0082062\ttotal: 55.6s\tremaining: 23.3s\n",
            "705:\tlearn: 0.0081997\ttotal: 55.7s\tremaining: 23.2s\n",
            "706:\tlearn: 0.0081938\ttotal: 55.7s\tremaining: 23.1s\n",
            "707:\tlearn: 0.0081863\ttotal: 55.8s\tremaining: 23s\n",
            "708:\tlearn: 0.0081708\ttotal: 55.8s\tremaining: 22.9s\n",
            "709:\tlearn: 0.0081645\ttotal: 55.9s\tremaining: 22.8s\n",
            "710:\tlearn: 0.0081608\ttotal: 55.9s\tremaining: 22.7s\n",
            "711:\tlearn: 0.0081528\ttotal: 56s\tremaining: 22.6s\n",
            "712:\tlearn: 0.0081462\ttotal: 56s\tremaining: 22.6s\n",
            "713:\tlearn: 0.0081290\ttotal: 56.1s\tremaining: 22.5s\n",
            "714:\tlearn: 0.0081219\ttotal: 56.2s\tremaining: 22.4s\n",
            "715:\tlearn: 0.0081159\ttotal: 56.2s\tremaining: 22.3s\n",
            "716:\tlearn: 0.0080965\ttotal: 56.3s\tremaining: 22.2s\n",
            "717:\tlearn: 0.0080841\ttotal: 56.4s\tremaining: 22.1s\n",
            "718:\tlearn: 0.0080678\ttotal: 56.4s\tremaining: 22.1s\n",
            "719:\tlearn: 0.0080617\ttotal: 56.5s\tremaining: 22s\n",
            "720:\tlearn: 0.0080470\ttotal: 56.5s\tremaining: 21.9s\n",
            "721:\tlearn: 0.0080320\ttotal: 56.6s\tremaining: 21.8s\n",
            "722:\tlearn: 0.0080187\ttotal: 56.6s\tremaining: 21.7s\n",
            "723:\tlearn: 0.0080110\ttotal: 56.7s\tremaining: 21.6s\n",
            "724:\tlearn: 0.0080057\ttotal: 56.8s\tremaining: 21.5s\n",
            "725:\tlearn: 0.0079855\ttotal: 56.8s\tremaining: 21.4s\n",
            "726:\tlearn: 0.0079784\ttotal: 56.9s\tremaining: 21.4s\n",
            "727:\tlearn: 0.0079723\ttotal: 56.9s\tremaining: 21.3s\n",
            "728:\tlearn: 0.0079688\ttotal: 57s\tremaining: 21.2s\n",
            "729:\tlearn: 0.0079439\ttotal: 57.1s\tremaining: 21.1s\n",
            "730:\tlearn: 0.0079381\ttotal: 57.1s\tremaining: 21s\n",
            "731:\tlearn: 0.0079281\ttotal: 57.2s\tremaining: 20.9s\n",
            "732:\tlearn: 0.0079151\ttotal: 57.2s\tremaining: 20.8s\n",
            "733:\tlearn: 0.0078932\ttotal: 57.3s\tremaining: 20.8s\n",
            "734:\tlearn: 0.0078882\ttotal: 57.4s\tremaining: 20.7s\n",
            "735:\tlearn: 0.0078788\ttotal: 57.4s\tremaining: 20.6s\n",
            "736:\tlearn: 0.0078577\ttotal: 57.5s\tremaining: 20.5s\n",
            "737:\tlearn: 0.0078519\ttotal: 57.5s\tremaining: 20.4s\n",
            "738:\tlearn: 0.0078462\ttotal: 57.6s\tremaining: 20.3s\n",
            "739:\tlearn: 0.0078347\ttotal: 57.6s\tremaining: 20.3s\n",
            "740:\tlearn: 0.0078229\ttotal: 57.7s\tremaining: 20.2s\n",
            "741:\tlearn: 0.0078159\ttotal: 57.8s\tremaining: 20.1s\n",
            "742:\tlearn: 0.0077969\ttotal: 57.8s\tremaining: 20s\n",
            "743:\tlearn: 0.0077832\ttotal: 57.9s\tremaining: 19.9s\n",
            "744:\tlearn: 0.0077659\ttotal: 57.9s\tremaining: 19.8s\n",
            "745:\tlearn: 0.0077603\ttotal: 58s\tremaining: 19.7s\n",
            "746:\tlearn: 0.0077532\ttotal: 58s\tremaining: 19.7s\n",
            "747:\tlearn: 0.0077327\ttotal: 58.1s\tremaining: 19.6s\n",
            "748:\tlearn: 0.0077115\ttotal: 58.2s\tremaining: 19.5s\n",
            "749:\tlearn: 0.0076866\ttotal: 58.2s\tremaining: 19.4s\n",
            "750:\tlearn: 0.0076812\ttotal: 58.3s\tremaining: 19.3s\n",
            "751:\tlearn: 0.0076762\ttotal: 58.3s\tremaining: 19.2s\n",
            "752:\tlearn: 0.0076695\ttotal: 58.4s\tremaining: 19.2s\n",
            "753:\tlearn: 0.0076637\ttotal: 58.5s\tremaining: 19.1s\n",
            "754:\tlearn: 0.0076509\ttotal: 58.5s\tremaining: 19s\n",
            "755:\tlearn: 0.0076314\ttotal: 58.6s\tremaining: 18.9s\n",
            "756:\tlearn: 0.0076145\ttotal: 58.6s\tremaining: 18.8s\n",
            "757:\tlearn: 0.0075985\ttotal: 58.7s\tremaining: 18.7s\n",
            "758:\tlearn: 0.0075933\ttotal: 58.7s\tremaining: 18.7s\n",
            "759:\tlearn: 0.0075883\ttotal: 58.8s\tremaining: 18.6s\n",
            "760:\tlearn: 0.0075558\ttotal: 58.9s\tremaining: 18.5s\n",
            "761:\tlearn: 0.0075509\ttotal: 58.9s\tremaining: 18.4s\n",
            "762:\tlearn: 0.0075463\ttotal: 59s\tremaining: 18.3s\n",
            "763:\tlearn: 0.0075415\ttotal: 59s\tremaining: 18.2s\n",
            "764:\tlearn: 0.0075369\ttotal: 59.1s\tremaining: 18.1s\n",
            "765:\tlearn: 0.0075271\ttotal: 59.2s\tremaining: 18.1s\n",
            "766:\tlearn: 0.0075094\ttotal: 59.2s\tremaining: 18s\n",
            "767:\tlearn: 0.0074987\ttotal: 59.3s\tremaining: 17.9s\n",
            "768:\tlearn: 0.0074947\ttotal: 59.3s\tremaining: 17.8s\n",
            "769:\tlearn: 0.0074782\ttotal: 59.4s\tremaining: 17.7s\n",
            "770:\tlearn: 0.0074709\ttotal: 59.5s\tremaining: 17.7s\n",
            "771:\tlearn: 0.0074667\ttotal: 59.5s\tremaining: 17.6s\n",
            "772:\tlearn: 0.0074474\ttotal: 59.6s\tremaining: 17.5s\n",
            "773:\tlearn: 0.0074356\ttotal: 59.6s\tremaining: 17.4s\n",
            "774:\tlearn: 0.0074101\ttotal: 59.7s\tremaining: 17.3s\n",
            "775:\tlearn: 0.0074063\ttotal: 59.7s\tremaining: 17.2s\n",
            "776:\tlearn: 0.0073833\ttotal: 59.8s\tremaining: 17.2s\n",
            "777:\tlearn: 0.0073766\ttotal: 59.9s\tremaining: 17.1s\n",
            "778:\tlearn: 0.0073654\ttotal: 59.9s\tremaining: 17s\n",
            "779:\tlearn: 0.0073407\ttotal: 60s\tremaining: 16.9s\n",
            "780:\tlearn: 0.0073381\ttotal: 1m\tremaining: 16.8s\n",
            "781:\tlearn: 0.0073314\ttotal: 1m\tremaining: 16.8s\n",
            "782:\tlearn: 0.0073238\ttotal: 1m\tremaining: 16.7s\n",
            "783:\tlearn: 0.0073095\ttotal: 1m\tremaining: 16.6s\n",
            "784:\tlearn: 0.0073050\ttotal: 1m\tremaining: 16.5s\n",
            "785:\tlearn: 0.0073006\ttotal: 1m\tremaining: 16.4s\n",
            "786:\tlearn: 0.0072776\ttotal: 1m\tremaining: 16.3s\n",
            "787:\tlearn: 0.0072696\ttotal: 1m\tremaining: 16.3s\n",
            "788:\tlearn: 0.0072502\ttotal: 1m\tremaining: 16.2s\n",
            "789:\tlearn: 0.0072281\ttotal: 1m\tremaining: 16.1s\n",
            "790:\tlearn: 0.0072172\ttotal: 1m\tremaining: 16s\n",
            "791:\tlearn: 0.0072127\ttotal: 1m\tremaining: 15.9s\n",
            "792:\tlearn: 0.0071999\ttotal: 1m\tremaining: 15.9s\n",
            "793:\tlearn: 0.0071937\ttotal: 1m\tremaining: 15.8s\n",
            "794:\tlearn: 0.0071808\ttotal: 1m\tremaining: 15.7s\n",
            "795:\tlearn: 0.0071766\ttotal: 1m\tremaining: 15.6s\n",
            "796:\tlearn: 0.0071739\ttotal: 1m\tremaining: 15.5s\n",
            "797:\tlearn: 0.0071582\ttotal: 1m 1s\tremaining: 15.4s\n",
            "798:\tlearn: 0.0071353\ttotal: 1m 1s\tremaining: 15.4s\n",
            "799:\tlearn: 0.0071273\ttotal: 1m 1s\tremaining: 15.3s\n",
            "800:\tlearn: 0.0071235\ttotal: 1m 1s\tremaining: 15.2s\n",
            "801:\tlearn: 0.0071112\ttotal: 1m 1s\tremaining: 15.1s\n",
            "802:\tlearn: 0.0071050\ttotal: 1m 1s\tremaining: 15s\n",
            "803:\tlearn: 0.0070900\ttotal: 1m 1s\tremaining: 15s\n",
            "804:\tlearn: 0.0070797\ttotal: 1m 1s\tremaining: 14.9s\n",
            "805:\tlearn: 0.0070509\ttotal: 1m 1s\tremaining: 14.8s\n",
            "806:\tlearn: 0.0070443\ttotal: 1m 1s\tremaining: 14.7s\n",
            "807:\tlearn: 0.0070334\ttotal: 1m 1s\tremaining: 14.6s\n",
            "808:\tlearn: 0.0070268\ttotal: 1m 1s\tremaining: 14.6s\n",
            "809:\tlearn: 0.0070130\ttotal: 1m 1s\tremaining: 14.5s\n",
            "810:\tlearn: 0.0069960\ttotal: 1m 1s\tremaining: 14.4s\n",
            "811:\tlearn: 0.0069885\ttotal: 1m 1s\tremaining: 14.3s\n",
            "812:\tlearn: 0.0069776\ttotal: 1m 1s\tremaining: 14.2s\n",
            "813:\tlearn: 0.0069684\ttotal: 1m 1s\tremaining: 14.2s\n",
            "814:\tlearn: 0.0069605\ttotal: 1m 2s\tremaining: 14.1s\n",
            "815:\tlearn: 0.0069459\ttotal: 1m 2s\tremaining: 14s\n",
            "816:\tlearn: 0.0069343\ttotal: 1m 2s\tremaining: 13.9s\n",
            "817:\tlearn: 0.0069244\ttotal: 1m 2s\tremaining: 13.8s\n",
            "818:\tlearn: 0.0069115\ttotal: 1m 2s\tremaining: 13.8s\n",
            "819:\tlearn: 0.0068906\ttotal: 1m 2s\tremaining: 13.7s\n",
            "820:\tlearn: 0.0068771\ttotal: 1m 2s\tremaining: 13.6s\n",
            "821:\tlearn: 0.0068712\ttotal: 1m 2s\tremaining: 13.5s\n",
            "822:\tlearn: 0.0068655\ttotal: 1m 2s\tremaining: 13.4s\n",
            "823:\tlearn: 0.0068471\ttotal: 1m 2s\tremaining: 13.4s\n",
            "824:\tlearn: 0.0068384\ttotal: 1m 2s\tremaining: 13.3s\n",
            "825:\tlearn: 0.0068342\ttotal: 1m 2s\tremaining: 13.2s\n",
            "826:\tlearn: 0.0068237\ttotal: 1m 2s\tremaining: 13.1s\n",
            "827:\tlearn: 0.0068164\ttotal: 1m 2s\tremaining: 13s\n",
            "828:\tlearn: 0.0068016\ttotal: 1m 2s\tremaining: 13s\n",
            "829:\tlearn: 0.0067808\ttotal: 1m 2s\tremaining: 12.9s\n",
            "830:\tlearn: 0.0067748\ttotal: 1m 2s\tremaining: 12.8s\n",
            "831:\tlearn: 0.0067648\ttotal: 1m 3s\tremaining: 12.7s\n",
            "832:\tlearn: 0.0067445\ttotal: 1m 3s\tremaining: 12.6s\n",
            "833:\tlearn: 0.0067047\ttotal: 1m 3s\tremaining: 12.6s\n",
            "834:\tlearn: 0.0067011\ttotal: 1m 3s\tremaining: 12.5s\n",
            "835:\tlearn: 0.0066972\ttotal: 1m 3s\tremaining: 12.4s\n",
            "836:\tlearn: 0.0066933\ttotal: 1m 3s\tremaining: 12.3s\n",
            "837:\tlearn: 0.0066716\ttotal: 1m 3s\tremaining: 12.2s\n",
            "838:\tlearn: 0.0066678\ttotal: 1m 3s\tremaining: 12.2s\n",
            "839:\tlearn: 0.0066565\ttotal: 1m 3s\tremaining: 12.1s\n",
            "840:\tlearn: 0.0066529\ttotal: 1m 3s\tremaining: 12s\n",
            "841:\tlearn: 0.0066404\ttotal: 1m 3s\tremaining: 11.9s\n",
            "842:\tlearn: 0.0066221\ttotal: 1m 3s\tremaining: 11.9s\n",
            "843:\tlearn: 0.0066191\ttotal: 1m 3s\tremaining: 11.8s\n",
            "844:\tlearn: 0.0066106\ttotal: 1m 3s\tremaining: 11.7s\n",
            "845:\tlearn: 0.0065998\ttotal: 1m 3s\tremaining: 11.6s\n",
            "846:\tlearn: 0.0065962\ttotal: 1m 3s\tremaining: 11.5s\n",
            "847:\tlearn: 0.0065868\ttotal: 1m 3s\tremaining: 11.5s\n",
            "848:\tlearn: 0.0065801\ttotal: 1m 4s\tremaining: 11.4s\n",
            "849:\tlearn: 0.0065779\ttotal: 1m 4s\tremaining: 11.3s\n",
            "850:\tlearn: 0.0065600\ttotal: 1m 4s\tremaining: 11.2s\n",
            "851:\tlearn: 0.0065529\ttotal: 1m 4s\tremaining: 11.2s\n",
            "852:\tlearn: 0.0065445\ttotal: 1m 4s\tremaining: 11.1s\n",
            "853:\tlearn: 0.0065403\ttotal: 1m 4s\tremaining: 11s\n",
            "854:\tlearn: 0.0065340\ttotal: 1m 4s\tremaining: 10.9s\n",
            "855:\tlearn: 0.0065262\ttotal: 1m 4s\tremaining: 10.8s\n",
            "856:\tlearn: 0.0065226\ttotal: 1m 4s\tremaining: 10.8s\n",
            "857:\tlearn: 0.0065106\ttotal: 1m 4s\tremaining: 10.7s\n",
            "858:\tlearn: 0.0065031\ttotal: 1m 4s\tremaining: 10.6s\n",
            "859:\tlearn: 0.0064996\ttotal: 1m 4s\tremaining: 10.5s\n",
            "860:\tlearn: 0.0064910\ttotal: 1m 4s\tremaining: 10.4s\n",
            "861:\tlearn: 0.0064833\ttotal: 1m 4s\tremaining: 10.4s\n",
            "862:\tlearn: 0.0064749\ttotal: 1m 4s\tremaining: 10.3s\n",
            "863:\tlearn: 0.0064712\ttotal: 1m 4s\tremaining: 10.2s\n",
            "864:\tlearn: 0.0064630\ttotal: 1m 4s\tremaining: 10.1s\n",
            "865:\tlearn: 0.0064515\ttotal: 1m 5s\tremaining: 10.1s\n",
            "866:\tlearn: 0.0064447\ttotal: 1m 5s\tremaining: 9.98s\n",
            "867:\tlearn: 0.0064410\ttotal: 1m 5s\tremaining: 9.9s\n",
            "868:\tlearn: 0.0064379\ttotal: 1m 5s\tremaining: 9.83s\n",
            "869:\tlearn: 0.0064327\ttotal: 1m 5s\tremaining: 9.75s\n",
            "870:\tlearn: 0.0064199\ttotal: 1m 5s\tremaining: 9.67s\n",
            "871:\tlearn: 0.0064160\ttotal: 1m 5s\tremaining: 9.59s\n",
            "872:\tlearn: 0.0063863\ttotal: 1m 5s\tremaining: 9.52s\n",
            "873:\tlearn: 0.0063790\ttotal: 1m 5s\tremaining: 9.44s\n",
            "874:\tlearn: 0.0063692\ttotal: 1m 5s\tremaining: 9.36s\n",
            "875:\tlearn: 0.0063606\ttotal: 1m 5s\tremaining: 9.29s\n",
            "876:\tlearn: 0.0063484\ttotal: 1m 5s\tremaining: 9.21s\n",
            "877:\tlearn: 0.0063445\ttotal: 1m 5s\tremaining: 9.13s\n",
            "878:\tlearn: 0.0063410\ttotal: 1m 5s\tremaining: 9.05s\n",
            "879:\tlearn: 0.0063360\ttotal: 1m 5s\tremaining: 8.98s\n",
            "880:\tlearn: 0.0063085\ttotal: 1m 5s\tremaining: 8.9s\n",
            "881:\tlearn: 0.0062878\ttotal: 1m 5s\tremaining: 8.82s\n",
            "882:\tlearn: 0.0062797\ttotal: 1m 6s\tremaining: 8.75s\n",
            "883:\tlearn: 0.0062762\ttotal: 1m 6s\tremaining: 8.67s\n",
            "884:\tlearn: 0.0062730\ttotal: 1m 6s\tremaining: 8.59s\n",
            "885:\tlearn: 0.0062689\ttotal: 1m 6s\tremaining: 8.52s\n",
            "886:\tlearn: 0.0062597\ttotal: 1m 6s\tremaining: 8.44s\n",
            "887:\tlearn: 0.0062483\ttotal: 1m 6s\tremaining: 8.36s\n",
            "888:\tlearn: 0.0062392\ttotal: 1m 6s\tremaining: 8.29s\n",
            "889:\tlearn: 0.0062345\ttotal: 1m 6s\tremaining: 8.21s\n",
            "890:\tlearn: 0.0062310\ttotal: 1m 6s\tremaining: 8.13s\n",
            "891:\tlearn: 0.0062189\ttotal: 1m 6s\tremaining: 8.06s\n",
            "892:\tlearn: 0.0062041\ttotal: 1m 6s\tremaining: 7.98s\n",
            "893:\tlearn: 0.0061955\ttotal: 1m 6s\tremaining: 7.9s\n",
            "894:\tlearn: 0.0061886\ttotal: 1m 6s\tremaining: 7.83s\n",
            "895:\tlearn: 0.0061847\ttotal: 1m 6s\tremaining: 7.75s\n",
            "896:\tlearn: 0.0061815\ttotal: 1m 6s\tremaining: 7.67s\n",
            "897:\tlearn: 0.0061721\ttotal: 1m 6s\tremaining: 7.6s\n",
            "898:\tlearn: 0.0061649\ttotal: 1m 6s\tremaining: 7.52s\n",
            "899:\tlearn: 0.0061606\ttotal: 1m 7s\tremaining: 7.45s\n",
            "900:\tlearn: 0.0061558\ttotal: 1m 7s\tremaining: 7.37s\n",
            "901:\tlearn: 0.0061525\ttotal: 1m 7s\tremaining: 7.29s\n",
            "902:\tlearn: 0.0061251\ttotal: 1m 7s\tremaining: 7.22s\n",
            "903:\tlearn: 0.0061179\ttotal: 1m 7s\tremaining: 7.14s\n",
            "904:\tlearn: 0.0061133\ttotal: 1m 7s\tremaining: 7.06s\n",
            "905:\tlearn: 0.0061087\ttotal: 1m 7s\tremaining: 6.99s\n",
            "906:\tlearn: 0.0060971\ttotal: 1m 7s\tremaining: 6.91s\n",
            "907:\tlearn: 0.0060936\ttotal: 1m 7s\tremaining: 6.84s\n",
            "908:\tlearn: 0.0060869\ttotal: 1m 7s\tremaining: 6.76s\n",
            "909:\tlearn: 0.0060789\ttotal: 1m 7s\tremaining: 6.68s\n",
            "910:\tlearn: 0.0060676\ttotal: 1m 7s\tremaining: 6.61s\n",
            "911:\tlearn: 0.0060635\ttotal: 1m 7s\tremaining: 6.53s\n",
            "912:\tlearn: 0.0060493\ttotal: 1m 7s\tremaining: 6.46s\n",
            "913:\tlearn: 0.0060448\ttotal: 1m 7s\tremaining: 6.38s\n",
            "914:\tlearn: 0.0060424\ttotal: 1m 7s\tremaining: 6.31s\n",
            "915:\tlearn: 0.0060346\ttotal: 1m 7s\tremaining: 6.23s\n",
            "916:\tlearn: 0.0060225\ttotal: 1m 8s\tremaining: 6.15s\n",
            "917:\tlearn: 0.0060194\ttotal: 1m 8s\tremaining: 6.08s\n",
            "918:\tlearn: 0.0060048\ttotal: 1m 8s\tremaining: 6s\n",
            "919:\tlearn: 0.0060003\ttotal: 1m 8s\tremaining: 5.93s\n",
            "920:\tlearn: 0.0059881\ttotal: 1m 8s\tremaining: 5.85s\n",
            "921:\tlearn: 0.0059631\ttotal: 1m 8s\tremaining: 5.78s\n",
            "922:\tlearn: 0.0059562\ttotal: 1m 8s\tremaining: 5.7s\n",
            "923:\tlearn: 0.0059426\ttotal: 1m 8s\tremaining: 5.63s\n",
            "924:\tlearn: 0.0059339\ttotal: 1m 8s\tremaining: 5.55s\n",
            "925:\tlearn: 0.0059282\ttotal: 1m 8s\tremaining: 5.48s\n",
            "926:\tlearn: 0.0059123\ttotal: 1m 8s\tremaining: 5.4s\n",
            "927:\tlearn: 0.0059053\ttotal: 1m 8s\tremaining: 5.33s\n",
            "928:\tlearn: 0.0059018\ttotal: 1m 8s\tremaining: 5.25s\n",
            "929:\tlearn: 0.0058965\ttotal: 1m 8s\tremaining: 5.17s\n",
            "930:\tlearn: 0.0058898\ttotal: 1m 8s\tremaining: 5.1s\n",
            "931:\tlearn: 0.0058871\ttotal: 1m 8s\tremaining: 5.03s\n",
            "932:\tlearn: 0.0058791\ttotal: 1m 8s\tremaining: 4.95s\n",
            "933:\tlearn: 0.0058756\ttotal: 1m 8s\tremaining: 4.88s\n",
            "934:\tlearn: 0.0058708\ttotal: 1m 9s\tremaining: 4.8s\n",
            "935:\tlearn: 0.0058658\ttotal: 1m 9s\tremaining: 4.72s\n",
            "936:\tlearn: 0.0058593\ttotal: 1m 9s\tremaining: 4.65s\n",
            "937:\tlearn: 0.0058555\ttotal: 1m 9s\tremaining: 4.58s\n",
            "938:\tlearn: 0.0058520\ttotal: 1m 9s\tremaining: 4.5s\n",
            "939:\tlearn: 0.0058439\ttotal: 1m 9s\tremaining: 4.42s\n",
            "940:\tlearn: 0.0058362\ttotal: 1m 9s\tremaining: 4.35s\n",
            "941:\tlearn: 0.0058329\ttotal: 1m 9s\tremaining: 4.28s\n",
            "942:\tlearn: 0.0058224\ttotal: 1m 9s\tremaining: 4.2s\n",
            "943:\tlearn: 0.0058135\ttotal: 1m 9s\tremaining: 4.13s\n",
            "944:\tlearn: 0.0058088\ttotal: 1m 9s\tremaining: 4.05s\n",
            "945:\tlearn: 0.0058054\ttotal: 1m 9s\tremaining: 3.98s\n",
            "946:\tlearn: 0.0057978\ttotal: 1m 9s\tremaining: 3.9s\n",
            "947:\tlearn: 0.0057941\ttotal: 1m 9s\tremaining: 3.83s\n",
            "948:\tlearn: 0.0057849\ttotal: 1m 9s\tremaining: 3.75s\n",
            "949:\tlearn: 0.0057684\ttotal: 1m 9s\tremaining: 3.68s\n",
            "950:\tlearn: 0.0057595\ttotal: 1m 10s\tremaining: 3.61s\n",
            "951:\tlearn: 0.0057529\ttotal: 1m 10s\tremaining: 3.53s\n",
            "952:\tlearn: 0.0057483\ttotal: 1m 10s\tremaining: 3.46s\n",
            "953:\tlearn: 0.0057413\ttotal: 1m 10s\tremaining: 3.38s\n",
            "954:\tlearn: 0.0057244\ttotal: 1m 10s\tremaining: 3.31s\n",
            "955:\tlearn: 0.0057180\ttotal: 1m 10s\tremaining: 3.23s\n",
            "956:\tlearn: 0.0057108\ttotal: 1m 10s\tremaining: 3.16s\n",
            "957:\tlearn: 0.0057056\ttotal: 1m 10s\tremaining: 3.09s\n",
            "958:\tlearn: 0.0056943\ttotal: 1m 10s\tremaining: 3.01s\n",
            "959:\tlearn: 0.0056909\ttotal: 1m 10s\tremaining: 2.94s\n",
            "960:\tlearn: 0.0056833\ttotal: 1m 10s\tremaining: 2.86s\n",
            "961:\tlearn: 0.0056790\ttotal: 1m 10s\tremaining: 2.79s\n",
            "962:\tlearn: 0.0056730\ttotal: 1m 10s\tremaining: 2.71s\n",
            "963:\tlearn: 0.0056613\ttotal: 1m 10s\tremaining: 2.64s\n",
            "964:\tlearn: 0.0056535\ttotal: 1m 10s\tremaining: 2.57s\n",
            "965:\tlearn: 0.0056502\ttotal: 1m 10s\tremaining: 2.49s\n",
            "966:\tlearn: 0.0056459\ttotal: 1m 10s\tremaining: 2.42s\n",
            "967:\tlearn: 0.0056391\ttotal: 1m 10s\tremaining: 2.35s\n",
            "968:\tlearn: 0.0056304\ttotal: 1m 11s\tremaining: 2.27s\n",
            "969:\tlearn: 0.0056095\ttotal: 1m 11s\tremaining: 2.2s\n",
            "970:\tlearn: 0.0056032\ttotal: 1m 11s\tremaining: 2.13s\n",
            "971:\tlearn: 0.0055958\ttotal: 1m 11s\tremaining: 2.05s\n",
            "972:\tlearn: 0.0055842\ttotal: 1m 11s\tremaining: 1.98s\n",
            "973:\tlearn: 0.0055787\ttotal: 1m 11s\tremaining: 1.9s\n",
            "974:\tlearn: 0.0055571\ttotal: 1m 11s\tremaining: 1.83s\n",
            "975:\tlearn: 0.0055507\ttotal: 1m 11s\tremaining: 1.76s\n",
            "976:\tlearn: 0.0055427\ttotal: 1m 11s\tremaining: 1.68s\n",
            "977:\tlearn: 0.0055376\ttotal: 1m 11s\tremaining: 1.61s\n",
            "978:\tlearn: 0.0055268\ttotal: 1m 11s\tremaining: 1.54s\n",
            "979:\tlearn: 0.0055219\ttotal: 1m 11s\tremaining: 1.46s\n",
            "980:\tlearn: 0.0055081\ttotal: 1m 11s\tremaining: 1.39s\n",
            "981:\tlearn: 0.0055025\ttotal: 1m 11s\tremaining: 1.32s\n",
            "982:\tlearn: 0.0054984\ttotal: 1m 11s\tremaining: 1.24s\n",
            "983:\tlearn: 0.0054966\ttotal: 1m 11s\tremaining: 1.17s\n",
            "984:\tlearn: 0.0054907\ttotal: 1m 11s\tremaining: 1.1s\n",
            "985:\tlearn: 0.0054867\ttotal: 1m 12s\tremaining: 1.02s\n",
            "986:\tlearn: 0.0054729\ttotal: 1m 12s\tremaining: 950ms\n",
            "987:\tlearn: 0.0054644\ttotal: 1m 12s\tremaining: 876ms\n",
            "988:\tlearn: 0.0054491\ttotal: 1m 12s\tremaining: 803ms\n",
            "989:\tlearn: 0.0054444\ttotal: 1m 12s\tremaining: 730ms\n",
            "990:\tlearn: 0.0054416\ttotal: 1m 12s\tremaining: 657ms\n",
            "991:\tlearn: 0.0054327\ttotal: 1m 12s\tremaining: 584ms\n",
            "992:\tlearn: 0.0054288\ttotal: 1m 12s\tremaining: 511ms\n",
            "993:\tlearn: 0.0054156\ttotal: 1m 12s\tremaining: 438ms\n",
            "994:\tlearn: 0.0054117\ttotal: 1m 12s\tremaining: 365ms\n",
            "995:\tlearn: 0.0054086\ttotal: 1m 12s\tremaining: 292ms\n",
            "996:\tlearn: 0.0054013\ttotal: 1m 12s\tremaining: 219ms\n",
            "997:\tlearn: 0.0053847\ttotal: 1m 12s\tremaining: 146ms\n",
            "998:\tlearn: 0.0053778\ttotal: 1m 12s\tremaining: 72.9ms\n",
            "999:\tlearn: 0.0053704\ttotal: 1m 12s\tremaining: 0us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at benchmark xgboost\n",
        "\n",
        "# using the default xgboost classifier\n",
        "xgb_bench = XGBClassifier(random_state=RANDOM_STATE, use_best_model=True)\n",
        "xgb_bench.fit(x_train, y_train)\n",
        "\n",
        "# getting predicitons for xgboost model\n",
        "xgb_train_pred = xgb_bench.predict(x_train)\n",
        "xgb_test_pred = xgb_bench.predict(x_test)"
      ],
      "metadata": {
        "id": "CkxHiYHwdMSn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores = all_scores.append(get_scores(lr_train_pred, lr_test_pred, y_train, y_test, 'lr', 'benchmark'))\n",
        "all_scores = all_scores.append(get_scores(cat_train_pred, cat_test_pred, y_train, y_test, 'cat', 'benchmark'))\n",
        "all_scores = all_scores.append(get_scores(xgb_train_pred, xgb_test_pred, y_train, y_test, 'xgb', 'benchmark'))\n",
        "\n",
        "all_scores.set_index(['Model_version', 'Model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "pXJwPfKFi_dm",
        "outputId": "e78a8b88-2cb7-45e5-851e-6d1aa9b87889"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              Train_acc Train_prec         Train_recall  \\\n",
              "Model_version Model                                                       \n",
              "benchmark     lr     0.9906394615061002        0.0                  0.0   \n",
              "              cat    0.9992637778712663        1.0   0.9213483146067416   \n",
              "              xgb    0.9912705090450147        1.0  0.06741573033707865   \n",
              "\n",
              "                            Train_fscore           Train_auc  \\\n",
              "Model_version Model                                            \n",
              "benchmark     lr                     0.0                 0.5   \n",
              "              cat     0.9590643274853802  0.9606741573033708   \n",
              "              xgb    0.12631578947368421  0.5337078651685393   \n",
              "\n",
              "                               Test_acc Test_prec Test_recall Test_fscore  \\\n",
              "Model_version Model                                                         \n",
              "benchmark     lr     0.9903948772678762       0.0         0.0         0.0   \n",
              "              cat    0.9901814300960512       0.0         0.0         0.0   \n",
              "              xgb    0.9901814300960512       0.0         0.0         0.0   \n",
              "\n",
              "                               Test_auc  \n",
              "Model_version Model                      \n",
              "benchmark     lr                    0.5  \n",
              "              cat    0.4998922413793103  \n",
              "              xgb    0.4998922413793103  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dde15336-4303-4207-8055-8331d61be201\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Train_acc</th>\n",
              "      <th>Train_prec</th>\n",
              "      <th>Train_recall</th>\n",
              "      <th>Train_fscore</th>\n",
              "      <th>Train_auc</th>\n",
              "      <th>Test_acc</th>\n",
              "      <th>Test_prec</th>\n",
              "      <th>Test_recall</th>\n",
              "      <th>Test_fscore</th>\n",
              "      <th>Test_auc</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model_version</th>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">benchmark</th>\n",
              "      <th>lr</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.9992637778712663</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9213483146067416</td>\n",
              "      <td>0.9590643274853802</td>\n",
              "      <td>0.9606741573033708</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xgb</th>\n",
              "      <td>0.9912705090450147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06741573033707865</td>\n",
              "      <td>0.12631578947368421</td>\n",
              "      <td>0.5337078651685393</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dde15336-4303-4207-8055-8331d61be201')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dde15336-4303-4207-8055-8331d61be201 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dde15336-4303-4207-8055-8331d61be201');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like the every benchmark model performs poorly on the test set, and none of the models seem to predict 1 for any of the observations.\n",
        "\n",
        "Catboost seems to have pretty good results for the training set, however, so we will proceed with catboost as the algorithm of choice, even though it greatly overfits the training set."
      ],
      "metadata": {
        "id": "UtL4QjIioqOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning on baseline classifiers"
      ],
      "metadata": {
        "id": "t2Ofgq8ijQ3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning the hyper parameters for all three models was tested, but there were no improvements to any of the models, and it exponentially increased training time, so these results will not be shown."
      ],
      "metadata": {
        "id": "Swwj9iB4lrLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modify objective and evaluation function of CatBoost"
      ],
      "metadata": {
        "id": "Aanl5bsTiqfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Catboost base package provides only logloss and crossentropy as optimization functions.  For binary classification, the two are identical, so it will not be changed.  However, we can use precision as our evaluation metric to and try to have catboost fit the data for the testing set."
      ],
      "metadata": {
        "id": "daZAIYTFIuMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_obj_func = CatBoostClassifier(eval_metric='Precision', \n",
        "                   custom_metric=['F1', 'Precision', 'Recall'],\n",
        "                   random_state=RANDOM_STATE,\n",
        "                   use_best_model=True)\n",
        "cat_obj_func.fit(x_train, y_train, eval_set=(x_test, y_test))\n",
        "\n",
        "\n",
        "# getting the predictions\n",
        "cat_obj_func_train_pred = cat_obj_func.predict(x_train)\n",
        "cat_obj_func_test_pred = cat_obj_func.predict(x_test)"
      ],
      "metadata": {
        "id": "nLjLrN2BoWzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores = all_scores.append(get_scores(cat_obj_func_train_pred,\n",
        "                                          cat_obj_func_test_pred,\n",
        "                                          y_train, y_test,\n",
        "                                          'cat', 'maximizing precision'))\n",
        "\n",
        "all_scores.set_index(['Model_version', 'Model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "aeiCk3KnM49l",
        "outputId": "1174962d-a857-43a8-bd88-0c17509b587c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     Train_acc Train_prec  \\\n",
              "Model_version        Model                                  \n",
              "benchmark            lr     0.9906394615061002        0.0   \n",
              "                     cat    0.9992637778712663        1.0   \n",
              "                     xgb    0.9912705090450147        1.0   \n",
              "maximizing precision cat    0.9906394615061002        0.0   \n",
              "\n",
              "                                   Train_recall         Train_fscore  \\\n",
              "Model_version        Model                                             \n",
              "benchmark            lr                     0.0                  0.0   \n",
              "                     cat     0.9213483146067416   0.9590643274853802   \n",
              "                     xgb    0.06741573033707865  0.12631578947368421   \n",
              "maximizing precision cat                    0.0                  0.0   \n",
              "\n",
              "                                     Train_auc            Test_acc Test_prec  \\\n",
              "Model_version        Model                                                     \n",
              "benchmark            lr                    0.5  0.9903948772678762       0.0   \n",
              "                     cat    0.9606741573033708  0.9901814300960512       0.0   \n",
              "                     xgb    0.5337078651685393  0.9901814300960512       0.0   \n",
              "maximizing precision cat                   0.5  0.9903948772678762       0.0   \n",
              "\n",
              "                           Test_recall Test_fscore            Test_auc  \n",
              "Model_version        Model                                              \n",
              "benchmark            lr            0.0         0.0                 0.5  \n",
              "                     cat           0.0         0.0  0.4998922413793103  \n",
              "                     xgb           0.0         0.0  0.4998922413793103  \n",
              "maximizing precision cat           0.0         0.0                 0.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab08b762-401f-4161-98e5-3a3b98fb3e65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Train_acc</th>\n",
              "      <th>Train_prec</th>\n",
              "      <th>Train_recall</th>\n",
              "      <th>Train_fscore</th>\n",
              "      <th>Train_auc</th>\n",
              "      <th>Test_acc</th>\n",
              "      <th>Test_prec</th>\n",
              "      <th>Test_recall</th>\n",
              "      <th>Test_fscore</th>\n",
              "      <th>Test_auc</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model_version</th>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">benchmark</th>\n",
              "      <th>lr</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.9992637778712663</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9213483146067416</td>\n",
              "      <td>0.9590643274853802</td>\n",
              "      <td>0.9606741573033708</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xgb</th>\n",
              "      <td>0.9912705090450147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06741573033707865</td>\n",
              "      <td>0.12631578947368421</td>\n",
              "      <td>0.5337078651685393</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maximizing precision</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab08b762-401f-4161-98e5-3a3b98fb3e65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab08b762-401f-4161-98e5-3a3b98fb3e65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab08b762-401f-4161-98e5-3a3b98fb3e65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This did not improve the model, so the next thing we will try is selecting a threshold based off the roc curve for the benchmark model to improve precision."
      ],
      "metadata": {
        "id": "SHCrSl8fcgIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting thresholds for what probability counts as a prediction based off fpr and npr\n",
        "\n",
        "cat_pool = Pool(x_test, y_test)\n",
        "curve = get_roc_curve(cat_bench, cat_pool)\n",
        "thresh_l_fpr = []\n",
        "thresh_l_fnr = []\n",
        "rates = [.1,.2, .3, .4, .5, .6, .7, .8, .9,]\n",
        "for i in rates:\n",
        "  thresh_l_fpr.append(select_threshold(cat_bench, curve=curve, FPR = i))\n",
        "  thresh_l_fnr.append(select_threshold(cat_bench, curve=curve, FNR = i))"
      ],
      "metadata": {
        "id": "PaPBHhVec0bj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting predicitons for benchmark catboost model\n",
        "cat_train_thresh = cat_bench.predict_proba(x_train)\n",
        "cat_test_thresh = cat_bench.predict_proba(x_test)"
      ],
      "metadata": {
        "id": "KR04oXSei2Hu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting scores for different thresholds based off different fpr and npr rates\n",
        "\n",
        "for i in range(len(thresh_l_fpr)):\n",
        "  cat_train_thresh_pred = [1 if cat_train_thresh[j][1] > thresh_l_fpr[i] else 0 for j in range(len(cat_train_thresh))]\n",
        "  cat_test_thresh_pred = [1 if cat_test_thresh[j][1] > thresh_l_fpr[i] else 0 for j in range(len(cat_test_thresh))]\n",
        "  all_scores = all_scores.append(get_scores(cat_train_thresh_pred,\n",
        "                                          cat_test_thresh_pred,\n",
        "                                          y_train, y_test,\n",
        "                                          'cat', 'pred_thresh_for_fpr_' + str(rates[i])))\n",
        "  \n",
        "for i in range(len(thresh_l_fnr)):\n",
        "  cat_train_thresh_pred = [1 if cat_train_thresh[j][1] > thresh_l_fnr[i] else 0 for j in range(len(cat_train_thresh))]\n",
        "  cat_test_thresh_pred = [1 if cat_test_thresh[j][1] > thresh_l_fnr[i] else 0 for j in range(len(cat_test_thresh))]\n",
        "  all_scores = all_scores.append(get_scores(cat_train_thresh_pred,\n",
        "                                          cat_test_thresh_pred,\n",
        "                                          y_train, y_test,\n",
        "                                          'cat', 'pred_thresh_for_fnr_' + str(rates[i])))"
      ],
      "metadata": {
        "id": "gu19KyFznT7z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores.set_index(['Model_version', 'Model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "JOZ-NHHBerA_",
        "outputId": "a3f3f789-ea7a-4c9c-f8d1-0ff14ac51b4a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         Train_acc            Train_prec  \\\n",
              "Model_version           Model                                              \n",
              "benchmark               lr      0.9906394615061002                   0.0   \n",
              "                        cat     0.9992637778712663                   1.0   \n",
              "                        xgb     0.9912705090450147                   1.0   \n",
              "maximizing precision    cat     0.9906394615061002                   0.0   \n",
              "pred_thresh_for_fpr_0.1 cat     0.9252208666386201               0.11125   \n",
              "pred_thresh_for_fpr_0.2 cat     0.8113167858645352   0.04726500265533723   \n",
              "pred_thresh_for_fpr_0.3 cat     0.7164493058477072   0.03195691202872531   \n",
              "pred_thresh_for_fpr_0.4 cat      0.612852334875894  0.023607427055702918   \n",
              "pred_thresh_for_fpr_0.5 cat      0.515776188472865  0.018964415086298744   \n",
              "pred_thresh_for_fpr_0.6 cat    0.41838451830037865   0.01583911728065492   \n",
              "pred_thresh_for_fpr_0.7 cat     0.3148927219183845  0.013478721793124338   \n",
              "pred_thresh_for_fpr_0.8 cat     0.2123474968447623  0.011744523621008182   \n",
              "pred_thresh_for_fpr_0.9 cat    0.11432477913336138   0.01045828437132785   \n",
              "pred_thresh_for_fnr_0.1 cat     0.8043752629364745  0.045664443304258596   \n",
              "pred_thresh_for_fnr_0.2 cat     0.8533866217921751   0.06001348617666891   \n",
              "pred_thresh_for_fnr_0.3 cat     0.8904080774084981   0.07869142351900972   \n",
              "pred_thresh_for_fnr_0.4 cat     0.9462557846024401   0.14833333333333334   \n",
              "pred_thresh_for_fnr_0.5 cat     0.9608750525872949   0.19305856832971802   \n",
              "pred_thresh_for_fnr_0.6 cat     0.9700252419015566   0.23796791443850268   \n",
              "pred_thresh_for_fnr_0.7 cat      0.987273874631889    0.4238095238095238   \n",
              "pred_thresh_for_fnr_0.8 cat     0.9937946992006731    0.6013513513513513   \n",
              "pred_thresh_for_fnr_0.9 cat                    1.0                   1.0   \n",
              "\n",
              "                                      Train_recall          Train_fscore  \\\n",
              "Model_version           Model                                              \n",
              "benchmark               lr                     0.0                   0.0   \n",
              "                        cat     0.9213483146067416    0.9590643274853802   \n",
              "                        xgb    0.06741573033707865   0.12631578947368421   \n",
              "maximizing precision    cat                    0.0                   0.0   \n",
              "pred_thresh_for_fpr_0.1 cat                    1.0   0.20022497187851518   \n",
              "pred_thresh_for_fpr_0.2 cat                    1.0   0.09026369168356999   \n",
              "pred_thresh_for_fpr_0.3 cat                    1.0   0.06193458594293667   \n",
              "pred_thresh_for_fpr_0.4 cat                    1.0   0.04612593936252915   \n",
              "pred_thresh_for_fpr_0.5 cat                    1.0   0.03722291928063572   \n",
              "pred_thresh_for_fpr_0.6 cat                    1.0  0.031184302733006306   \n",
              "pred_thresh_for_fpr_0.7 cat                    1.0   0.02659892408846384   \n",
              "pred_thresh_for_fpr_0.8 cat                    1.0   0.02321638189643929   \n",
              "pred_thresh_for_fpr_0.9 cat                    1.0  0.020700081404814514   \n",
              "pred_thresh_for_fnr_0.1 cat                    1.0   0.08734052993130521   \n",
              "pred_thresh_for_fnr_0.2 cat                    1.0   0.11323155216284986   \n",
              "pred_thresh_for_fnr_0.3 cat                    1.0   0.14590163934426228   \n",
              "pred_thresh_for_fnr_0.4 cat                    1.0   0.25834542815674894   \n",
              "pred_thresh_for_fnr_0.5 cat                    1.0    0.3236363636363636   \n",
              "pred_thresh_for_fnr_0.6 cat                    1.0   0.38444924406047515   \n",
              "pred_thresh_for_fnr_0.7 cat                    1.0    0.5953177257525083   \n",
              "pred_thresh_for_fnr_0.8 cat                    1.0     0.751054852320675   \n",
              "pred_thresh_for_fnr_0.9 cat                    1.0                   1.0   \n",
              "\n",
              "                                        Train_auc             Test_acc  \\\n",
              "Model_version           Model                                            \n",
              "benchmark               lr                    0.5   0.9903948772678762   \n",
              "                        cat    0.9606741573033708   0.9901814300960512   \n",
              "                        xgb    0.5337078651685393   0.9901814300960512   \n",
              "maximizing precision    cat                   0.5   0.9903948772678762   \n",
              "pred_thresh_for_fpr_0.1 cat    0.9622571398237605   0.8975453575240128   \n",
              "pred_thresh_for_fpr_0.2 cat    0.9047669603991931   0.8008537886872998   \n",
              "pred_thresh_for_fpr_0.3 cat    0.8568850196411508   0.7026680896478121   \n",
              "pred_thresh_for_fpr_0.4 cat    0.8045970909863043   0.6036286019210245   \n",
              "pred_thresh_for_fpr_0.5 cat     0.755600382206179   0.5045891141942369   \n",
              "pred_thresh_for_fpr_0.6 cat    0.7064444208514704   0.4057630736392743   \n",
              "pred_thresh_for_fpr_0.7 cat    0.6542095763881516  0.30672358591248666   \n",
              "pred_thresh_for_fpr_0.8 cat    0.6024524896485827  0.20768409818569905   \n",
              "pred_thresh_for_fpr_0.9 cat    0.5529780231447075  0.10864461045891143   \n",
              "pred_thresh_for_fnr_0.1 cat    0.9012634037583608   0.7946638207043757   \n",
              "pred_thresh_for_fnr_0.2 cat    0.9260006370102983   0.8379935965848453   \n",
              "pred_thresh_for_fnr_0.3 cat    0.9446862724280709   0.8670224119530416   \n",
              "pred_thresh_for_fnr_0.4 cat    0.9728739781293131   0.9212379935965849   \n",
              "pred_thresh_for_fnr_0.5 cat    0.9802526807516722   0.9346851654215582   \n",
              "pred_thresh_for_fnr_0.6 cat    0.9848710054145876   0.9451440768409819   \n",
              "pred_thresh_for_fnr_0.7 cat    0.9935768128251407   0.9658484525080042   \n",
              "pred_thresh_for_fnr_0.8 cat    0.9968680326998619    0.975880469583778   \n",
              "pred_thresh_for_fnr_0.9 cat                   1.0   0.9882604055496265   \n",
              "\n",
              "                                          Test_prec         Test_recall  \\\n",
              "Model_version           Model                                             \n",
              "benchmark               lr                      0.0                 0.0   \n",
              "                        cat                     0.0                 0.0   \n",
              "                        xgb                     0.0                 0.0   \n",
              "maximizing precision    cat                     0.0                 0.0   \n",
              "pred_thresh_for_fpr_0.1 cat    0.058823529411764705  0.6444444444444445   \n",
              "pred_thresh_for_fpr_0.2 cat     0.04132231404958678  0.8888888888888888   \n",
              "pred_thresh_for_fpr_0.3 cat     0.03064066852367688  0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.4 cat    0.023157894736842106  0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.5 cat    0.018612521150592216  0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.6 cat    0.015906680805938492                 1.0   \n",
              "pred_thresh_for_fpr_0.7 cat    0.013665350744002429                 1.0   \n",
              "pred_thresh_for_fpr_0.8 cat    0.011977641735427202                 1.0   \n",
              "pred_thresh_for_fpr_0.9 cat    0.010660980810234541                 1.0   \n",
              "pred_thresh_for_fnr_0.1 cat     0.04104104104104104  0.9111111111111111   \n",
              "pred_thresh_for_fnr_0.2 cat     0.04580152671755725                 0.8   \n",
              "pred_thresh_for_fnr_0.3 cat     0.04984423676012461  0.7111111111111111   \n",
              "pred_thresh_for_fnr_0.4 cat     0.07142857142857142                 0.6   \n",
              "pred_thresh_for_fnr_0.5 cat      0.0749185667752443  0.5111111111111111   \n",
              "pred_thresh_for_fnr_0.6 cat     0.07258064516129033                 0.4   \n",
              "pred_thresh_for_fnr_0.7 cat      0.0979020979020979  0.3111111111111111   \n",
              "pred_thresh_for_fnr_0.8 cat     0.10465116279069768                 0.2   \n",
              "pred_thresh_for_fnr_0.9 cat                    0.25  0.1111111111111111   \n",
              "\n",
              "                                        Test_fscore            Test_auc  \n",
              "Model_version           Model                                            \n",
              "benchmark               lr                      0.0                 0.5  \n",
              "                        cat                     0.0  0.4998922413793103  \n",
              "                        xgb                     0.0  0.4998922413793103  \n",
              "maximizing precision    cat                     0.0                 0.5  \n",
              "pred_thresh_for_fpr_0.1 cat     0.10780669144981413  0.7722222222222223  \n",
              "pred_thresh_for_fpr_0.2 cat     0.07897334649555775  0.8444444444444444  \n",
              "pred_thresh_for_fpr_0.3 cat    0.059419311276164746  0.8388888888888888  \n",
              "pred_thresh_for_fpr_0.4 cat     0.04524421593830334   0.788888888888889  \n",
              "pred_thresh_for_fpr_0.5 cat      0.0365296803652968  0.7388888888888889  \n",
              "pred_thresh_for_fpr_0.6 cat    0.031315240083507306                 0.7  \n",
              "pred_thresh_for_fpr_0.7 cat     0.02696225284601558                0.65  \n",
              "pred_thresh_for_fpr_0.8 cat    0.023671751709626515                 0.6  \n",
              "pred_thresh_for_fpr_0.9 cat    0.021097046413502112                0.55  \n",
              "pred_thresh_for_fnr_0.1 cat       0.078544061302682  0.8523227969348659  \n",
              "pred_thresh_for_fnr_0.2 cat     0.08664259927797834  0.8191810344827587  \n",
              "pred_thresh_for_fnr_0.3 cat     0.09315866084425035  0.7898227969348659  \n",
              "pred_thresh_for_fnr_0.4 cat      0.1276595744680851   0.762176724137931  \n",
              "pred_thresh_for_fnr_0.5 cat     0.13068181818181818  0.7249521072796934  \n",
              "pred_thresh_for_fnr_0.6 cat     0.12286689419795223  0.6752155172413793  \n",
              "pred_thresh_for_fnr_0.7 cat     0.14893617021276595  0.6416546934865901  \n",
              "pred_thresh_for_fnr_0.8 cat     0.13740458015267176  0.5917025862068965  \n",
              "pred_thresh_for_fnr_0.9 cat     0.15384615384615383  0.5539391762452107  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3311f90b-d902-4126-a5f1-a3d79c6aeb6d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Train_acc</th>\n",
              "      <th>Train_prec</th>\n",
              "      <th>Train_recall</th>\n",
              "      <th>Train_fscore</th>\n",
              "      <th>Train_auc</th>\n",
              "      <th>Test_acc</th>\n",
              "      <th>Test_prec</th>\n",
              "      <th>Test_recall</th>\n",
              "      <th>Test_fscore</th>\n",
              "      <th>Test_auc</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model_version</th>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">benchmark</th>\n",
              "      <th>lr</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.9992637778712663</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9213483146067416</td>\n",
              "      <td>0.9590643274853802</td>\n",
              "      <td>0.9606741573033708</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xgb</th>\n",
              "      <td>0.9912705090450147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06741573033707865</td>\n",
              "      <td>0.12631578947368421</td>\n",
              "      <td>0.5337078651685393</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maximizing precision</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9252208666386201</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.20022497187851518</td>\n",
              "      <td>0.9622571398237605</td>\n",
              "      <td>0.8975453575240128</td>\n",
              "      <td>0.058823529411764705</td>\n",
              "      <td>0.6444444444444445</td>\n",
              "      <td>0.10780669144981413</td>\n",
              "      <td>0.7722222222222223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8113167858645352</td>\n",
              "      <td>0.04726500265533723</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.09026369168356999</td>\n",
              "      <td>0.9047669603991931</td>\n",
              "      <td>0.8008537886872998</td>\n",
              "      <td>0.04132231404958678</td>\n",
              "      <td>0.8888888888888888</td>\n",
              "      <td>0.07897334649555775</td>\n",
              "      <td>0.8444444444444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.7164493058477072</td>\n",
              "      <td>0.03195691202872531</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06193458594293667</td>\n",
              "      <td>0.8568850196411508</td>\n",
              "      <td>0.7026680896478121</td>\n",
              "      <td>0.03064066852367688</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.059419311276164746</td>\n",
              "      <td>0.8388888888888888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.612852334875894</td>\n",
              "      <td>0.023607427055702918</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.04612593936252915</td>\n",
              "      <td>0.8045970909863043</td>\n",
              "      <td>0.6036286019210245</td>\n",
              "      <td>0.023157894736842106</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.04524421593830334</td>\n",
              "      <td>0.788888888888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.515776188472865</td>\n",
              "      <td>0.018964415086298744</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.03722291928063572</td>\n",
              "      <td>0.755600382206179</td>\n",
              "      <td>0.5045891141942369</td>\n",
              "      <td>0.018612521150592216</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.0365296803652968</td>\n",
              "      <td>0.7388888888888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.41838451830037865</td>\n",
              "      <td>0.01583911728065492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031184302733006306</td>\n",
              "      <td>0.7064444208514704</td>\n",
              "      <td>0.4057630736392743</td>\n",
              "      <td>0.015906680805938492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031315240083507306</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.3148927219183845</td>\n",
              "      <td>0.013478721793124338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02659892408846384</td>\n",
              "      <td>0.6542095763881516</td>\n",
              "      <td>0.30672358591248666</td>\n",
              "      <td>0.013665350744002429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02696225284601558</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.2123474968447623</td>\n",
              "      <td>0.011744523621008182</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02321638189643929</td>\n",
              "      <td>0.6024524896485827</td>\n",
              "      <td>0.20768409818569905</td>\n",
              "      <td>0.011977641735427202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.023671751709626515</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.11432477913336138</td>\n",
              "      <td>0.01045828437132785</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.020700081404814514</td>\n",
              "      <td>0.5529780231447075</td>\n",
              "      <td>0.10864461045891143</td>\n",
              "      <td>0.010660980810234541</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.021097046413502112</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8043752629364745</td>\n",
              "      <td>0.045664443304258596</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.08734052993130521</td>\n",
              "      <td>0.9012634037583608</td>\n",
              "      <td>0.7946638207043757</td>\n",
              "      <td>0.04104104104104104</td>\n",
              "      <td>0.9111111111111111</td>\n",
              "      <td>0.078544061302682</td>\n",
              "      <td>0.8523227969348659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8533866217921751</td>\n",
              "      <td>0.06001348617666891</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.11323155216284986</td>\n",
              "      <td>0.9260006370102983</td>\n",
              "      <td>0.8379935965848453</td>\n",
              "      <td>0.04580152671755725</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.08664259927797834</td>\n",
              "      <td>0.8191810344827587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8904080774084981</td>\n",
              "      <td>0.07869142351900972</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.14590163934426228</td>\n",
              "      <td>0.9446862724280709</td>\n",
              "      <td>0.8670224119530416</td>\n",
              "      <td>0.04984423676012461</td>\n",
              "      <td>0.7111111111111111</td>\n",
              "      <td>0.09315866084425035</td>\n",
              "      <td>0.7898227969348659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9462557846024401</td>\n",
              "      <td>0.14833333333333334</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25834542815674894</td>\n",
              "      <td>0.9728739781293131</td>\n",
              "      <td>0.9212379935965849</td>\n",
              "      <td>0.07142857142857142</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.1276595744680851</td>\n",
              "      <td>0.762176724137931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9608750525872949</td>\n",
              "      <td>0.19305856832971802</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3236363636363636</td>\n",
              "      <td>0.9802526807516722</td>\n",
              "      <td>0.9346851654215582</td>\n",
              "      <td>0.0749185667752443</td>\n",
              "      <td>0.5111111111111111</td>\n",
              "      <td>0.13068181818181818</td>\n",
              "      <td>0.7249521072796934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9700252419015566</td>\n",
              "      <td>0.23796791443850268</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.38444924406047515</td>\n",
              "      <td>0.9848710054145876</td>\n",
              "      <td>0.9451440768409819</td>\n",
              "      <td>0.07258064516129033</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.12286689419795223</td>\n",
              "      <td>0.6752155172413793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.987273874631889</td>\n",
              "      <td>0.4238095238095238</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5953177257525083</td>\n",
              "      <td>0.9935768128251407</td>\n",
              "      <td>0.9658484525080042</td>\n",
              "      <td>0.0979020979020979</td>\n",
              "      <td>0.3111111111111111</td>\n",
              "      <td>0.14893617021276595</td>\n",
              "      <td>0.6416546934865901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9937946992006731</td>\n",
              "      <td>0.6013513513513513</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.751054852320675</td>\n",
              "      <td>0.9968680326998619</td>\n",
              "      <td>0.975880469583778</td>\n",
              "      <td>0.10465116279069768</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.13740458015267176</td>\n",
              "      <td>0.5917025862068965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9882604055496265</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.1111111111111111</td>\n",
              "      <td>0.15384615384615383</td>\n",
              "      <td>0.5539391762452107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3311f90b-d902-4126-a5f1-a3d79c6aeb6d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3311f90b-d902-4126-a5f1-a3d79c6aeb6d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3311f90b-d902-4126-a5f1-a3d79c6aeb6d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A threshold based off of a false negative rate of .9 results in the highest precision and f score for this test data set, though its auc isn't the best.  A threshold based off of the fnr being at .4 or .5 results in a better balance for precision, fscore, and auc."
      ],
      "metadata": {
        "id": "Vq0Yy3PTpwZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample weighting scheme"
      ],
      "metadata": {
        "id": "9CkVwZAkixFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since less than 1% of the observations having a target of 1, this is very imbalanced dataset.  As this is troublesome for machine learning models, we will proceed by balancing the class distributions with different techniques and compare their effects.  The class balancing will only occur on the training set."
      ],
      "metadata": {
        "id": "wkovNXHEj-R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE\n",
        "\n",
        "SMOTE (synthetic minority oversampling technique) is a class balancing technique that we can experiment with.\n",
        "\n",
        "On a high level, SMOTE randomly picks a point from the minority class and computes KNN on it.  New synthetic points are added between this point and the neighbors, which increases instances of the minority class, hence being an oversampling technique.  "
      ],
      "metadata": {
        "id": "8bytRHZDkQ7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use smote to balance data\n",
        "\n",
        "smote = SMOTE(random_state=RANDOM_STATE)\n",
        "smote_x, smote_y = smote.fit_resample(x_train, y_train)\n",
        "smote_x = pd.DataFrame(data=smote_x, columns=x_train.columns)\n",
        "smote_y = pd.Series(data=smote_y, name = 'target')"
      ],
      "metadata": {
        "id": "SFGxhkNWj-GA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use smote-balanced data to train catboost\n",
        "\n",
        "cat_smote = CatBoostClassifier(random_state=RANDOM_STATE)\n",
        "cat_smote.fit(smote_x, smote_y, eval_set=(x_test, y_test))\n",
        "\n",
        "\n",
        "# getting the predictions\n",
        "cat_smote_train_pred = cat_smote.predict(smote_x)\n",
        "cat_smote_test_pred = cat_smote.predict(x_test)"
      ],
      "metadata": {
        "id": "JdJ4pClKNV7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TOMEK links\n",
        "\n",
        "Another balancing method we can look at is Tomek links, which is a pair of observations that are are close together, but have different classes.  By removing the majority class within a pair, we can undersample the data."
      ],
      "metadata": {
        "id": "fWc1mnjNkuX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tomek links to balance data\n",
        "\n",
        "tl = TomekLinks(sampling_strategy='majority')\n",
        "tl_x, tl_y = tl.fit_resample(x_train, y_train)\n",
        "tl_x = pd.DataFrame(data = tl_x, columns = x_train.columns)\n",
        "tl_y = pd.Series(data=tl_y, name = 'target')"
      ],
      "metadata": {
        "id": "GVjgAhyWkuGb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use tomek links-balanced data to train catboost\n",
        "\n",
        "cat_tl = CatBoostClassifier(random_state=RANDOM_STATE)\n",
        "cat_tl.fit(tl_x, tl_y, eval_set=(x_test, y_test))\n",
        "\n",
        "\n",
        "# getting the predictions\n",
        "cat_tl_train_pred = cat_tl.predict(tl_x)\n",
        "cat_tl_test_pred = cat_tl.predict(x_test)"
      ],
      "metadata": {
        "id": "R4Z9Sj_TN401"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SmoteTomek\n",
        "\n",
        "The last balancing method we will look at is Smote Tomek.  It essentially combines Smote and Tomek links by first using SMOTE then cleaning with Tomek Links."
      ],
      "metadata": {
        "id": "Bo0J0s1Lkt4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using SmoteTomek to balance data\n",
        "\n",
        "smo_tl = SMOTETomek(random_state=RANDOM_STATE)\n",
        "smo_tl_x, smo_tl_y = smo_tl.fit_resample(x_train, y_train)\n",
        "smo_tl_x = pd.DataFrame(data = smo_tl_x, columns = x_train.columns)\n",
        "smo_tl_y = pd.Series(data=smo_tl_y, name = 'target')"
      ],
      "metadata": {
        "id": "AKHBPubslQEv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use smotetomek-balanced data to train catboost\n",
        "\n",
        "cat_smo_tl = CatBoostClassifier(random_state=RANDOM_STATE)\n",
        "cat_smo_tl.fit(smo_tl_x, smo_tl_y, eval_set=(x_test, y_test))\n",
        "\n",
        "\n",
        "# getting the predictions\n",
        "cat_smo_tl_train_pred = cat_smo_tl.predict(smo_tl_x)\n",
        "cat_smo_tl_test_pred = cat_smo_tl.predict(x_test)"
      ],
      "metadata": {
        "id": "-JIKlFg_lRs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores = all_scores.append(get_scores(cat_smote_train_pred, cat_smote_test_pred, smote_y, y_test, 'cat', 'smote'))\n",
        "all_scores = all_scores.append(get_scores(cat_tl_train_pred, cat_tl_test_pred, tl_y, y_test, 'cat', 'tomek'))\n",
        "all_scores = all_scores.append(get_scores(cat_smo_tl_train_pred, cat_smo_tl_test_pred, smo_tl_y, y_test, 'cat', 'smote_tomek'))\n",
        "\n",
        "all_scores.set_index(['Model_version', 'Model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "vEqKzQJYOEXI",
        "outputId": "bf5fd0e8-6afb-46b5-f276-088f8c5a7bff"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         Train_acc            Train_prec  \\\n",
              "Model_version           Model                                              \n",
              "benchmark               lr      0.9906394615061002                   0.0   \n",
              "                        cat     0.9992637778712663                   1.0   \n",
              "                        xgb     0.9912705090450147                   1.0   \n",
              "maximizing precision    cat     0.9906394615061002                   0.0   \n",
              "pred_thresh_for_fpr_0.1 cat     0.9252208666386201               0.11125   \n",
              "pred_thresh_for_fpr_0.2 cat     0.8113167858645352   0.04726500265533723   \n",
              "pred_thresh_for_fpr_0.3 cat     0.7164493058477072   0.03195691202872531   \n",
              "pred_thresh_for_fpr_0.4 cat      0.612852334875894  0.023607427055702918   \n",
              "pred_thresh_for_fpr_0.5 cat      0.515776188472865  0.018964415086298744   \n",
              "pred_thresh_for_fpr_0.6 cat    0.41838451830037865   0.01583911728065492   \n",
              "pred_thresh_for_fpr_0.7 cat     0.3148927219183845  0.013478721793124338   \n",
              "pred_thresh_for_fpr_0.8 cat     0.2123474968447623  0.011744523621008182   \n",
              "pred_thresh_for_fpr_0.9 cat    0.11432477913336138   0.01045828437132785   \n",
              "pred_thresh_for_fnr_0.1 cat     0.8043752629364745  0.045664443304258596   \n",
              "pred_thresh_for_fnr_0.2 cat     0.8533866217921751   0.06001348617666891   \n",
              "pred_thresh_for_fnr_0.3 cat     0.8904080774084981   0.07869142351900972   \n",
              "pred_thresh_for_fnr_0.4 cat     0.9462557846024401   0.14833333333333334   \n",
              "pred_thresh_for_fnr_0.5 cat     0.9608750525872949   0.19305856832971802   \n",
              "pred_thresh_for_fnr_0.6 cat     0.9700252419015566   0.23796791443850268   \n",
              "pred_thresh_for_fnr_0.7 cat      0.987273874631889    0.4238095238095238   \n",
              "pred_thresh_for_fnr_0.8 cat     0.9937946992006731    0.6013513513513513   \n",
              "pred_thresh_for_fnr_0.9 cat                    1.0                   1.0   \n",
              "smote                   cat     0.9998938316169445    0.9997877083112197   \n",
              "tomek                   cat     0.9942923581016806                   1.0   \n",
              "smote_tomek             cat     0.9998938316169445    0.9997877083112197   \n",
              "\n",
              "                                      Train_recall          Train_fscore  \\\n",
              "Model_version           Model                                              \n",
              "benchmark               lr                     0.0                   0.0   \n",
              "                        cat     0.9213483146067416    0.9590643274853802   \n",
              "                        xgb    0.06741573033707865   0.12631578947368421   \n",
              "maximizing precision    cat                    0.0                   0.0   \n",
              "pred_thresh_for_fpr_0.1 cat                    1.0   0.20022497187851518   \n",
              "pred_thresh_for_fpr_0.2 cat                    1.0   0.09026369168356999   \n",
              "pred_thresh_for_fpr_0.3 cat                    1.0   0.06193458594293667   \n",
              "pred_thresh_for_fpr_0.4 cat                    1.0   0.04612593936252915   \n",
              "pred_thresh_for_fpr_0.5 cat                    1.0   0.03722291928063572   \n",
              "pred_thresh_for_fpr_0.6 cat                    1.0  0.031184302733006306   \n",
              "pred_thresh_for_fpr_0.7 cat                    1.0   0.02659892408846384   \n",
              "pred_thresh_for_fpr_0.8 cat                    1.0   0.02321638189643929   \n",
              "pred_thresh_for_fpr_0.9 cat                    1.0  0.020700081404814514   \n",
              "pred_thresh_for_fnr_0.1 cat                    1.0   0.08734052993130521   \n",
              "pred_thresh_for_fnr_0.2 cat                    1.0   0.11323155216284986   \n",
              "pred_thresh_for_fnr_0.3 cat                    1.0   0.14590163934426228   \n",
              "pred_thresh_for_fnr_0.4 cat                    1.0   0.25834542815674894   \n",
              "pred_thresh_for_fnr_0.5 cat                    1.0    0.3236363636363636   \n",
              "pred_thresh_for_fnr_0.6 cat                    1.0   0.38444924406047515   \n",
              "pred_thresh_for_fnr_0.7 cat                    1.0    0.5953177257525083   \n",
              "pred_thresh_for_fnr_0.8 cat                    1.0     0.751054852320675   \n",
              "pred_thresh_for_fnr_0.9 cat                    1.0                   1.0   \n",
              "smote                   cat                    1.0    0.9998938428874735   \n",
              "tomek                   cat    0.39325842696629215     0.564516129032258   \n",
              "smote_tomek             cat                    1.0    0.9998938428874735   \n",
              "\n",
              "                                        Train_auc             Test_acc  \\\n",
              "Model_version           Model                                            \n",
              "benchmark               lr                    0.5   0.9903948772678762   \n",
              "                        cat    0.9606741573033708   0.9901814300960512   \n",
              "                        xgb    0.5337078651685393   0.9901814300960512   \n",
              "maximizing precision    cat                   0.5   0.9903948772678762   \n",
              "pred_thresh_for_fpr_0.1 cat    0.9622571398237605   0.8975453575240128   \n",
              "pred_thresh_for_fpr_0.2 cat    0.9047669603991931   0.8008537886872998   \n",
              "pred_thresh_for_fpr_0.3 cat    0.8568850196411508   0.7026680896478121   \n",
              "pred_thresh_for_fpr_0.4 cat    0.8045970909863043   0.6036286019210245   \n",
              "pred_thresh_for_fpr_0.5 cat     0.755600382206179   0.5045891141942369   \n",
              "pred_thresh_for_fpr_0.6 cat    0.7064444208514704   0.4057630736392743   \n",
              "pred_thresh_for_fpr_0.7 cat    0.6542095763881516  0.30672358591248666   \n",
              "pred_thresh_for_fpr_0.8 cat    0.6024524896485827  0.20768409818569905   \n",
              "pred_thresh_for_fpr_0.9 cat    0.5529780231447075  0.10864461045891143   \n",
              "pred_thresh_for_fnr_0.1 cat    0.9012634037583608   0.7946638207043757   \n",
              "pred_thresh_for_fnr_0.2 cat    0.9260006370102983   0.8379935965848453   \n",
              "pred_thresh_for_fnr_0.3 cat    0.9446862724280709   0.8670224119530416   \n",
              "pred_thresh_for_fnr_0.4 cat    0.9728739781293131   0.9212379935965849   \n",
              "pred_thresh_for_fnr_0.5 cat    0.9802526807516722   0.9346851654215582   \n",
              "pred_thresh_for_fnr_0.6 cat    0.9848710054145876   0.9451440768409819   \n",
              "pred_thresh_for_fnr_0.7 cat    0.9935768128251407   0.9658484525080042   \n",
              "pred_thresh_for_fnr_0.8 cat    0.9968680326998619    0.975880469583778   \n",
              "pred_thresh_for_fnr_0.9 cat                   1.0   0.9882604055496265   \n",
              "smote                   cat    0.9998938316169445   0.9820704375667022   \n",
              "tomek                   cat    0.6966292134831461   0.9899679829242263   \n",
              "smote_tomek             cat    0.9998938316169445   0.9820704375667022   \n",
              "\n",
              "                                          Test_prec          Test_recall  \\\n",
              "Model_version           Model                                              \n",
              "benchmark               lr                      0.0                  0.0   \n",
              "                        cat                     0.0                  0.0   \n",
              "                        xgb                     0.0                  0.0   \n",
              "maximizing precision    cat                     0.0                  0.0   \n",
              "pred_thresh_for_fpr_0.1 cat    0.058823529411764705   0.6444444444444445   \n",
              "pred_thresh_for_fpr_0.2 cat     0.04132231404958678   0.8888888888888888   \n",
              "pred_thresh_for_fpr_0.3 cat     0.03064066852367688   0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.4 cat    0.023157894736842106   0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.5 cat    0.018612521150592216   0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.6 cat    0.015906680805938492                  1.0   \n",
              "pred_thresh_for_fpr_0.7 cat    0.013665350744002429                  1.0   \n",
              "pred_thresh_for_fpr_0.8 cat    0.011977641735427202                  1.0   \n",
              "pred_thresh_for_fpr_0.9 cat    0.010660980810234541                  1.0   \n",
              "pred_thresh_for_fnr_0.1 cat     0.04104104104104104   0.9111111111111111   \n",
              "pred_thresh_for_fnr_0.2 cat     0.04580152671755725                  0.8   \n",
              "pred_thresh_for_fnr_0.3 cat     0.04984423676012461   0.7111111111111111   \n",
              "pred_thresh_for_fnr_0.4 cat     0.07142857142857142                  0.6   \n",
              "pred_thresh_for_fnr_0.5 cat      0.0749185667752443   0.5111111111111111   \n",
              "pred_thresh_for_fnr_0.6 cat     0.07258064516129033                  0.4   \n",
              "pred_thresh_for_fnr_0.7 cat      0.0979020979020979   0.3111111111111111   \n",
              "pred_thresh_for_fnr_0.8 cat     0.10465116279069768                  0.2   \n",
              "pred_thresh_for_fnr_0.9 cat                    0.25   0.1111111111111111   \n",
              "smote                   cat      0.0851063829787234  0.08888888888888889   \n",
              "tomek                   cat                     0.0                  0.0   \n",
              "smote_tomek             cat      0.0851063829787234  0.08888888888888889   \n",
              "\n",
              "                                        Test_fscore            Test_auc  \n",
              "Model_version           Model                                            \n",
              "benchmark               lr                      0.0                 0.5  \n",
              "                        cat                     0.0  0.4998922413793103  \n",
              "                        xgb                     0.0  0.4998922413793103  \n",
              "maximizing precision    cat                     0.0                 0.5  \n",
              "pred_thresh_for_fpr_0.1 cat     0.10780669144981413  0.7722222222222223  \n",
              "pred_thresh_for_fpr_0.2 cat     0.07897334649555775  0.8444444444444444  \n",
              "pred_thresh_for_fpr_0.3 cat    0.059419311276164746  0.8388888888888888  \n",
              "pred_thresh_for_fpr_0.4 cat     0.04524421593830334   0.788888888888889  \n",
              "pred_thresh_for_fpr_0.5 cat      0.0365296803652968  0.7388888888888889  \n",
              "pred_thresh_for_fpr_0.6 cat    0.031315240083507306                 0.7  \n",
              "pred_thresh_for_fpr_0.7 cat     0.02696225284601558                0.65  \n",
              "pred_thresh_for_fpr_0.8 cat    0.023671751709626515                 0.6  \n",
              "pred_thresh_for_fpr_0.9 cat    0.021097046413502112                0.55  \n",
              "pred_thresh_for_fnr_0.1 cat       0.078544061302682  0.8523227969348659  \n",
              "pred_thresh_for_fnr_0.2 cat     0.08664259927797834  0.8191810344827587  \n",
              "pred_thresh_for_fnr_0.3 cat     0.09315866084425035  0.7898227969348659  \n",
              "pred_thresh_for_fnr_0.4 cat      0.1276595744680851   0.762176724137931  \n",
              "pred_thresh_for_fnr_0.5 cat     0.13068181818181818  0.7249521072796934  \n",
              "pred_thresh_for_fnr_0.6 cat     0.12286689419795223  0.6752155172413793  \n",
              "pred_thresh_for_fnr_0.7 cat     0.14893617021276595  0.6416546934865901  \n",
              "pred_thresh_for_fnr_0.8 cat     0.13740458015267176  0.5917025862068965  \n",
              "pred_thresh_for_fnr_0.9 cat     0.15384615384615383  0.5539391762452107  \n",
              "smote                   cat     0.08695652173913045  0.5398108237547892  \n",
              "tomek                   cat                     0.0  0.4997844827586207  \n",
              "smote_tomek             cat     0.08695652173913045  0.5398108237547892  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb5b8cbf-63da-4875-ab83-bffdce28c15e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Train_acc</th>\n",
              "      <th>Train_prec</th>\n",
              "      <th>Train_recall</th>\n",
              "      <th>Train_fscore</th>\n",
              "      <th>Train_auc</th>\n",
              "      <th>Test_acc</th>\n",
              "      <th>Test_prec</th>\n",
              "      <th>Test_recall</th>\n",
              "      <th>Test_fscore</th>\n",
              "      <th>Test_auc</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model_version</th>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">benchmark</th>\n",
              "      <th>lr</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.9992637778712663</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9213483146067416</td>\n",
              "      <td>0.9590643274853802</td>\n",
              "      <td>0.9606741573033708</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xgb</th>\n",
              "      <td>0.9912705090450147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06741573033707865</td>\n",
              "      <td>0.12631578947368421</td>\n",
              "      <td>0.5337078651685393</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maximizing precision</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9252208666386201</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.20022497187851518</td>\n",
              "      <td>0.9622571398237605</td>\n",
              "      <td>0.8975453575240128</td>\n",
              "      <td>0.058823529411764705</td>\n",
              "      <td>0.6444444444444445</td>\n",
              "      <td>0.10780669144981413</td>\n",
              "      <td>0.7722222222222223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8113167858645352</td>\n",
              "      <td>0.04726500265533723</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.09026369168356999</td>\n",
              "      <td>0.9047669603991931</td>\n",
              "      <td>0.8008537886872998</td>\n",
              "      <td>0.04132231404958678</td>\n",
              "      <td>0.8888888888888888</td>\n",
              "      <td>0.07897334649555775</td>\n",
              "      <td>0.8444444444444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.7164493058477072</td>\n",
              "      <td>0.03195691202872531</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06193458594293667</td>\n",
              "      <td>0.8568850196411508</td>\n",
              "      <td>0.7026680896478121</td>\n",
              "      <td>0.03064066852367688</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.059419311276164746</td>\n",
              "      <td>0.8388888888888888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.612852334875894</td>\n",
              "      <td>0.023607427055702918</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.04612593936252915</td>\n",
              "      <td>0.8045970909863043</td>\n",
              "      <td>0.6036286019210245</td>\n",
              "      <td>0.023157894736842106</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.04524421593830334</td>\n",
              "      <td>0.788888888888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.515776188472865</td>\n",
              "      <td>0.018964415086298744</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.03722291928063572</td>\n",
              "      <td>0.755600382206179</td>\n",
              "      <td>0.5045891141942369</td>\n",
              "      <td>0.018612521150592216</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.0365296803652968</td>\n",
              "      <td>0.7388888888888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.41838451830037865</td>\n",
              "      <td>0.01583911728065492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031184302733006306</td>\n",
              "      <td>0.7064444208514704</td>\n",
              "      <td>0.4057630736392743</td>\n",
              "      <td>0.015906680805938492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031315240083507306</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.3148927219183845</td>\n",
              "      <td>0.013478721793124338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02659892408846384</td>\n",
              "      <td>0.6542095763881516</td>\n",
              "      <td>0.30672358591248666</td>\n",
              "      <td>0.013665350744002429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02696225284601558</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.2123474968447623</td>\n",
              "      <td>0.011744523621008182</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02321638189643929</td>\n",
              "      <td>0.6024524896485827</td>\n",
              "      <td>0.20768409818569905</td>\n",
              "      <td>0.011977641735427202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.023671751709626515</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.11432477913336138</td>\n",
              "      <td>0.01045828437132785</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.020700081404814514</td>\n",
              "      <td>0.5529780231447075</td>\n",
              "      <td>0.10864461045891143</td>\n",
              "      <td>0.010660980810234541</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.021097046413502112</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8043752629364745</td>\n",
              "      <td>0.045664443304258596</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.08734052993130521</td>\n",
              "      <td>0.9012634037583608</td>\n",
              "      <td>0.7946638207043757</td>\n",
              "      <td>0.04104104104104104</td>\n",
              "      <td>0.9111111111111111</td>\n",
              "      <td>0.078544061302682</td>\n",
              "      <td>0.8523227969348659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8533866217921751</td>\n",
              "      <td>0.06001348617666891</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.11323155216284986</td>\n",
              "      <td>0.9260006370102983</td>\n",
              "      <td>0.8379935965848453</td>\n",
              "      <td>0.04580152671755725</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.08664259927797834</td>\n",
              "      <td>0.8191810344827587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8904080774084981</td>\n",
              "      <td>0.07869142351900972</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.14590163934426228</td>\n",
              "      <td>0.9446862724280709</td>\n",
              "      <td>0.8670224119530416</td>\n",
              "      <td>0.04984423676012461</td>\n",
              "      <td>0.7111111111111111</td>\n",
              "      <td>0.09315866084425035</td>\n",
              "      <td>0.7898227969348659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9462557846024401</td>\n",
              "      <td>0.14833333333333334</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25834542815674894</td>\n",
              "      <td>0.9728739781293131</td>\n",
              "      <td>0.9212379935965849</td>\n",
              "      <td>0.07142857142857142</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.1276595744680851</td>\n",
              "      <td>0.762176724137931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9608750525872949</td>\n",
              "      <td>0.19305856832971802</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3236363636363636</td>\n",
              "      <td>0.9802526807516722</td>\n",
              "      <td>0.9346851654215582</td>\n",
              "      <td>0.0749185667752443</td>\n",
              "      <td>0.5111111111111111</td>\n",
              "      <td>0.13068181818181818</td>\n",
              "      <td>0.7249521072796934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9700252419015566</td>\n",
              "      <td>0.23796791443850268</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.38444924406047515</td>\n",
              "      <td>0.9848710054145876</td>\n",
              "      <td>0.9451440768409819</td>\n",
              "      <td>0.07258064516129033</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.12286689419795223</td>\n",
              "      <td>0.6752155172413793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.987273874631889</td>\n",
              "      <td>0.4238095238095238</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5953177257525083</td>\n",
              "      <td>0.9935768128251407</td>\n",
              "      <td>0.9658484525080042</td>\n",
              "      <td>0.0979020979020979</td>\n",
              "      <td>0.3111111111111111</td>\n",
              "      <td>0.14893617021276595</td>\n",
              "      <td>0.6416546934865901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9937946992006731</td>\n",
              "      <td>0.6013513513513513</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.751054852320675</td>\n",
              "      <td>0.9968680326998619</td>\n",
              "      <td>0.975880469583778</td>\n",
              "      <td>0.10465116279069768</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.13740458015267176</td>\n",
              "      <td>0.5917025862068965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9882604055496265</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.1111111111111111</td>\n",
              "      <td>0.15384615384615383</td>\n",
              "      <td>0.5539391762452107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smote</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9997877083112197</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9998938428874735</td>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9820704375667022</td>\n",
              "      <td>0.0851063829787234</td>\n",
              "      <td>0.08888888888888889</td>\n",
              "      <td>0.08695652173913045</td>\n",
              "      <td>0.5398108237547892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tomek</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9942923581016806</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39325842696629215</td>\n",
              "      <td>0.564516129032258</td>\n",
              "      <td>0.6966292134831461</td>\n",
              "      <td>0.9899679829242263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4997844827586207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smote_tomek</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9997877083112197</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9998938428874735</td>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9820704375667022</td>\n",
              "      <td>0.0851063829787234</td>\n",
              "      <td>0.08888888888888889</td>\n",
              "      <td>0.08695652173913045</td>\n",
              "      <td>0.5398108237547892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb5b8cbf-63da-4875-ab83-bffdce28c15e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb5b8cbf-63da-4875-ab83-bffdce28c15e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb5b8cbf-63da-4875-ab83-bffdce28c15e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smote_tomek and smote seem to perform the same on the test set and training set with benchmark catboost model.  We will go with smote_tomek for the final model since it is a bit more complex and may perform better on future unseen data."
      ],
      "metadata": {
        "id": "t-beeO-Zwj_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cat classifier with smotetomek and threshold - Final Model"
      ],
      "metadata": {
        "id": "ZHtA5f8VkCEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will combine smotetomek with our previous method of looking at the fnr and fpr rates to get the threshold.  The previous thresholds won't work since the data has changed."
      ],
      "metadata": {
        "id": "KXyZmSfHtFGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pool = Pool(x_test, y_test)\n",
        "curve = get_roc_curve(cat_smo_tl, cat_pool)\n",
        "thresh_l_fpr = []\n",
        "thresh_l_fnr = []\n",
        "rates = [.1,.2, .3, .4, .5, .6, .7, .8, .9,]\n",
        "for i in rates:\n",
        "  thresh_l_fpr.append(select_threshold(cat_smo_tl, curve=curve, FPR = i))\n",
        "  thresh_l_fnr.append(select_threshold(cat_smo_tl, curve=curve, FNR = i))"
      ],
      "metadata": {
        "id": "n2PsVcTEtEcl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting predicitons for benchmark catboost model\n",
        "cat_smo_tl_train_thresh = cat_smo_tl.predict_proba(x_train)\n",
        "cat_smo_tl_test_thresh = cat_smo_tl.predict_proba(x_test)"
      ],
      "metadata": {
        "id": "fQq9GvnvtUEJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting scores for different thresholds based off different fpr and npr rates\n",
        "\n",
        "for i in range(len(thresh_l_fpr)):\n",
        "  cat_smo_tl_train_thresh_pred = [1 if cat_smo_tl_train_thresh[j][1] > thresh_l_fpr[i] else 0 for j in range(len(cat_smo_tl_train_thresh))]\n",
        "  cat_smo_tl_test_thresh_pred = [1 if cat_smo_tl_test_thresh[j][1] > thresh_l_fpr[i] else 0 for j in range(len(cat_smo_tl_test_thresh))]\n",
        "  all_scores = all_scores.append(get_scores(cat_smo_tl_train_thresh_pred,\n",
        "                                          cat_smo_tl_test_thresh_pred,\n",
        "                                          y_train, y_test,\n",
        "                                          'cat', 'smo_tl_pred_thresh_for_fpr_' + str(rates[i])))\n",
        "  \n",
        "for i in range(len(thresh_l_fnr)):\n",
        "  cat_smo_tl_train_thresh_pred = [1 if cat_smo_tl_train_thresh[j][1] > thresh_l_fnr[i] else 0 for j in range(len(cat_smo_tl_train_thresh))]\n",
        "  cat_smo_tl_test_thresh_pred = [1 if cat_smo_tl_test_thresh[j][1] > thresh_l_fnr[i] else 0 for j in range(len(cat_smo_tl_test_thresh))]\n",
        "  all_scores = all_scores.append(get_scores(cat_smo_tl_train_thresh_pred,\n",
        "                                          cat_smo_tl_test_thresh_pred,\n",
        "                                          y_train, y_test,\n",
        "                                          'cat', 'smo_tl_pred_thresh_for_fnr_' + str(rates[i])))"
      ],
      "metadata": {
        "id": "cBo2Jglex6I1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores.set_index(['Model_version', 'Model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CMnmrrMuY6T2",
        "outputId": "3f6e22e8-02fd-43d5-dc7e-91ec1a15875f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Train_acc  \\\n",
              "Model_version                  Model                        \n",
              "benchmark                      lr      0.9906394615061002   \n",
              "                               cat     0.9992637778712663   \n",
              "                               xgb     0.9912705090450147   \n",
              "maximizing precision           cat     0.9906394615061002   \n",
              "pred_thresh_for_fpr_0.1        cat     0.9252208666386201   \n",
              "pred_thresh_for_fpr_0.2        cat     0.8113167858645352   \n",
              "pred_thresh_for_fpr_0.3        cat     0.7164493058477072   \n",
              "pred_thresh_for_fpr_0.4        cat      0.612852334875894   \n",
              "pred_thresh_for_fpr_0.5        cat      0.515776188472865   \n",
              "pred_thresh_for_fpr_0.6        cat    0.41838451830037865   \n",
              "pred_thresh_for_fpr_0.7        cat     0.3148927219183845   \n",
              "pred_thresh_for_fpr_0.8        cat     0.2123474968447623   \n",
              "pred_thresh_for_fpr_0.9        cat    0.11432477913336138   \n",
              "pred_thresh_for_fnr_0.1        cat     0.8043752629364745   \n",
              "pred_thresh_for_fnr_0.2        cat     0.8533866217921751   \n",
              "pred_thresh_for_fnr_0.3        cat     0.8904080774084981   \n",
              "pred_thresh_for_fnr_0.4        cat     0.9462557846024401   \n",
              "pred_thresh_for_fnr_0.5        cat     0.9608750525872949   \n",
              "pred_thresh_for_fnr_0.6        cat     0.9700252419015566   \n",
              "pred_thresh_for_fnr_0.7        cat      0.987273874631889   \n",
              "pred_thresh_for_fnr_0.8        cat     0.9937946992006731   \n",
              "pred_thresh_for_fnr_0.9        cat                    1.0   \n",
              "smote                          cat     0.9998938316169445   \n",
              "tomek                          cat     0.9942923581016806   \n",
              "smote_tomek                    cat     0.9998938316169445   \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat     0.9175431215818258   \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat     0.8105805637358015   \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat     0.7132940681531342   \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat     0.6182162389566681   \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat     0.5287126630206143   \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat    0.42280185107278084   \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat     0.3222549432057215   \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat    0.21518721076987798   \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat    0.11379890618426589   \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat     0.8197307530500632   \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat     0.8452881783761044   \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat     0.9080774084981068   \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat     0.9601388304585612   \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat     0.9750736222128734   \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat     0.9832772402187632   \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat     0.9880100967606227   \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat     0.9994741270509045   \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat     0.9997896508203618   \n",
              "\n",
              "                                                Train_prec  \\\n",
              "Model_version                  Model                         \n",
              "benchmark                      lr                      0.0   \n",
              "                               cat                     1.0   \n",
              "                               xgb                     1.0   \n",
              "maximizing precision           cat                     0.0   \n",
              "pred_thresh_for_fpr_0.1        cat                 0.11125   \n",
              "pred_thresh_for_fpr_0.2        cat     0.04726500265533723   \n",
              "pred_thresh_for_fpr_0.3        cat     0.03195691202872531   \n",
              "pred_thresh_for_fpr_0.4        cat    0.023607427055702918   \n",
              "pred_thresh_for_fpr_0.5        cat    0.018964415086298744   \n",
              "pred_thresh_for_fpr_0.6        cat     0.01583911728065492   \n",
              "pred_thresh_for_fpr_0.7        cat    0.013478721793124338   \n",
              "pred_thresh_for_fpr_0.8        cat    0.011744523621008182   \n",
              "pred_thresh_for_fpr_0.9        cat     0.01045828437132785   \n",
              "pred_thresh_for_fnr_0.1        cat    0.045664443304258596   \n",
              "pred_thresh_for_fnr_0.2        cat     0.06001348617666891   \n",
              "pred_thresh_for_fnr_0.3        cat     0.07869142351900972   \n",
              "pred_thresh_for_fnr_0.4        cat     0.14833333333333334   \n",
              "pred_thresh_for_fnr_0.5        cat     0.19305856832971802   \n",
              "pred_thresh_for_fnr_0.6        cat     0.23796791443850268   \n",
              "pred_thresh_for_fnr_0.7        cat      0.4238095238095238   \n",
              "pred_thresh_for_fnr_0.8        cat      0.6013513513513513   \n",
              "pred_thresh_for_fnr_0.9        cat                     1.0   \n",
              "smote                          cat      0.9997877083112197   \n",
              "tomek                          cat                     1.0   \n",
              "smote_tomek                    cat      0.9997877083112197   \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat     0.10194730813287514   \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat     0.04708994708994709   \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat     0.03161634103019538   \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat    0.023931164291476202   \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat     0.01947483588621444   \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat     0.01595840057378519   \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat     0.01362314403796112   \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat    0.011786518341941464   \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat    0.010452143276570758   \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat     0.04936217415418746   \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat     0.05705128205128205   \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat     0.09241952232606439   \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat     0.19017094017094016   \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat     0.27300613496932513   \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat      0.3588709677419355   \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat     0.43842364532019706   \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat      0.9468085106382979   \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat       0.978021978021978   \n",
              "\n",
              "                                             Train_recall  \\\n",
              "Model_version                  Model                        \n",
              "benchmark                      lr                     0.0   \n",
              "                               cat     0.9213483146067416   \n",
              "                               xgb    0.06741573033707865   \n",
              "maximizing precision           cat                    0.0   \n",
              "pred_thresh_for_fpr_0.1        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.2        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.3        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.4        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.5        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.6        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.7        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.8        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.9        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.1        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.2        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.3        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.4        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.5        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.6        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.7        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.8        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.9        cat                    1.0   \n",
              "smote                          cat                    1.0   \n",
              "tomek                          cat    0.39325842696629215   \n",
              "smote_tomek                    cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat                    1.0   \n",
              "\n",
              "                                              Train_fscore  \\\n",
              "Model_version                  Model                         \n",
              "benchmark                      lr                      0.0   \n",
              "                               cat      0.9590643274853802   \n",
              "                               xgb     0.12631578947368421   \n",
              "maximizing precision           cat                     0.0   \n",
              "pred_thresh_for_fpr_0.1        cat     0.20022497187851518   \n",
              "pred_thresh_for_fpr_0.2        cat     0.09026369168356999   \n",
              "pred_thresh_for_fpr_0.3        cat     0.06193458594293667   \n",
              "pred_thresh_for_fpr_0.4        cat     0.04612593936252915   \n",
              "pred_thresh_for_fpr_0.5        cat     0.03722291928063572   \n",
              "pred_thresh_for_fpr_0.6        cat    0.031184302733006306   \n",
              "pred_thresh_for_fpr_0.7        cat     0.02659892408846384   \n",
              "pred_thresh_for_fpr_0.8        cat     0.02321638189643929   \n",
              "pred_thresh_for_fpr_0.9        cat    0.020700081404814514   \n",
              "pred_thresh_for_fnr_0.1        cat     0.08734052993130521   \n",
              "pred_thresh_for_fnr_0.2        cat     0.11323155216284986   \n",
              "pred_thresh_for_fnr_0.3        cat     0.14590163934426228   \n",
              "pred_thresh_for_fnr_0.4        cat     0.25834542815674894   \n",
              "pred_thresh_for_fnr_0.5        cat      0.3236363636363636   \n",
              "pred_thresh_for_fnr_0.6        cat     0.38444924406047515   \n",
              "pred_thresh_for_fnr_0.7        cat      0.5953177257525083   \n",
              "pred_thresh_for_fnr_0.8        cat       0.751054852320675   \n",
              "pred_thresh_for_fnr_0.9        cat                     1.0   \n",
              "smote                          cat      0.9998938428874735   \n",
              "tomek                          cat       0.564516129032258   \n",
              "smote_tomek                    cat      0.9998938428874735   \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat       0.185031185031185   \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat       0.089944416371905   \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat     0.06129476584022039   \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat      0.0467436974789916   \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat     0.03820562352436145   \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat    0.031415460642428526   \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat     0.02688009664753851   \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat    0.023298429319371726   \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat     0.02068805206880521   \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat     0.09408033826638477   \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat     0.10794420861127955   \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat     0.16920152091254753   \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat     0.31956912028725315   \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat      0.4289156626506024   \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat      0.5281899109792285   \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat      0.6095890410958904   \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat      0.9726775956284154   \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat      0.9888888888888888   \n",
              "\n",
              "                                               Train_auc             Test_acc  \\\n",
              "Model_version                  Model                                            \n",
              "benchmark                      lr                    0.5   0.9903948772678762   \n",
              "                               cat    0.9606741573033708   0.9901814300960512   \n",
              "                               xgb    0.5337078651685393   0.9901814300960512   \n",
              "maximizing precision           cat                   0.5   0.9903948772678762   \n",
              "pred_thresh_for_fpr_0.1        cat    0.9622571398237605   0.8975453575240128   \n",
              "pred_thresh_for_fpr_0.2        cat    0.9047669603991931   0.8008537886872998   \n",
              "pred_thresh_for_fpr_0.3        cat    0.8568850196411508   0.7026680896478121   \n",
              "pred_thresh_for_fpr_0.4        cat    0.8045970909863043   0.6036286019210245   \n",
              "pred_thresh_for_fpr_0.5        cat     0.755600382206179   0.5045891141942369   \n",
              "pred_thresh_for_fpr_0.6        cat    0.7064444208514704   0.4057630736392743   \n",
              "pred_thresh_for_fpr_0.7        cat    0.6542095763881516  0.30672358591248666   \n",
              "pred_thresh_for_fpr_0.8        cat    0.6024524896485827  0.20768409818569905   \n",
              "pred_thresh_for_fpr_0.9        cat    0.5529780231447075  0.10864461045891143   \n",
              "pred_thresh_for_fnr_0.1        cat    0.9012634037583608   0.7946638207043757   \n",
              "pred_thresh_for_fnr_0.2        cat    0.9260006370102983   0.8379935965848453   \n",
              "pred_thresh_for_fnr_0.3        cat    0.9446862724280709   0.8670224119530416   \n",
              "pred_thresh_for_fnr_0.4        cat    0.9728739781293131   0.9212379935965849   \n",
              "pred_thresh_for_fnr_0.5        cat    0.9802526807516722   0.9346851654215582   \n",
              "pred_thresh_for_fnr_0.6        cat    0.9848710054145876   0.9451440768409819   \n",
              "pred_thresh_for_fnr_0.7        cat    0.9935768128251407   0.9658484525080042   \n",
              "pred_thresh_for_fnr_0.8        cat    0.9968680326998619    0.975880469583778   \n",
              "pred_thresh_for_fnr_0.9        cat                   1.0   0.9882604055496265   \n",
              "smote                          cat    0.9998938316169445   0.9820704375667022   \n",
              "tomek                          cat    0.6966292134831461   0.9899679829242263   \n",
              "smote_tomek                    cat    0.9998938316169445   0.9820704375667022   \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat    0.9583819938422338   0.8977588046958378   \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat    0.9043953710584988   0.8010672358591249   \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat    0.8552924938953179   0.7026680896478121   \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat    0.8073043847542202   0.6038420490928496   \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat    0.7621297377640939   0.5048025613660619   \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat    0.7086739568956365   0.4057630736392743   \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat     0.657925469795095  0.30672358591248666   \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat    0.6038857628198322  0.20768409818569905   \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat    0.5527126021870687  0.10864461045891143   \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat    0.9090136957214141   0.8117395944503736   \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat    0.9219131542626605   0.8386339381003202   \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat     0.953604416604735   0.8905016008537887   \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat    0.9798810914109778   0.9261472785485593   \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat    0.9874190466079202   0.9372465314834578   \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat    0.9915596135470857   0.9455709711846318   \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat     0.993948402165835   0.9502668089647812   \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat    0.9997345790423612   0.9765208110992529   \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat    0.9998938316169445   0.9822838847385272   \n",
              "\n",
              "                                                 Test_prec  \\\n",
              "Model_version                  Model                         \n",
              "benchmark                      lr                      0.0   \n",
              "                               cat                     0.0   \n",
              "                               xgb                     0.0   \n",
              "maximizing precision           cat                     0.0   \n",
              "pred_thresh_for_fpr_0.1        cat    0.058823529411764705   \n",
              "pred_thresh_for_fpr_0.2        cat     0.04132231404958678   \n",
              "pred_thresh_for_fpr_0.3        cat     0.03064066852367688   \n",
              "pred_thresh_for_fpr_0.4        cat    0.023157894736842106   \n",
              "pred_thresh_for_fpr_0.5        cat    0.018612521150592216   \n",
              "pred_thresh_for_fpr_0.6        cat    0.015906680805938492   \n",
              "pred_thresh_for_fpr_0.7        cat    0.013665350744002429   \n",
              "pred_thresh_for_fpr_0.8        cat    0.011977641735427202   \n",
              "pred_thresh_for_fpr_0.9        cat    0.010660980810234541   \n",
              "pred_thresh_for_fnr_0.1        cat     0.04104104104104104   \n",
              "pred_thresh_for_fnr_0.2        cat     0.04580152671755725   \n",
              "pred_thresh_for_fnr_0.3        cat     0.04984423676012461   \n",
              "pred_thresh_for_fnr_0.4        cat     0.07142857142857142   \n",
              "pred_thresh_for_fnr_0.5        cat      0.0749185667752443   \n",
              "pred_thresh_for_fnr_0.6        cat     0.07258064516129033   \n",
              "pred_thresh_for_fnr_0.7        cat      0.0979020979020979   \n",
              "pred_thresh_for_fnr_0.8        cat     0.10465116279069768   \n",
              "pred_thresh_for_fnr_0.9        cat                    0.25   \n",
              "smote                          cat      0.0851063829787234   \n",
              "tomek                          cat                     0.0   \n",
              "smote_tomek                    cat      0.0851063829787234   \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat     0.06072874493927125   \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat     0.04231166150670795   \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat     0.03064066852367688   \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat     0.02367175170962651   \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat    0.019027484143763214   \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat    0.015906680805938492   \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat    0.013665350744002429   \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat    0.011977641735427202   \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat    0.010660980810234541   \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat     0.04461371055495103   \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat     0.04597701149425287   \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat     0.06015037593984962   \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat     0.07605633802816901   \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat     0.07796610169491526   \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat     0.07317073170731707   \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat     0.06481481481481481   \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat     0.10843373493975904   \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat     0.10416666666666667   \n",
              "\n",
              "                                              Test_recall  \\\n",
              "Model_version                  Model                        \n",
              "benchmark                      lr                     0.0   \n",
              "                               cat                    0.0   \n",
              "                               xgb                    0.0   \n",
              "maximizing precision           cat                    0.0   \n",
              "pred_thresh_for_fpr_0.1        cat     0.6444444444444445   \n",
              "pred_thresh_for_fpr_0.2        cat     0.8888888888888888   \n",
              "pred_thresh_for_fpr_0.3        cat     0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.4        cat     0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.5        cat     0.9777777777777777   \n",
              "pred_thresh_for_fpr_0.6        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.7        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.8        cat                    1.0   \n",
              "pred_thresh_for_fpr_0.9        cat                    1.0   \n",
              "pred_thresh_for_fnr_0.1        cat     0.9111111111111111   \n",
              "pred_thresh_for_fnr_0.2        cat                    0.8   \n",
              "pred_thresh_for_fnr_0.3        cat     0.7111111111111111   \n",
              "pred_thresh_for_fnr_0.4        cat                    0.6   \n",
              "pred_thresh_for_fnr_0.5        cat     0.5111111111111111   \n",
              "pred_thresh_for_fnr_0.6        cat                    0.4   \n",
              "pred_thresh_for_fnr_0.7        cat     0.3111111111111111   \n",
              "pred_thresh_for_fnr_0.8        cat                    0.2   \n",
              "pred_thresh_for_fnr_0.9        cat     0.1111111111111111   \n",
              "smote                          cat    0.08888888888888889   \n",
              "tomek                          cat                    0.0   \n",
              "smote_tomek                    cat    0.08888888888888889   \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat     0.6666666666666666   \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat     0.9111111111111111   \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat     0.9777777777777777   \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat                    1.0   \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat     0.9111111111111111   \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat                    0.8   \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat     0.7111111111111111   \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat                    0.6   \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat     0.5111111111111111   \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat                    0.4   \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat     0.3111111111111111   \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat                    0.2   \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat     0.1111111111111111   \n",
              "\n",
              "                                               Test_fscore            Test_auc  \n",
              "Model_version                  Model                                            \n",
              "benchmark                      lr                      0.0                 0.5  \n",
              "                               cat                     0.0  0.4998922413793103  \n",
              "                               xgb                     0.0  0.4998922413793103  \n",
              "maximizing precision           cat                     0.0                 0.5  \n",
              "pred_thresh_for_fpr_0.1        cat     0.10780669144981413  0.7722222222222223  \n",
              "pred_thresh_for_fpr_0.2        cat     0.07897334649555775  0.8444444444444444  \n",
              "pred_thresh_for_fpr_0.3        cat    0.059419311276164746  0.8388888888888888  \n",
              "pred_thresh_for_fpr_0.4        cat     0.04524421593830334   0.788888888888889  \n",
              "pred_thresh_for_fpr_0.5        cat      0.0365296803652968  0.7388888888888889  \n",
              "pred_thresh_for_fpr_0.6        cat    0.031315240083507306                 0.7  \n",
              "pred_thresh_for_fpr_0.7        cat     0.02696225284601558                0.65  \n",
              "pred_thresh_for_fpr_0.8        cat    0.023671751709626515                 0.6  \n",
              "pred_thresh_for_fpr_0.9        cat    0.021097046413502112                0.55  \n",
              "pred_thresh_for_fnr_0.1        cat       0.078544061302682  0.8523227969348659  \n",
              "pred_thresh_for_fnr_0.2        cat     0.08664259927797834  0.8191810344827587  \n",
              "pred_thresh_for_fnr_0.3        cat     0.09315866084425035  0.7898227969348659  \n",
              "pred_thresh_for_fnr_0.4        cat      0.1276595744680851   0.762176724137931  \n",
              "pred_thresh_for_fnr_0.5        cat     0.13068181818181818  0.7249521072796934  \n",
              "pred_thresh_for_fnr_0.6        cat     0.12286689419795223  0.6752155172413793  \n",
              "pred_thresh_for_fnr_0.7        cat     0.14893617021276595  0.6416546934865901  \n",
              "pred_thresh_for_fnr_0.8        cat     0.13740458015267176  0.5917025862068965  \n",
              "pred_thresh_for_fnr_0.9        cat     0.15384615384615383  0.5539391762452107  \n",
              "smote                          cat     0.08695652173913045  0.5398108237547892  \n",
              "tomek                          cat                     0.0  0.4997844827586207  \n",
              "smote_tomek                    cat     0.08695652173913045  0.5398108237547892  \n",
              "smo_tl_pred_thresh_for_fpr_0.1 cat     0.11131725417439702  0.7833333333333333  \n",
              "smo_tl_pred_thresh_for_fpr_0.2 cat     0.08086785009861934  0.8555555555555556  \n",
              "smo_tl_pred_thresh_for_fpr_0.3 cat    0.059419311276164746  0.8388888888888888  \n",
              "smo_tl_pred_thresh_for_fpr_0.4 cat    0.046248715313463515                 0.8  \n",
              "smo_tl_pred_thresh_for_fpr_0.5 cat     0.03734439834024896                0.75  \n",
              "smo_tl_pred_thresh_for_fpr_0.6 cat    0.031315240083507306                 0.7  \n",
              "smo_tl_pred_thresh_for_fpr_0.7 cat     0.02696225284601558                0.65  \n",
              "smo_tl_pred_thresh_for_fpr_0.8 cat    0.023671751709626515                 0.6  \n",
              "smo_tl_pred_thresh_for_fpr_0.9 cat    0.021097046413502112                0.55  \n",
              "smo_tl_pred_thresh_for_fnr_0.1 cat      0.0850622406639004  0.8609434865900383  \n",
              "smo_tl_pred_thresh_for_fnr_0.2 cat     0.08695652173913043  0.8195043103448276  \n",
              "smo_tl_pred_thresh_for_fnr_0.3 cat     0.11091854419410746   0.801676245210728  \n",
              "smo_tl_pred_thresh_for_fnr_0.4 cat     0.13499999999999998  0.7646551724137932  \n",
              "smo_tl_pred_thresh_for_fnr_0.5 cat     0.13529411764705881  0.7262452107279693  \n",
              "smo_tl_pred_thresh_for_fnr_0.6 cat     0.12371134020618556  0.6754310344827585  \n",
              "smo_tl_pred_thresh_for_fnr_0.7 cat     0.10727969348659003  0.6337883141762453  \n",
              "smo_tl_pred_thresh_for_fnr_0.8 cat                0.140625  0.5920258620689655  \n",
              "smo_tl_pred_thresh_for_fnr_0.9 cat      0.1075268817204301  0.5509219348659005  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac9d2de2-07d4-4086-9767-690cb91be015\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Train_acc</th>\n",
              "      <th>Train_prec</th>\n",
              "      <th>Train_recall</th>\n",
              "      <th>Train_fscore</th>\n",
              "      <th>Train_auc</th>\n",
              "      <th>Test_acc</th>\n",
              "      <th>Test_prec</th>\n",
              "      <th>Test_recall</th>\n",
              "      <th>Test_fscore</th>\n",
              "      <th>Test_auc</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model_version</th>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">benchmark</th>\n",
              "      <th>lr</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.9992637778712663</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9213483146067416</td>\n",
              "      <td>0.9590643274853802</td>\n",
              "      <td>0.9606741573033708</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xgb</th>\n",
              "      <td>0.9912705090450147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06741573033707865</td>\n",
              "      <td>0.12631578947368421</td>\n",
              "      <td>0.5337078651685393</td>\n",
              "      <td>0.9901814300960512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4998922413793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maximizing precision</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9906394615061002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.9903948772678762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9252208666386201</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.20022497187851518</td>\n",
              "      <td>0.9622571398237605</td>\n",
              "      <td>0.8975453575240128</td>\n",
              "      <td>0.058823529411764705</td>\n",
              "      <td>0.6444444444444445</td>\n",
              "      <td>0.10780669144981413</td>\n",
              "      <td>0.7722222222222223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8113167858645352</td>\n",
              "      <td>0.04726500265533723</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.09026369168356999</td>\n",
              "      <td>0.9047669603991931</td>\n",
              "      <td>0.8008537886872998</td>\n",
              "      <td>0.04132231404958678</td>\n",
              "      <td>0.8888888888888888</td>\n",
              "      <td>0.07897334649555775</td>\n",
              "      <td>0.8444444444444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.7164493058477072</td>\n",
              "      <td>0.03195691202872531</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06193458594293667</td>\n",
              "      <td>0.8568850196411508</td>\n",
              "      <td>0.7026680896478121</td>\n",
              "      <td>0.03064066852367688</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.059419311276164746</td>\n",
              "      <td>0.8388888888888888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.612852334875894</td>\n",
              "      <td>0.023607427055702918</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.04612593936252915</td>\n",
              "      <td>0.8045970909863043</td>\n",
              "      <td>0.6036286019210245</td>\n",
              "      <td>0.023157894736842106</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.04524421593830334</td>\n",
              "      <td>0.788888888888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.515776188472865</td>\n",
              "      <td>0.018964415086298744</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.03722291928063572</td>\n",
              "      <td>0.755600382206179</td>\n",
              "      <td>0.5045891141942369</td>\n",
              "      <td>0.018612521150592216</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.0365296803652968</td>\n",
              "      <td>0.7388888888888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.41838451830037865</td>\n",
              "      <td>0.01583911728065492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031184302733006306</td>\n",
              "      <td>0.7064444208514704</td>\n",
              "      <td>0.4057630736392743</td>\n",
              "      <td>0.015906680805938492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031315240083507306</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.3148927219183845</td>\n",
              "      <td>0.013478721793124338</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02659892408846384</td>\n",
              "      <td>0.6542095763881516</td>\n",
              "      <td>0.30672358591248666</td>\n",
              "      <td>0.013665350744002429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02696225284601558</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.2123474968447623</td>\n",
              "      <td>0.011744523621008182</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02321638189643929</td>\n",
              "      <td>0.6024524896485827</td>\n",
              "      <td>0.20768409818569905</td>\n",
              "      <td>0.011977641735427202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.023671751709626515</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fpr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.11432477913336138</td>\n",
              "      <td>0.01045828437132785</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.020700081404814514</td>\n",
              "      <td>0.5529780231447075</td>\n",
              "      <td>0.10864461045891143</td>\n",
              "      <td>0.010660980810234541</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.021097046413502112</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8043752629364745</td>\n",
              "      <td>0.045664443304258596</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.08734052993130521</td>\n",
              "      <td>0.9012634037583608</td>\n",
              "      <td>0.7946638207043757</td>\n",
              "      <td>0.04104104104104104</td>\n",
              "      <td>0.9111111111111111</td>\n",
              "      <td>0.078544061302682</td>\n",
              "      <td>0.8523227969348659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8533866217921751</td>\n",
              "      <td>0.06001348617666891</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.11323155216284986</td>\n",
              "      <td>0.9260006370102983</td>\n",
              "      <td>0.8379935965848453</td>\n",
              "      <td>0.04580152671755725</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.08664259927797834</td>\n",
              "      <td>0.8191810344827587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8904080774084981</td>\n",
              "      <td>0.07869142351900972</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.14590163934426228</td>\n",
              "      <td>0.9446862724280709</td>\n",
              "      <td>0.8670224119530416</td>\n",
              "      <td>0.04984423676012461</td>\n",
              "      <td>0.7111111111111111</td>\n",
              "      <td>0.09315866084425035</td>\n",
              "      <td>0.7898227969348659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9462557846024401</td>\n",
              "      <td>0.14833333333333334</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25834542815674894</td>\n",
              "      <td>0.9728739781293131</td>\n",
              "      <td>0.9212379935965849</td>\n",
              "      <td>0.07142857142857142</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.1276595744680851</td>\n",
              "      <td>0.762176724137931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9608750525872949</td>\n",
              "      <td>0.19305856832971802</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3236363636363636</td>\n",
              "      <td>0.9802526807516722</td>\n",
              "      <td>0.9346851654215582</td>\n",
              "      <td>0.0749185667752443</td>\n",
              "      <td>0.5111111111111111</td>\n",
              "      <td>0.13068181818181818</td>\n",
              "      <td>0.7249521072796934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9700252419015566</td>\n",
              "      <td>0.23796791443850268</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.38444924406047515</td>\n",
              "      <td>0.9848710054145876</td>\n",
              "      <td>0.9451440768409819</td>\n",
              "      <td>0.07258064516129033</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.12286689419795223</td>\n",
              "      <td>0.6752155172413793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.987273874631889</td>\n",
              "      <td>0.4238095238095238</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5953177257525083</td>\n",
              "      <td>0.9935768128251407</td>\n",
              "      <td>0.9658484525080042</td>\n",
              "      <td>0.0979020979020979</td>\n",
              "      <td>0.3111111111111111</td>\n",
              "      <td>0.14893617021276595</td>\n",
              "      <td>0.6416546934865901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9937946992006731</td>\n",
              "      <td>0.6013513513513513</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.751054852320675</td>\n",
              "      <td>0.9968680326998619</td>\n",
              "      <td>0.975880469583778</td>\n",
              "      <td>0.10465116279069768</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.13740458015267176</td>\n",
              "      <td>0.5917025862068965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred_thresh_for_fnr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9882604055496265</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.1111111111111111</td>\n",
              "      <td>0.15384615384615383</td>\n",
              "      <td>0.5539391762452107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smote</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9997877083112197</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9998938428874735</td>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9820704375667022</td>\n",
              "      <td>0.0851063829787234</td>\n",
              "      <td>0.08888888888888889</td>\n",
              "      <td>0.08695652173913045</td>\n",
              "      <td>0.5398108237547892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tomek</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9942923581016806</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39325842696629215</td>\n",
              "      <td>0.564516129032258</td>\n",
              "      <td>0.6966292134831461</td>\n",
              "      <td>0.9899679829242263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4997844827586207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smote_tomek</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9997877083112197</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9998938428874735</td>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9820704375667022</td>\n",
              "      <td>0.0851063829787234</td>\n",
              "      <td>0.08888888888888889</td>\n",
              "      <td>0.08695652173913045</td>\n",
              "      <td>0.5398108237547892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9175431215818258</td>\n",
              "      <td>0.10194730813287514</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.185031185031185</td>\n",
              "      <td>0.9583819938422338</td>\n",
              "      <td>0.8977588046958378</td>\n",
              "      <td>0.06072874493927125</td>\n",
              "      <td>0.6666666666666666</td>\n",
              "      <td>0.11131725417439702</td>\n",
              "      <td>0.7833333333333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8105805637358015</td>\n",
              "      <td>0.04708994708994709</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.089944416371905</td>\n",
              "      <td>0.9043953710584988</td>\n",
              "      <td>0.8010672358591249</td>\n",
              "      <td>0.04231166150670795</td>\n",
              "      <td>0.9111111111111111</td>\n",
              "      <td>0.08086785009861934</td>\n",
              "      <td>0.8555555555555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.7132940681531342</td>\n",
              "      <td>0.03161634103019538</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.06129476584022039</td>\n",
              "      <td>0.8552924938953179</td>\n",
              "      <td>0.7026680896478121</td>\n",
              "      <td>0.03064066852367688</td>\n",
              "      <td>0.9777777777777777</td>\n",
              "      <td>0.059419311276164746</td>\n",
              "      <td>0.8388888888888888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.6182162389566681</td>\n",
              "      <td>0.023931164291476202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0467436974789916</td>\n",
              "      <td>0.8073043847542202</td>\n",
              "      <td>0.6038420490928496</td>\n",
              "      <td>0.02367175170962651</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.046248715313463515</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.5287126630206143</td>\n",
              "      <td>0.01947483588621444</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.03820562352436145</td>\n",
              "      <td>0.7621297377640939</td>\n",
              "      <td>0.5048025613660619</td>\n",
              "      <td>0.019027484143763214</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.03734439834024896</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.42280185107278084</td>\n",
              "      <td>0.01595840057378519</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031415460642428526</td>\n",
              "      <td>0.7086739568956365</td>\n",
              "      <td>0.4057630736392743</td>\n",
              "      <td>0.015906680805938492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.031315240083507306</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.3222549432057215</td>\n",
              "      <td>0.01362314403796112</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02688009664753851</td>\n",
              "      <td>0.657925469795095</td>\n",
              "      <td>0.30672358591248666</td>\n",
              "      <td>0.013665350744002429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02696225284601558</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.21518721076987798</td>\n",
              "      <td>0.011786518341941464</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.023298429319371726</td>\n",
              "      <td>0.6038857628198322</td>\n",
              "      <td>0.20768409818569905</td>\n",
              "      <td>0.011977641735427202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.023671751709626515</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fpr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.11379890618426589</td>\n",
              "      <td>0.010452143276570758</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02068805206880521</td>\n",
              "      <td>0.5527126021870687</td>\n",
              "      <td>0.10864461045891143</td>\n",
              "      <td>0.010660980810234541</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.021097046413502112</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.1</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8197307530500632</td>\n",
              "      <td>0.04936217415418746</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.09408033826638477</td>\n",
              "      <td>0.9090136957214141</td>\n",
              "      <td>0.8117395944503736</td>\n",
              "      <td>0.04461371055495103</td>\n",
              "      <td>0.9111111111111111</td>\n",
              "      <td>0.0850622406639004</td>\n",
              "      <td>0.8609434865900383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.2</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.8452881783761044</td>\n",
              "      <td>0.05705128205128205</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.10794420861127955</td>\n",
              "      <td>0.9219131542626605</td>\n",
              "      <td>0.8386339381003202</td>\n",
              "      <td>0.04597701149425287</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.08695652173913043</td>\n",
              "      <td>0.8195043103448276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.3</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9080774084981068</td>\n",
              "      <td>0.09241952232606439</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.16920152091254753</td>\n",
              "      <td>0.953604416604735</td>\n",
              "      <td>0.8905016008537887</td>\n",
              "      <td>0.06015037593984962</td>\n",
              "      <td>0.7111111111111111</td>\n",
              "      <td>0.11091854419410746</td>\n",
              "      <td>0.801676245210728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.4</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9601388304585612</td>\n",
              "      <td>0.19017094017094016</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.31956912028725315</td>\n",
              "      <td>0.9798810914109778</td>\n",
              "      <td>0.9261472785485593</td>\n",
              "      <td>0.07605633802816901</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.13499999999999998</td>\n",
              "      <td>0.7646551724137932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.5</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9750736222128734</td>\n",
              "      <td>0.27300613496932513</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4289156626506024</td>\n",
              "      <td>0.9874190466079202</td>\n",
              "      <td>0.9372465314834578</td>\n",
              "      <td>0.07796610169491526</td>\n",
              "      <td>0.5111111111111111</td>\n",
              "      <td>0.13529411764705881</td>\n",
              "      <td>0.7262452107279693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.6</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9832772402187632</td>\n",
              "      <td>0.3588709677419355</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5281899109792285</td>\n",
              "      <td>0.9915596135470857</td>\n",
              "      <td>0.9455709711846318</td>\n",
              "      <td>0.07317073170731707</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.12371134020618556</td>\n",
              "      <td>0.6754310344827585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.7</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9880100967606227</td>\n",
              "      <td>0.43842364532019706</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6095890410958904</td>\n",
              "      <td>0.993948402165835</td>\n",
              "      <td>0.9502668089647812</td>\n",
              "      <td>0.06481481481481481</td>\n",
              "      <td>0.3111111111111111</td>\n",
              "      <td>0.10727969348659003</td>\n",
              "      <td>0.6337883141762453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.8</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9994741270509045</td>\n",
              "      <td>0.9468085106382979</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9726775956284154</td>\n",
              "      <td>0.9997345790423612</td>\n",
              "      <td>0.9765208110992529</td>\n",
              "      <td>0.10843373493975904</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.5920258620689655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smo_tl_pred_thresh_for_fnr_0.9</th>\n",
              "      <th>cat</th>\n",
              "      <td>0.9997896508203618</td>\n",
              "      <td>0.978021978021978</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9888888888888888</td>\n",
              "      <td>0.9998938316169445</td>\n",
              "      <td>0.9822838847385272</td>\n",
              "      <td>0.10416666666666667</td>\n",
              "      <td>0.1111111111111111</td>\n",
              "      <td>0.1075268817204301</td>\n",
              "      <td>0.5509219348659005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac9d2de2-07d4-4086-9767-690cb91be015')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac9d2de2-07d4-4086-9767-690cb91be015 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac9d2de2-07d4-4086-9767-690cb91be015');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the above table, our final model should be the catboost classifier trained on data that has been balanced with smotetomek, with a threshold based off the the fnr of .05.  Athough there are other models that perform better on the precision, such as some models with different thresholds and the default threshold on the smote and smote tomek balanced data, all these other models have a worse auc.  Our final model has the best balance between precision, f score, and auc."
      ],
      "metadata": {
        "id": "BNtSIzUQyayY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_nAfFbvM0fXD"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}